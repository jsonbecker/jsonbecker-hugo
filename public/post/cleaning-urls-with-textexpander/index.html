<html>
<head>
  <link href = "http://www.jsonbecker.com/css/simple-grid.css" rel = "stylesheet" type = "text/css" />
  <link href = "http://www.jsonbecker.com/css/post.css" rel = "stylesheet" type = "text/css" />
  <link rel="stylesheet" href="http://www.jsonbecker.com/css/tomorrow-night-bright.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.020/css/hack.min.css">
  <script src="http://www.jsonbecker.com/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
	    inlineMath: [['$','$'], ['\\(','\\)']],
	    displayMath: [['$$','$$'], ['\[','\]']],
	    processEscapes: true,
	    processEnvironments: true,
	    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	    TeX: { equationNumbers: { autoNumber: "AMS" },
	         extensions: ["AMSmath.js", "AMSsymbols.js"] }
	  }
	});
	</script>
	
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Queue(function() {
	    
	    
	    
	    var all = MathJax.Hub.getAllJax(), i;
	    for(i = 0; i < all.length; i += 1) {
	        all[i].SourceElement().parentNode.className += ' has-jax';
	    }
		});
	</script>
</head>
<body>

<div class = 'container'>
  <div class = 'row'>
    <div class = 'col-6'>
      <h1>Cleaning URLs with TextExpander</h1>
      <h6>2013-05-30</h6>
    </div>
  </div>
  <div class = 'row'>
    <div class = 'col-9'>
      <p>One thing I really dislike about Google Reader is it replaces the links to posts in my RSS feed. My <a href="http://pinboard.in/u:jasonpbecker">Pinboard account</a> is littered with links that start with <code>http://feedproxy.google.com</code>. I am quite concerned that with the demise of <a href="http://googlereader.blogspot.com/2013/03/powering-down-google-reader.html">Google Reader</a> on July 1, 2013, these redirects will no longer work.</p>

<p>It&rsquo;s not just Google that obscures the actually address of links on the internet. The popularity of using link shortening services, both to save characters on Twitter and to collect analytics, has proliferated the <em>Internet of Redirects</em>.</p>

<p>Worse still, after I am done cutting through redirects, I often find that the ultimate link include all kinds of extraneous attributes, most especially a barrage of <code>utm_*</code> campaign tracking.</p>

<p>Now, I understand why all of this is happening and the importance of the services and analytics this link cruft provides. I am quite happy to click on shortened links, move through all the redirects, and let sites know just how I found them. But quite often, like when using a bookmarking service or writing a blog post, I just want the simple, plain text URL that gets me directly to the permanent home of the content.</p>

<p>One part of my workflow to deal with link cruft is a TextExpander snippet I call <code>cleanURL</code>. It triggers a simple Python script that grabs the URL in my clipboard, traces through the redirects to the final destination, then strips links of campaign tracking attributes, and ultimately pastes a new URL that is much &ldquo;cleaner&rdquo;.</p>

<p>Below I have provided the script. I hope it is useful to some other folks, and I would love some recommendations for additional &ldquo;cleaning&rdquo; that could be performed.</p>

<p>My next task is expanding this script to work with <a href="http://pinboard.in">Pinboard</a> so that I can clean up all my links before the end of the month when Google Reader goes belly up.</p>

<pre><code>:::python
#!/usr/bin/python
import requests
import sys
from re import search
from subprocess import check_output

url = check_output('pbpaste')

# Go through the redirects to get the destination URL
r = requests.get(url)

# Look for utm attributes
match =  search(r'[?&amp;#]utm_', r.url)

# Because I'm not smart and trigger this with
# already clean URLs
if match:
  cleanURL = r.url.split(match.group())[0]
else:
  cleanURL = r.url

print cleanURL
</code></pre>

    </div>
  </div>
</div>
</body>
</html>
