<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JSONBecker</title>
    <link>http://www.json.blog/</link>
    <description>Recent content on JSONBecker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Feb 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.json.blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How I start my work in R</title>
      <link>http://www.json.blog/2018/02/how-i-start-my-work-in-r/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2018/02/how-i-start-my-work-in-r/</guid>
      <description>

&lt;h2 id=&#34;ideation&#34;&gt;Ideation&lt;/h2&gt;

&lt;p&gt;At the start of every project, there&amp;rsquo;s a blinking cursor.&lt;/p&gt;

&lt;p&gt;Actually, that&amp;rsquo;s almost never true for me. If I start staring at a blinking cursor, I&amp;rsquo;m almost guaranteed to keep looking at a blinking cursor, often for hours. The real work almost always starts weeks or months before I actually type anything. I think it&amp;rsquo;s easy for folks for whom their ultimate product is a bunch of code or an analysis report to undervalue that our work is creative. Writing a package or doing data analysis is still fundamentally creative work. We&amp;rsquo;re in the business of using computers to generate evidence to support insights into how things work. If all there was to it was a procedural search through models, then this would all have been automated already.&lt;/p&gt;

&lt;p&gt;When I think, &amp;ldquo;How do I wish I could write my code to solves this problem?&amp;rdquo; I know that I am getting a great idea for a package. Often, I&amp;rsquo;m staring at a function I just wrote to make my work easier and start to think &amp;ldquo;This is still too specific to my work.&amp;rdquo; I can start to see the steps of generalizing my solution a little bit further. Then I start to see how further generalization of this function will require supporting scaffolding and steps that would have been valuable. I start to think through what other problems exist in data sets unlike my own or future data I expect to work with. And I ask myself again and again, &amp;ldquo;How do I wish I could write my code to solves this problem?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Data analysis almost always starts with an existing hypothesis of interest. My guiding thoughts are &amp;ldquo;What do I need to know to understand this data? What kind of evidence would convince me?&amp;rdquo; Sometimes the first thoughts are how I would model the data, but most of the time I begin to picture 2-3 data visualizations that would present the main results of my work. Nothing I produce is meant to convince an academic audience or even other data professionals of my results. Everything I make is about delivering value back to the folks who generate the data I use in the first place. I am trying to deliver value back to organizations by using data on their current work to inform future work. So my hypotheses are &amp;ldquo;What decisions are they making with this data? What decisions are they making without this data that should be informed by it? How can I analyze and present results to influence and improve both of these processes?&amp;rdquo; The answer to that is rarely a table of model specifications. But even if your audience is one of peer technical experts, I think it&amp;rsquo;s valuable to start with what someone should learn from your analysis and how can you present that most clearly and convincingly to that audience.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t rush this process. If you don&amp;rsquo;t know where you&amp;rsquo;re heading, it&amp;rsquo;s hard to do a good job getting there. That doesn&amp;rsquo;t mean that once I &lt;em&gt;do&lt;/em&gt; start writing code, I always know exactly what I am going to do. But I find it far easier to design the right data product if I have a few guiding light ideas of what I want to accomplish from the start.&lt;/p&gt;

&lt;h2 id=&#34;design&#34;&gt;Design&lt;/h2&gt;

&lt;p&gt;The next step is not writing code, but it may still happen in your code editor of choice. Once I have some concept of where I am headed, I start to write out my ideas for the project in a &lt;code&gt;README.md&lt;/code&gt; in a new RStudio project. Now is the time to describe who your work is for and how you expect them to interact with that work. Similar to something like a &amp;ldquo;project charter&amp;rdquo;, your README should talk about what the goals are for the project, what form the project will take (a package? an Rmd -&amp;gt; pdf report? a website? a model to be deployed into production for use in this part of the application?), and who the audience is for the end product. If you&amp;rsquo;re working with collaborators, this is a great way to level-set and build common understanding. If you&amp;rsquo;re not working with collaborators, this is a great way to articulate the scope of the project and hold yourself accountable to that scope. It also is helpful for communicating to managers, mentors, and others who may eventually interact with your work even if they will be less involved at the inception.&lt;/p&gt;

&lt;p&gt;For a package, I would write out the primary functions you expect someone to interact with and how those functions interact with each other. Use your first README to specify that this package will have functions to get data from a source, process that data into a more easy to use format, validate that data prior to analysis, and produce common descriptive statistics and visuals that you&amp;rsquo;d want to produce before using that data set for something more complex. That&amp;rsquo;s just an example, but now you have the skeletons for your first functions: &lt;code&gt;fetch&lt;/code&gt;, &lt;code&gt;transform&lt;/code&gt;, &lt;code&gt;validate&lt;/code&gt;, and &lt;code&gt;describe&lt;/code&gt;. Maybe each of those functions will need multiple variants. Maybe &lt;code&gt;validate&lt;/code&gt; will get folded into a step at &lt;code&gt;fetch&lt;/code&gt;. You&amp;rsquo;re not guaranteed to get this stuff right from the start, but you&amp;rsquo;re far more likely to design a clear, clean API made with composable functions that each help with one part of the process if you think this through before writing your first function. Like I said earlier, I often think of writing a package when I look at one of my existing functions and realize I can generalize it further. Who among us hasn&amp;rsquo;t written a monster function that does all of the work of &lt;code&gt;fetch&lt;/code&gt;, &lt;code&gt;transform&lt;/code&gt;, &lt;code&gt;validate&lt;/code&gt;, and &lt;code&gt;describe&lt;/code&gt; all at once?&lt;/p&gt;

&lt;h2 id=&#34;design-your-data&#34;&gt;Design Your Data&lt;/h2&gt;

&lt;p&gt;I always write up a data model at the start of a new project. What are the main data entities I&amp;rsquo;ll be working with? What properties do I expect they will have? How do I expect them to relate to one another? Even when writing a package, I want to think about &amp;ldquo;What are the ideal inputs and outputs for this function?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Importantly, when what I have in mind is a visualization, I actually fake data and get things working in &lt;code&gt;ggplot&lt;/code&gt; or &lt;code&gt;highcharter&lt;/code&gt;, depending on what the final product will be. Why? I want to make sure the visual is compelling with a fairly realistic set of data. I also want to know how to organize my data to make that visualization easy to achieve. It helps me to define the output of my other work far more clearly.&lt;/p&gt;

&lt;p&gt;In many cases, I want to store my data in a database, so I want to start with a simple design of the tables I expect to have, along with validity and referential constraints I want to apply. If I understand what data I will have, how it is related, what are valid values, and how and where I expect the data set to expand, I find it far easier to write useful functions and reproducible work. I think this is perhaps the most unique thing I do and it comes from spending a lot of time thinking about data architectures in general. If I&amp;rsquo;m analyzing school district data, I want to understand what district level properties and measures I&amp;rsquo;ll have, what school properties and measures I&amp;rsquo;ll have, what student property and measures I&amp;rsquo;ll have, what teacher properties and measures I&amp;rsquo;ll have, etc. Even if the analysis is coming from or will ultimately produce a single, flattened out, big rectangle of data, I crave &lt;a href=&#34;https://en.wikipedia.org/wiki/Database_normalization&#34;&gt;normality&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;make-files&#34;&gt;Make Files&lt;/h2&gt;

&lt;p&gt;So now my README defines a purpose, it talks about how I expect someone to interact with my code or what outputs they should expect from the analysis, and has a description of the data to be used and how it&amp;rsquo;s organized. Only then do I start to write &lt;code&gt;.R&lt;/code&gt; files in my &lt;code&gt;R/&lt;/code&gt; directory. Even then I&amp;rsquo;m probably not writing code but instead pseudocode outlines of how I want things to work, or fake example data to be used later. I&amp;rsquo;m not much of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Test-driven_development&#34;&gt;test-driven development&lt;/a&gt; person, but the first code I write looks a lot like test data and basic functions that are meeting some test assertions. Here&amp;rsquo;s some small bit of data, can I pass it into this function and get what I want out? What if I create this failure state? What if I can&amp;rsquo;t assume column are right?&lt;/p&gt;

&lt;p&gt;Writing code is far more fun when I know where I am heading. So that&amp;rsquo;s how I start my work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stack of Glue</title>
      <link>http://www.json.blog/2018/01/stack-of-glue/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2018/01/stack-of-glue/</guid>
      <description>&lt;p&gt;I have some text, but I want the content of that text to be dynamic based on data. This is a case for string interpolation. Lots of languages have the ability to write something like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;pet = &amp;quot;dog&amp;quot;
puts &amp;quot;This is my {#pet}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pet = &amp;quot;dog&amp;quot;
print(f&amp;quot;This is my {pet}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There have been ways to do this in R, but I&amp;rsquo;ve mostly hated them until &lt;code&gt;glue&lt;/code&gt; came along. Using &lt;code&gt;glue&lt;/code&gt; in R should look really familiar now:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pet &amp;lt;- &amp;quot;dog&amp;quot;
glue(&amp;quot;This is my {pet}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Awesome! Now I have a way to make text bend to my bidding using data. But this is pretty simple, and we could have just used something like &lt;code&gt;paste(&amp;quot;This is my&amp;quot;, pet)&lt;/code&gt; and been done with it.&lt;/p&gt;

&lt;p&gt;Let me provide a little motivation in the form of &lt;code&gt;data.frame&lt;/code&gt;s, &lt;code&gt;glue_data&lt;/code&gt;, and some &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pretend we have a field in a database called &lt;code&gt;notes&lt;/code&gt;. I want to set the &lt;code&gt;notes&lt;/code&gt; for each entity to follow the same pattern, but use other data to fill in the blanks. Like maybe something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;notes &amp;lt;- &amp;quot;This item price is valid through {end_date} and will then increase {price_change} to {new_price}.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a terrible contrived example, but we can imagine displaying this note to someone with different content for each item. Now in most scenarios, the right thing to do for an application is to produce this content dynamically based on what&amp;rsquo;s in the database, but let&amp;rsquo;s pretend no one looked far enough ahead to store this data or like notes can serve lots of different purposes using different data. So there is no place for the application to find &lt;code&gt;end_date&lt;/code&gt;, &lt;code&gt;price_change&lt;/code&gt;, or &lt;code&gt;new_price&lt;/code&gt; in its database. Instead, this was something prepared by sales in Excel yesterday and they want these notes added to all items to warn their customers.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how to take a table that has &lt;code&gt;item_id&lt;/code&gt;, &lt;code&gt;end_date&lt;/code&gt;, &lt;code&gt;price_change&lt;/code&gt;, and &lt;code&gt;new_price&lt;/code&gt; as columns and turn it into a table with &lt;code&gt;item_id&lt;/code&gt;, and &lt;code&gt;notes&lt;/code&gt; as columns, with your properly formatted note for each item to be updated in a database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glue)
library(purrr)

item_notes &amp;lt;- data.frame(
  item_id = seq_len(10),
  end_date = c(rep(as.Date(&#39;2018-03-01&#39;, format = &#39;%Y-%m-%d&#39;), 5),
               rep(as.Date(&#39;2018-03-05&#39;, format = &#39;%Y-%m-%d&#39;), 3),
               rep(as.Date(&#39;2018-03-09&#39;, format = &#39;%Y-%m-%d&#39;), 2)),
  price_change = sample(x = seq_len(5),replace = TRUE,size = 10),
  new_price = sample(x = 10:20,replace = TRUE,size = 10)
)

template &amp;lt;- &amp;quot;This item price is valid through {end_date} and will then increase {price_change} to {new_price}.&amp;quot;

map_chr(split(item_notes, item_notes$item_id), 
    glue_data, 
    template) %&amp;gt;% 
stack() %&amp;gt;% 
rename(item_id = ind,
       notes = values)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What&amp;rsquo;s going on here? First, I want to apply my &lt;code&gt;glue&lt;/code&gt; technique to rows of a &lt;code&gt;data.frame&lt;/code&gt;,
so I &lt;code&gt;split&lt;/code&gt; the data into a &lt;code&gt;list&lt;/code&gt; using &lt;code&gt;item_id&lt;/code&gt; as the identifier. That&amp;rsquo;s because at the end of all this I want to preserve that id to match back up in a database. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:rownames&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:rownames&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; The function &lt;code&gt;glue_data&lt;/code&gt; works like &lt;code&gt;glue&lt;/code&gt;, but it accepts things that are &amp;ldquo;listish&amp;rdquo; as it&amp;rsquo;s first argument (like &lt;code&gt;data.frames&lt;/code&gt; and named &lt;code&gt;lists&lt;/code&gt;). So with handy &lt;code&gt;map&lt;/code&gt; over my newly created &lt;code&gt;list&lt;/code&gt; of &amp;ldquo;listish&amp;rdquo; data, I create a named &lt;code&gt;list&lt;/code&gt; with the text I wanted to generate. I then use a base R function that&amp;rsquo;s new to me &lt;code&gt;stack&lt;/code&gt;, which will take a list and make each element a row in a &lt;code&gt;data.frame&lt;/code&gt; with &lt;code&gt;ind&lt;/code&gt; as the name of the &lt;code&gt;list&lt;/code&gt; element and &lt;code&gt;values&lt;/code&gt; as the value.&lt;/p&gt;

&lt;p&gt;Now I&amp;rsquo;ve got a nice &lt;code&gt;data.frame&lt;/code&gt;, ready to be joined with any table that has &lt;code&gt;item_id&lt;/code&gt; so it can have the attached note!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:rownames&#34;&gt;You can split on &lt;code&gt;row.names&lt;/code&gt; if you don&amp;rsquo;t have a similar identifer and just want to go from &lt;code&gt;data.frame&lt;/code&gt; to a &lt;code&gt;list&lt;/code&gt; of your rows.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:rownames&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Naming Manual Colors with ggplot2</title>
      <link>http://www.json.blog/2018/01/naming-manual-colors-with-ggplot2/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2018/01/naming-manual-colors-with-ggplot2/</guid>
      <description>&lt;p&gt;I have been using &lt;code&gt;ggplot2&lt;/code&gt; for 7 years I think. In all that time, I&amp;rsquo;ve been frustrated that I can never figure out what order to put my color values in for &lt;code&gt;scale_*_manual&lt;/code&gt;. Not only is the order mapping seemingly random to me, I know that sometimes if I change something about how I&amp;rsquo;m treating the data, the order switches up.&lt;/p&gt;

&lt;p&gt;Countless hours could have been saved if I knew that this one, in hindsight, obvious thing was possible.&lt;/p&gt;

&lt;p&gt;Whenever using &lt;code&gt;scale_*_manual&lt;/code&gt;, you can directly reference a color using a character vector and then name your &lt;code&gt;value&lt;/code&gt; in the &lt;code&gt;scale_&lt;/code&gt; call like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;geom_blah(aes(color = &#39;good&#39;)) +
geom_blah(aes(color = &#39;bad&#39;)) +
scale_blah_manual(values = c(good = &#39;green&#39;, bad = &#39;red&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously this is a toy example, but holy game changer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>This Year in My R Code</title>
      <link>http://www.json.blog/2017/12/this-year-in-my-r-code/</link>
      <pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/12/this-year-in-my-r-code/</guid>
      <description>

&lt;p&gt;Looking back on 2017, there were three major trends in my R code: the end of S4, directly writing to SQL database, and &lt;code&gt;purrr&lt;/code&gt; everywhere.&lt;/p&gt;

&lt;h2 id=&#34;the-end-of-s4&#34;&gt;The End of S4&lt;/h2&gt;

&lt;p&gt;The first package I ever wrote extensively used S4 classes. I wanted to have the security of things like &lt;code&gt;setValidity&lt;/code&gt;. I liked the idea of calling &lt;code&gt;new&lt;/code&gt; as it felt more like class systems I was familiar with from that one semester of Java in college. S4 felt more grown up than S3, more like it was utilizing the advantages of object oriented programming, and less exotic than R6, which in 2014 felt riskier to build with and teach future employees. Using S4 was a mistake from day one and never led to any advantages in the code I wrote.&lt;/p&gt;

&lt;p&gt;So this year, I rewrote that original package. It&amp;rsquo;s internal (and a core function) at my job so I can&amp;rsquo;t share too much, but this was a long time coming. Not only did I clean up a lot of code that was just plain bad (in the way all old code is), but I got rid of S4 in favor of S3 or more functional code wherever possible. Our test coverage is far more complete, the code is far easier to extend without duplication, and it looks far more idiomatic to the standard non-BioConductor R user.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s the lesson learned here? From a technical perspective, it would be avoid premature optimization and, of course, that everyone can and wants to throw out old code they revist with greater knowledge and context. But I know those things. What drove me to making the wrong decision here was purely imposter syndrome. I was writing code that had to be run unattended on a regular basis as a part of a product in a new job. I didn&amp;rsquo;t feel up to the task, so I felt working with a new, complex, scary part of R that promised some notion of &amp;ldquo;safety&amp;rdquo; would mean I really knew what I was doing. So my takeaway from walking away from S4 is this: start small, build what you know, have confidence you can solve problems one at a time, and trust yourself.&lt;/p&gt;

&lt;h2 id=&#34;directly-writing-sql&#34;&gt;Directly Writing SQL&lt;/h2&gt;

&lt;p&gt;I use SQL far more than R, but almost entirely as a consumer (e.g. &lt;code&gt;SELECT&lt;/code&gt; only). I&amp;rsquo;ve almost always directly used SQL for my queries into &lt;em&gt;other people&amp;rsquo;s data&lt;/em&gt;, but rarely ventured into the world of &lt;code&gt;INSERT&lt;/code&gt; or &lt;code&gt;UPDATE&lt;/code&gt; directly, preferring to use interfaces like &lt;code&gt;dbWriteTable&lt;/code&gt;. This gets back to imposter syndrome&amp;ndash; there&amp;rsquo;s so little damage that can be done with a &lt;code&gt;SELECT&lt;/code&gt; statement, but writing into databases I don&amp;rsquo;t control means taking on risk and responsiblity.&lt;/p&gt;

&lt;p&gt;This year I said fuck it&amp;ndash; there&amp;rsquo;s a whole lot of work and complexity going on that&amp;rsquo;s entirely related to me not wanting to write &lt;code&gt;INSERT INTO&lt;/code&gt; a PostgreSQL has the amazing &lt;code&gt;ON CONFLICT...&lt;/code&gt;-based &amp;ldquo;upserts&amp;rdquo; now. So I started to write a lot of queries, some of them pretty complex &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:lateral&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:lateral&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. R is a great wrapper language, and it&amp;rsquo;s database story is getting even better with the new DBI, odbc, and RPostgres packages. Although it&amp;rsquo;s native table writing support is a little weak, there&amp;rsquo;s no problem at all just using &lt;code&gt;dbSendStatement&lt;/code&gt; with complex queries. I&amp;rsquo;ve fallen into a pattern I really like of writing temporary tables (with &lt;code&gt;dplyr::copy_to&lt;/code&gt; because it&amp;rsquo;s clean in a pipeline) and then executing complex SQL with &lt;code&gt;dbSendStatement&lt;/code&gt;. In the future, I might be inclined to make these database functions, but either way this change has been great. I feel more confident than ever working with databases and R (my two favorite places to be) and I have been able to simplify a whole lot of code that involved passing around text files (and boy do I hate the type inference and other madness that can happen with CSVs. Oy.).&lt;/p&gt;

&lt;h2 id=&#34;purrr&#34;&gt;purrr&lt;/h2&gt;

&lt;p&gt;This is the year that &lt;code&gt;purrr&lt;/code&gt; not only clicked, but became my preferred way to write code. Where there was &lt;code&gt;apply&lt;/code&gt;, now there was &lt;code&gt;purrr&lt;/code&gt;. Everything started to look like a list. I&amp;rsquo;m still only scratching the surface here, but I love code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;locations %&amp;gt;%
  filter(code %in% enrollment$locations) %$%
  code %&amp;gt;%
  walk(function(x) render(input = &#39;schprofiles.Rmd&#39;,
                          html_document(theme = NULL,
                                        template = NULL,
                                        self_contained = FALSE,
                                        css = &#39;static/styles.css&#39;,
                                        lib_dir = &#39;cache/demo/output/static/&#39;,
                                        includes = includes(&#39;fonts.html&#39;)),
                          params = list(school_code = x),
                          output_file = paste0(x,&#39;.html&#39;),
                          output_dir = &amp;quot;cache/demo/output/&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s a simple way to run through all of the &lt;code&gt;locations&lt;/code&gt; (a &lt;code&gt;data.frame&lt;/code&gt; with columns &lt;code&gt;code&lt;/code&gt; and &lt;code&gt;name&lt;/code&gt;) and render an HTML-based profile of each school (defined by having student enrollment). &lt;code&gt;walk&lt;/code&gt; is beautiful, and so is &lt;code&gt;purrr&lt;/code&gt;. I mean, who does need to do &lt;code&gt;map(., mutate_if, is.numeric, as.character)&lt;/code&gt; 10 times a day?&lt;/p&gt;

&lt;h2 id=&#34;2018-r-goals&#34;&gt;2018 R Goals&lt;/h2&gt;

&lt;p&gt;One thing that&amp;rsquo;s bittersweet is that 2017 is probably the last year in a long time that writing code is the main thing my every day job is about. With increased responsibility and the growth of my employees, I find myself reviewing code a lot more than writing it, and sometimes not even that. With that in mind, I have a few goals for 2018 that I hope will keep the part of me that loves R engaged.&lt;/p&gt;

&lt;p&gt;First, I want to start writing command line utilities using R. I know almost nothing beyond &lt;code&gt;Rscript -e&lt;/code&gt; or &lt;code&gt;./script.sh&lt;/code&gt; when it comes to writing a CLI. But there are all kinds of tasks I do every day that could be written as small command line scripts. Plus, my favorite part of package authoring is writing interfaces for other people to use. How do I expect someone to want to use R and reason about a problem I&amp;rsquo;m helping to solve? It&amp;rsquo;s no wonder that I work on product every day with this interest. So I figure one way to keep engaged in R is to learn how to design command line utilities in R and get good at it. Rather than write R code purely intended to be called and used from R, my R code is going to get an interface this year.&lt;/p&gt;

&lt;p&gt;Like every year, I&amp;rsquo;d like to keep up with this blog. I never do, but this year I had a lot of encouraging signs. I actually got considerable attention for every R-related post (high hundreds of views), so I think it&amp;rsquo;s time to lean into that. I&amp;rsquo;m hoping to write one R related post each week. I think the focus will help me have some chance of pulling this off. Since I also want to keep my R chops alive while I move further and further away from day to day programming responsibilities, it should be a two birds with one stone scenario. One major thing I haven&amp;rsquo;t decided&amp;ndash; do I want to submit to r-bloggers? I&amp;rsquo;m sure it&amp;rsquo;d be a huge source of traffic, but I find it frustrating to have to click through from my RSS reader of choice when finding things there.&lt;/p&gt;

&lt;p&gt;Lastly, I&amp;rsquo;d like to start to understand the internals of a core package I use every day. I haven&amp;rsquo;t decided what that&amp;rsquo;ll be. Maybe it&amp;rsquo;ll be something really fundamental like &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;DBI&lt;/code&gt;, or &lt;code&gt;ggplot2&lt;/code&gt;. Maybe it&amp;rsquo;ll be something &amp;ldquo;simpler&amp;rdquo;. But I use a lot more R code than I read. And one thing I&amp;rsquo;ve learned every time I&amp;rsquo;ve forced myself to dig in is that I understand more R than I thought and also that reading code is one of the best ways to learn more. I want to do at least one deep study that advances my sense of self-R-worth. Maybe I&amp;rsquo;ll even have to take the time to learn a little C++ and understand how Rccp is being used to change the R world.&lt;/p&gt;

&lt;h2 id=&#34;special-thanks&#34;&gt;Special Thanks&lt;/h2&gt;

&lt;p&gt;The #rstats world on Twitter has been the only reason I can get on that service anymore. It&amp;rsquo;s a great and positive place where I learn a ton and I really appreciate feeling like there is a family of nerds out there talking about stuff that I feel like no one should care about. My tweets are mostly stupid musings that come to me and retweeting enraging political stuff in the dumpster fire that is Trump&amp;rsquo;s America, so I&amp;rsquo;m always surprised and appreciative that anyone follows me. It&amp;rsquo;s so refreshing to get away from that and just read #rstats. So thank you for inspiring me and teaching me and being a fun place to be.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:lateral&#34;&gt;I let out quite the &amp;ldquo;fuck yea!&amp;rdquo; when I got that two-common-table-expression, two joins with one lateral join in an upsert query to work.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:lateral&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R, Docker, Circle, and Environments</title>
      <link>http://www.json.blog/2017/07/r-docker-circle-and-environments/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/07/r-docker-circle-and-environments/</guid>
      <description>

&lt;p&gt;My latest project at work involves (surprise!) an R package that interacts with a database. For the most part, that&amp;rsquo;s nothing new for me. Almost all the work I&amp;rsquo;ve done in R in the last 7 years has interacted with databases in some way. What was new for this project is that the database would not be remote, but instead would be running alongside my code in a linked Docker container.&lt;/p&gt;

&lt;h2 id=&#34;a-quick-step-back-about-docker&#34;&gt;A quick step back about Docker&lt;/h2&gt;

&lt;p&gt;Docker is something you use if you want to be cool on Hacker News. But Docker is also a great way to have a reproducible environment to run your code in, from the operating system up. A full review of Docker is beyond the scope of this post (maybe &lt;a href=&#34;https://www.docker.com/what-container&#34;&gt;check this out&lt;/a&gt;), but I would think of it like this: if you run your code in a Docker container, you can guarantee your code works because you&amp;rsquo;re creating a reproducible environment that can be spun up anywhere. Think of it like making an R package instead of writing an analysis script. Installing the package means you get all your dependency packages and have confidence the functions contained within will work on different machines. Docker takes that to the next level and includes operating system level dependencies like drivers and network configurations in addition to just the thing your R functions use.&lt;/p&gt;

&lt;h2 id=&#34;some-challenges-with-testing-in-r&#34;&gt;Some challenges with testing in R&lt;/h2&gt;

&lt;p&gt;Like many folks, I use &lt;code&gt;devtools&lt;/code&gt; and &lt;code&gt;testthat&lt;/code&gt; extensively when developing packages. I strive for as-near-as-feasible 100% coverage with my tests, and I am constantly hitting Cmd + Shift + T while writing code in RStudio or running &lt;code&gt;devtools::test()&lt;/code&gt;. I even use &lt;code&gt;Check&lt;/code&gt; in the Build pane in RStudio and &lt;code&gt;goodpractice::gp()&lt;/code&gt; to keep me honest even if my code won&amp;rsquo;t make it to CRAN. But I ran into a few things working with CircleCI running my tests inside of a docker container that pushed me to learn a few critical pieces of information about testing in R.&lt;/p&gt;

&lt;h3 id=&#34;achieving-exit-status-1&#34;&gt;Achieving exit status 1&lt;/h3&gt;

&lt;p&gt;Only two ways of running tests (that I can tell) will result in a returning an exit status code of 1 (error in Unix systems) and therefore cause a build to fail in a continuous integration system. Without that exit status, failing tests won&amp;rsquo;t fail a build, so don&amp;rsquo;t run &lt;code&gt;devtools::test()&lt;/code&gt; and think you&amp;rsquo;re good to go.&lt;/p&gt;

&lt;p&gt;This means using &lt;code&gt;R CMD build . &amp;amp;&amp;amp; R CMD check *tar.gz&lt;/code&gt; or &lt;code&gt;testthat::test_package($MY_PACKAGE)&lt;/code&gt; are your best bet in most cases. I prefer using &lt;code&gt;testthat::test_package()&lt;/code&gt; because &lt;code&gt;R CMD check&lt;/code&gt; cuts off a ton of useful information about test failures without digging into the &lt;code&gt;*.Rcheck&lt;/code&gt; folder. Since I want to see information about test failures directly in my CI tool, this is a pain. Also, although not released yet, because &lt;code&gt;testthat::test_package()&lt;/code&gt; supports alternative reporters, I can have jUnit output, which plays very nicely with many CI tools.&lt;/p&gt;

&lt;h3 id=&#34;methods-for-s4&#34;&gt;Methods for S4&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;methods&lt;/code&gt; package is not loaded using &lt;code&gt;Rscript -e&lt;/code&gt;, so if you use &lt;code&gt;S4&lt;/code&gt; classes make sure you call &lt;code&gt;library(methods);&lt;/code&gt; as part of your tests. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:methods&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:methods&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&#34;environment-variables-and-r-cmd-check&#34;&gt;Environment Variables and R CMD check&lt;/h3&gt;

&lt;p&gt;When using &lt;code&gt;R CMD check&lt;/code&gt; and other functions that call to that program, your environment variables from the OS may not &amp;ldquo;make it&amp;rdquo; through to R. That means calls to &lt;code&gt;Sys.getenv()&lt;/code&gt; when using &lt;code&gt;devtools::test()&lt;/code&gt; might work, but using &lt;code&gt;testthat::test_package()&lt;/code&gt; or &lt;code&gt;R CMD check&lt;/code&gt; may fail.&lt;/p&gt;

&lt;p&gt;This was a big thing I ran into. The way I know the host address and port to talk to in the database container running along side my code is using environment variables. All of my tests that were testing against a test database containers were failing for a while and I couldn&amp;rsquo;t figure out why. The key content was on &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html&#34;&gt;this page about R startup&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;R CMD check and R CMD build do not always read the standard startup files, but they do always read specific Renviron files. The location of these can be controlled by the environment variables R_CHECK_ENVIRON and R_BUILD_ENVIRON. If these are set their value is used as the path for the Renviron file; otherwise, files ‘~/.R/check.Renviron’ or ‘~/.R/build.Renviron’ or sub-architecture-specific versions are employed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So it turns out I had to get my environment variables of interest into the &lt;code&gt;R_CHECK_ENVIRON&lt;/code&gt;. At first I tried this by using &lt;code&gt;env &amp;gt; ~/.R/check.Renviron&lt;/code&gt; but it turns out that &lt;code&gt;docker run&lt;/code&gt; runs commands as &lt;code&gt;root&lt;/code&gt;, and R doesn&amp;rsquo;t like that very much. Instead, I had to specify &lt;code&gt;R_CHECK_ENVIRON=some_path&lt;/code&gt; and then used &lt;code&gt;env &amp;gt; $R_CHECK_ENVIRON&lt;/code&gt; to make sure that my environment variables were available during testing.&lt;/p&gt;

&lt;p&gt;In the end, I have everything set up quite nice. Here are some snippets that might help.&lt;/p&gt;

&lt;h2 id=&#34;circle-yml&#34;&gt;circle.yml&lt;/h2&gt;

&lt;p&gt;At the top I specify my &lt;code&gt;R_CHECK_ENVIRON&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;machine:
  services:
    - docker
  environment:
    R_CHECK_ENVIRON: /var/$MY_PACKAGE/check.Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I run my actual tests roughly like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;test:
  override:
    - docker run --link my_database_container -it -e R_CHECK_ENVIRON=$R_CHECK_ENVIRON my_container:my_tag /bin/bash ./scripts/run_r_tests.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker adds critical environment variables to the container when using &lt;code&gt;--link&lt;/code&gt; that point to the host and port I can use to find the database container.&lt;/p&gt;

&lt;h2 id=&#34;run-r-tests-sh&#34;&gt;run_r_tests.sh&lt;/h2&gt;

&lt;p&gt;I use a small script that takes care of dumping my environment properly and sets me up to take advantage of &lt;code&gt;test_package()&lt;/code&gt;&amp;rsquo;s reporter option rather than directly writing my commands in line with &lt;code&gt;docker run&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#! /bin/bash
# dump environment into R check.Renviron
env &amp;gt; /var/my_package/check.Renviron

Rscript -e &amp;quot;library(devtools);devtools::install();library(testthat);library(my_package);test_package(&#39;my_package&#39;, reporter = &#39;Summary&#39;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be honest, I&amp;rsquo;m not convinced I need to do either the &lt;code&gt;install()&lt;/code&gt; step or &lt;code&gt;library(my_package)&lt;/code&gt;. Also, you can run &lt;code&gt;R CMD build . &amp;amp;&amp;amp; R CMD check *tar.gz&lt;/code&gt; instead of using the &lt;code&gt;Rscript&lt;/code&gt; line. I am also considering copying the &lt;code&gt;.Rcheck&lt;/code&gt; folder to &lt;code&gt;$CIRCLE_ARTIFACTS&lt;/code&gt; so that I can download it as desired. To do that, you can just add:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p $CIRCLE_ARTIFACTS/test_results
cp -r *.Rcheck $CIRCLE_ARTIFACTS/test_results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope that some of this information is useful if you&amp;rsquo;re thinking about mixing R, continuous integration, and Docker. If not, at least when I start searching the internet for this information next time, at least this post will show up and remind me of what I used to know.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:methods&#34;&gt;This is only a problem for my older packages. I&amp;rsquo;ve long since decided S4 is horrible and not worth it. Just use S3, although R6 looks very attractive.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:methods&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Announcing jsonfeedr, a new R package</title>
      <link>http://www.json.blog/2017/06/announcing-jsonfeedr-a-new-r-package/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/06/announcing-jsonfeedr-a-new-r-package/</guid>
      <description>&lt;p&gt;I have not yet spent the time to figure out how to generate a &lt;a href=&#34;https://jsonfeed.org/&#34;&gt;JSON feed&lt;/a&gt; in Hugo yet. But I have built an R package to play with JSON feeds. It&amp;rsquo;s called &lt;a href=&#34;https://github.com/jsonbecker/jsonfeedr&#34;&gt;jsonfeedr&lt;/a&gt;, and it&amp;rsquo;s silly simple.&lt;/p&gt;

&lt;p&gt;Maybe I&amp;rsquo;ll extend this in the future. I hope people will submit PRs to expand it. For now, I was inspired by all the talk about &lt;em&gt;why&lt;/em&gt; JSON feed even exists. Working with JSON is fun and easy. Working with XML is not.&lt;/p&gt;

&lt;p&gt;Anyway, I figured the guy who registered &lt;a href=&#34;http://json.blog&#34;&gt;json.blog&lt;/a&gt; should have a package out there working with JSON.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functions as Arguments in R</title>
      <link>http://www.json.blog/2017/05/functions-as-arguments-in-r/</link>
      <pubDate>Mon, 29 May 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/05/functions-as-arguments-in-r/</guid>
      <description>&lt;p&gt;Sometimes, silly small things about code I write just delight me. There are &lt;a href=&#34;http://www.alexejgossmann.com/benchmarking_r/&#34;&gt;lots of ways to time things&lt;/a&gt; in R. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:tictoc&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:tictoc&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Tools like &lt;code&gt;microbenchmark&lt;/code&gt; are great for profiling code, but what I do all the time is log how long database queries that are scheduled to run each night are taking.&lt;/p&gt;

&lt;p&gt;It is really easy to use calls to &lt;code&gt;Sys.time&lt;/code&gt; and &lt;code&gt;difftime&lt;/code&gt; when working interactively, but I didn&amp;rsquo;t want to pepper all of my code with the same log statements all over the place. So instead, I wrote a function.&lt;/p&gt;

&lt;p&gt;Almost all of &lt;code&gt;timing&lt;/code&gt; is straightforward to even a novice R user. I record what time it is using &lt;code&gt;Sys.time&lt;/code&gt;, do a little formatting work to make things look the way I want for reading logs, and pass in an optional message.&lt;/p&gt;

&lt;p&gt;The form of &lt;code&gt;timing&lt;/code&gt; was easy for me to sketch out: &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:realize&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:realize&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;timing &amp;lt;- function(STUFF, msg = &#39;&#39;) {
  start_time &amp;lt;- format(Sys.time(), &#39;%a %b %d %X %Y&#39;)
  start_msg &amp;lt;- paste(&#39;Starting&#39;, msg,
                     &#39;at:&#39;, start_time, &#39;\n&#39;)
  cat(start_msg)
  # Call my function here
  end_time &amp;lt;- format(Sys.time(), &#39;%a %b %d %X %Y&#39;)
  end_msg &amp;lt;- paste(&#39;Completed&#39;, &#39;at:&#39;, end_time, &#39;\n&#39;)
  cat(end_msg)
  elapsed &amp;lt;- difftime(as.POSIXlt(end_time, format = &#39;%a %b %d %X %Y&#39;),
                      as.POSIXlt(start_time, format = &#39;%a %b %d %X %Y&#39;))
  cat(&#39;Elapsed Time: &#39;, format(unclass(elapsed), digits = getOption(&#39;digits&#39;)),
      &#39; &#39;, attr(elapsed, &#39;units&#39;), &#39;\n\n\n&#39;, sep = &#39;&#39;)
  result
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The thing I needed to learn when I wrote &lt;code&gt;timing&lt;/code&gt; a few years back was how to fill in &lt;code&gt;STUFF&lt;/code&gt; and &lt;code&gt;# Call my function here&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Did you know that you can pass a function as an argument in another function in R? I had been using &lt;code&gt;*apply&lt;/code&gt; with its &lt;code&gt;FUN&lt;/code&gt; argument all over the place, but never &lt;em&gt;really&lt;/em&gt; thought about it until I wrote &lt;code&gt;timing&lt;/code&gt;. Of course in R you can pass a function name, and I even know how to pass arguments to that function&amp;ndash; just like &lt;code&gt;apply&lt;/code&gt;, just declare a function with the magical &lt;code&gt;...&lt;/code&gt; and pass that along to the fucntion being passed in.&lt;/p&gt;

&lt;p&gt;So from there, it was clear to see how I&amp;rsquo;d want my function declartion to look. It would definitely have the form &lt;code&gt;function(f, ..., msg = &#39;&#39;)&lt;/code&gt;, where &lt;code&gt;f&lt;/code&gt; was some function and &lt;code&gt;...&lt;/code&gt; were the arguments for that function. What I didn&amp;rsquo;t know was how to properly call that function. Normally, I&amp;rsquo;d write something like &lt;code&gt;mean(...)&lt;/code&gt;, but I don&amp;rsquo;t know what &lt;code&gt;f&lt;/code&gt; is in this case!&lt;/p&gt;

&lt;p&gt;As it turns out, the first thing I tried worked, much to my surprise. R actually makes this super easy&amp;ndash; you can just write &lt;code&gt;f(...)&lt;/code&gt;, and &lt;code&gt;f&lt;/code&gt; will be replaced with whatever the argument is to &lt;code&gt;f&lt;/code&gt;! This just tickles me. It&amp;rsquo;s stupid elegant to my eyes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;timing &amp;lt;- function(f, ..., msg = &#39;&#39;) {
  start_time &amp;lt;- format(Sys.time(), &#39;%a %b %d %X %Y&#39;)
  start_msg &amp;lt;- paste(&#39;Starting&#39;, msg,
                     &#39;at:&#39;, start_time, &#39;\n&#39;)
  cat(start_msg)
  x &amp;lt;- f(...)
  end_time &amp;lt;- format(Sys.time(), &#39;%a %b %d %X %Y&#39;)
  end_msg &amp;lt;- paste(&#39;Completed&#39;, &#39;at:&#39;, end_time, &#39;\n&#39;)
  cat(end_msg)
  elapsed &amp;lt;- difftime(as.POSIXlt(end_time, format = &#39;%a %b %d %X %Y&#39;),
                      as.POSIXlt(start_time, format = &#39;%a %b %d %X %Y&#39;))
  cat(&#39;Elapsed Time: &#39;, format(unclass(elapsed), digits = getOption(&#39;digits&#39;)),
      &#39; &#39;, attr(elapsed, &#39;units&#39;), &#39;\n\n\n&#39;, sep = &#39;&#39;)
  x
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I can monitor the run time of any function by wrapping it in &lt;code&gt;timing&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;timing(read.csv, &#39;my_big_file.csv&#39;, header = TRUE, stringsAsFactors = FALSE)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here&amp;rsquo;s an example of the output from a job that ran this morning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;Starting queries/accounts.sql at: Mon May 29 06:24:12 AM 2017
Completed at: Mon May 29 06:24:41 AM 2017
Elapsed Time: 29 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:tictoc&#34;&gt;&lt;code&gt;tictoc&lt;/code&gt; is new to me, but I&amp;rsquo;m glad it is. I would have probably never written the code in this post if it existed, and then I would be sad and this blog post wouldn&amp;rsquo;t exist.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:tictoc&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:realize&#34;&gt;Yes, I realize that having the calls to &lt;code&gt;paste&lt;/code&gt; and &lt;code&gt;cat&lt;/code&gt; after setting &lt;code&gt;start_time&lt;/code&gt; technically add those calls to the stack of stuff being timed and both of those things could occur after function execution. For my purposes, the timing does not have to be nearly that precise and the timing of those functions will contribute virtually nothing. So I opted for what I think is the clearer style of code as well as ensuring that live monitoring would inform me of what&amp;rsquo;s currently running.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:realize&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making Sense of dplyr 0.6</title>
      <link>http://www.json.blog/2017/05/making-sense-of-dplyr-0.6/</link>
      <pubDate>Thu, 18 May 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/05/making-sense-of-dplyr-0.6/</guid>
      <description>

&lt;p&gt;Non-standard evaluation is one of R&amp;rsquo;s best features, and also one of it&amp;rsquo;s most perplexing. Recently I have been making good use of &lt;code&gt;wrapr::let&lt;/code&gt; to allow me to write reusable functions without a lot of assumptions about my data. For example, let&amp;rsquo;s say I always want to &lt;code&gt;group_by&lt;/code&gt; schools when adding up dollars spent, but that sometimes my data calls what is conceptually a school &lt;code&gt;schools&lt;/code&gt;, &lt;code&gt;school&lt;/code&gt;, &lt;code&gt;location&lt;/code&gt;, &lt;code&gt;cost_center&lt;/code&gt;, &lt;code&gt;Loc.Name&lt;/code&gt;, etc. What I have been doing is storing a set of parameters in a &lt;code&gt;list&lt;/code&gt; that mapped the actual names in my data to consistent names I want to use in my code. Sometimes that comes from using &lt;code&gt;params&lt;/code&gt; in an Rmd file. So the top of my file may say something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;params:
    school: &amp;quot;locations&amp;quot;
    amount: &amp;quot;dollars&amp;quot;
    enrollment: n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my code, I may want to write a chain like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_per_pupil &amp;lt;- . %&amp;gt;%
                    group_by(school) %&amp;gt;%
                    summarize(per_pupil = sum(amount) / n)
pp &amp;lt;- district_data %&amp;gt;%
      create_per_pupil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only my problem is that &lt;code&gt;school&lt;/code&gt; isn&amp;rsquo;t always &lt;code&gt;school&lt;/code&gt;. In this toy case, you could use &lt;code&gt;group_by_(params$school)&lt;/code&gt;, but it&amp;rsquo;s pretty easy to run into limitations with the &lt;code&gt;_&lt;/code&gt; functions in &lt;code&gt;dplyr&lt;/code&gt; when writing functions.&lt;/p&gt;

&lt;p&gt;Using &lt;code&gt;wrapr::let&lt;/code&gt;, I can easily use the code above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;let(alias = params, {
    create_per_pupil &amp;lt;- . %&amp;gt;%
                        group_by(school) %&amp;gt;%
                        summarize(per_pupil = sum(amount)/n)
})

pp &amp;lt;- district_data %&amp;gt;%
      create_per_pupil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The core of &lt;code&gt;wrapr::let&lt;/code&gt; is really scary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;body &amp;lt;- strexpr
for (ni in names(alias)) {
    value &amp;lt;- as.character(alias[[ni]])
    if (ni != value) {
        pattern &amp;lt;- paste0(&amp;quot;\\b&amp;quot;, ni, &amp;quot;\\b&amp;quot;)
        body &amp;lt;- gsub(pattern, value, body)
    }
}
parse(text = body)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically let is holidng onto the code block contained within it, iterating over the list of key-value pairs that are provided, and then runs a &lt;code&gt;gsub&lt;/code&gt; on word boundaries to replace all instances of the list names with their values. Yikes.&lt;/p&gt;

&lt;p&gt;This works, I use it all over, but I have never felt confident about it.&lt;/p&gt;

&lt;h2 id=&#34;the-new-world-of-tidyeval&#34;&gt;The New World of tidyeval&lt;/h2&gt;

&lt;p&gt;The release of dplyr 0.6 along with tidyeval brings wtih it a ton of features to making programming over dplyr functions far better supported. I am going to &lt;a href=&#34;http://dplyr.tidyverse.org/articles/programming.html&#34;&gt;read this page&lt;/a&gt; by Hadley Wickham at least 100 times. There are all kinds of new goodies (&lt;code&gt;!!!&lt;/code&gt; looks amazing).&lt;/p&gt;

&lt;p&gt;So how would I re-write the chain above sans &lt;code&gt;let&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;create_per_pupil &amp;lt;- . %&amp;gt;%
                    group_by(!!sym(school)) %&amp;gt;%
                    summarize(per_pupil = sum(amount)/n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I understand &lt;code&gt;tidyeval&lt;/code&gt;, then this is what&amp;rsquo;s going on.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sym&lt;/code&gt; evaluates &lt;code&gt;school&lt;/code&gt; and makes the result a &lt;code&gt;symbol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;and &lt;code&gt;!!&lt;/code&gt; says, roughly &amp;ldquo;evaluate that symbol now&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This way with &lt;code&gt;params$school&lt;/code&gt; having the value &lt;code&gt;&amp;quot;school_name&amp;quot;&lt;/code&gt;, &lt;code&gt;sym(school)&lt;/code&gt; creates evaulates that to &lt;code&gt;&amp;quot;school_name&amp;quot;&lt;/code&gt; and then makes it an unquoted symbol &lt;code&gt;school_name&lt;/code&gt;. Then &lt;code&gt;!!&lt;/code&gt; tells R &amp;ldquo;You can evaluate this next thing in place as it is.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;I originally wrote this post trying to understand &lt;code&gt;enquo&lt;/code&gt;, but I never got it to work right and it makes no sense to me yet. What&amp;rsquo;s great is that &lt;code&gt;rlang::sym&lt;/code&gt; and &lt;code&gt;rlang::syms&lt;/code&gt; with &lt;code&gt;!!&lt;/code&gt; and &lt;code&gt;!!!&lt;/code&gt; respectively work really well so far. There is definitely less flexibility&amp;ndash; with the full on &lt;code&gt;quosure&lt;/code&gt; stuff you can have very complex evaluations. But I&amp;rsquo;m mostly worried about having very generic names for my data so &lt;code&gt;sym&lt;/code&gt; and &lt;code&gt;syms&lt;/code&gt; seems to work great.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Foreign Keys and assertr</title>
      <link>http://www.json.blog/2017/04/foreign-keys-and-assertr/</link>
      <pubDate>Sun, 02 Apr 2017 21:19:30 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/04/foreign-keys-and-assertr/</guid>
      <description>

&lt;p&gt;I have been fascinated with assertive programming in R since &lt;a href=&#34;http://www.json.blog/2015/07/news-on-assertions-in-r/&#34;&gt;this&lt;/a&gt; from 2015 &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:missing&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:missing&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Tony Fischetti wrote a &lt;a href=&#34;http://www.onthelambda.com/2017/03/20/data-validation-with-the-assertr-package/&#34;&gt;great blog post&lt;/a&gt; to announce &lt;code&gt;assertr&lt;/code&gt; 2.0&amp;rsquo;s release on CRAN that really clarified the package&amp;rsquo;s design.&lt;/p&gt;

&lt;p&gt;UseRs often do crazy things that no sane developer in another language would do. Today I decided to build a way to check foreign key constraints in R to help me learn the &lt;code&gt;assertr&lt;/code&gt; package.&lt;/p&gt;

&lt;h2 id=&#34;what-do-you-mean-foreign-key-constraints&#34;&gt;What do you mean, foreign key constraints?&lt;/h2&gt;

&lt;p&gt;Well, in many ways this is an extension of my &lt;a href=&#34;http://www.json.blog/2016/12/a-gateway-drug-to-purrr/&#34;&gt;last post&lt;/a&gt; on using &lt;code&gt;purrr::reduce&lt;/code&gt;. I have a set of data with codes (like FIPS codes, or user ids, etc) and I want to make sure that all of those codes are &amp;ldquo;real&amp;rdquo; codes (as in I have a defintion for that value). So I may have a FIPS code &lt;code&gt;data.frame&lt;/code&gt; with &lt;code&gt;fips_code&lt;/code&gt; and &lt;code&gt;name&lt;/code&gt; as the columns or a user &lt;code&gt;data.frame&lt;/code&gt; with columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;fname&lt;/code&gt;, &lt;code&gt;lname&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In a database, I might have a foreign key constraint on my table that just has codes so that I could not create a row that uses an &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;code&lt;/code&gt; value or whatever that did not exist in my lookup table. Of course in R, our data is disconnected and non-relational. New users may exist in my dataset that weren&amp;rsquo;t there the last time I downloaded the &lt;code&gt;users&lt;/code&gt; table, for example.&lt;/p&gt;

&lt;h2 id=&#34;ok-so-these-are-just-collections-of-enumerated-values&#34;&gt;Ok, so these are just collections of enumerated values&lt;/h2&gt;

&lt;p&gt;Yup! That&amp;rsquo;s right! In some ways like R&amp;rsquo;s &lt;em&gt;beloved&lt;/em&gt; &lt;code&gt;factors&lt;/code&gt;, I want to have problems when my data contains values that don&amp;rsquo;t have a corresponding row in another &lt;code&gt;data.frame&lt;/code&gt;, just like trying to insert a value into a &lt;code&gt;factor&lt;/code&gt; that isn&amp;rsquo;t an existing level.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;assertr&lt;/code&gt; anticipates just this, with the &lt;code&gt;in_set&lt;/code&gt; helper. This way I can &lt;code&gt;assert&lt;/code&gt; that my data is in a defined set of values or get an error.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;my_df &amp;lt;- data.frame(x = c(0,1,1,2))
assert(my_df, in_set(0,1), x)
# Column &#39;x&#39; violates assertion &#39;in_set(0, 1)&#39; 1 time
#   index value
# 1     4     2
# Error: assertr stopped execution
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;please-don-t-stop&#34;&gt;Please Don&amp;rsquo;t stop()&lt;/h2&gt;

&lt;p&gt;By default, &lt;code&gt;assert&lt;/code&gt; raises an error with an incredibly helpful message. It tells you which column the assertion was on, what the assertion was, how many times that assertion failed, and then returns the column index and value of the failed cases.&lt;/p&gt;

&lt;p&gt;Even better, &lt;code&gt;assert&lt;/code&gt; has an argument for &lt;code&gt;error_fun&lt;/code&gt;, which, combined with some built in functions, can allow for all kinds of fun behavior when an assertion fails. What if, for example, I actually want to collect that error message for later and not have a hard stop if an assertion failed?&lt;/p&gt;

&lt;p&gt;By using &lt;code&gt;error_append&lt;/code&gt;, &lt;code&gt;assert&lt;/code&gt; will return the original &lt;code&gt;data.frame&lt;/code&gt; when there&amp;rsquo;s a failure with a special attribute called &lt;code&gt;assertr_errors&lt;/code&gt; that can be accessed later with all the information about failed assertions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;my_df %&amp;lt;&amp;gt;%
  assert(in_set(0,1), x, error_fun = error_append) %&amp;gt;%
  verify(x == 1, error_fun = error_append)
my_df
#   x
# 1 0
# 2 1
# 3 1
# 4 2
attr(my_df, &#39;assertr_errors&#39;)
# [[1]]
# Column &#39;x&#39; violates assertion &#39;in_set(0, 1)&#39; 1 time
#   index value
# 1     4     2
# 
# [[2]]
# verification [x == 1] failed! (2 failures)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Ok I cheated there folks. I used &lt;code&gt;verify&lt;/code&gt;, a new function from &lt;code&gt;assertr&lt;/code&gt; and a bunch of &lt;code&gt;magrittr&lt;/code&gt; pipes like &lt;code&gt;%&amp;lt;&amp;gt;%&lt;/code&gt;)&lt;/p&gt;

&lt;h2 id=&#34;enough-with-the-toy-examples&#34;&gt;Enough with the toy examples&lt;/h2&gt;

&lt;p&gt;Ok, so here&amp;rsquo;s the code I wrote today. This started as a huge mess I ended up turning into two functions. First &lt;code&gt;is_valid_fk&lt;/code&gt; provides a straight forward way to get &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt; on whether or not all of your codes/ids exist in a lookup &lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;is_valid_fk &amp;lt;- function(data, key, values,
                        error_fun = error_logical,
                        success_fun = success_logical){

  assert_(data, in_set(values), key,
          error_fun = error_fun, success_fun = success_fun)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first argument &lt;code&gt;data&lt;/code&gt; is your &lt;code&gt;data.frame&lt;/code&gt;, the second argument &lt;code&gt;key&lt;/code&gt; is the foreign key column in &lt;code&gt;data&lt;/code&gt;, and &lt;code&gt;values&lt;/code&gt; are all valide values for &lt;code&gt;key&lt;/code&gt;. Defaulting the &lt;code&gt;error_fun&lt;/code&gt; and &lt;code&gt;success_fun&lt;/code&gt; to &lt;code&gt;*_logical&lt;/code&gt; means a single boolean is the expected response.&lt;/p&gt;

&lt;p&gt;But I don&amp;rsquo;t really want to do these one column at a time. I want to check if all of the foreign keys in a table are good to go. I also don&amp;rsquo;t want a boolean, I want to get back all the errors in a useable format. So I wrote &lt;code&gt;all_valid_fk&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take it one bit at a time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;all_valid_fk &amp;lt;- function(data, fk_list, id = &#39;code&#39;) {
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; is the &lt;code&gt;data.frame&lt;/code&gt; we&amp;rsquo;re checking foreign keys in.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fk_list&lt;/code&gt; is a list of &lt;code&gt;data.frames&lt;/code&gt;. Each element is named for the &lt;code&gt;key&lt;/code&gt; that it looks up; each &lt;code&gt;data.frame&lt;/code&gt; contains the valid values for that &lt;code&gt;key&lt;/code&gt; named&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt;, the name of the column in each &lt;code&gt;data.frame&lt;/code&gt; in the list &lt;code&gt;fk_list&lt;/code&gt; that corresponds to the valid &lt;code&gt;keys&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verify(data, do.call(has_all_names, as.list(names(fk_list))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right away, I want to know if my data has all the values my &lt;code&gt;fk_list&lt;/code&gt; says it should. I have to do some &lt;code&gt;do.call&lt;/code&gt; magic because &lt;code&gt;has_all_names&lt;/code&gt; wants something like &lt;code&gt;has_all_names(&#39;this&#39;, &#39;that&#39;, &#39;the_other&#39;)&lt;/code&gt; not &lt;code&gt;has_all_names(c(&#39;this&#39;, &#39;that&#39;, &#39;the_other&#39;)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The next part is where the magic happens.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;accumulated_errors &amp;lt;- map(names(fk_list),
                            ~ is_valid_fk(data,
                                          key = .x,
                                          values = fk_list[[.x]][[id]],
                                          error_fun = error_append,
                                          success_fun = success_continue)) %&amp;gt;%
                        map(attr, &#39;assertr_errors&#39;) %&amp;gt;%
                        reduce(append)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;map&lt;/code&gt;, I am able to call &lt;code&gt;is_valid_fk&lt;/code&gt; on each of the columns in &lt;code&gt;data&lt;/code&gt; that have a corresponding lookup table in &lt;code&gt;fk_list&lt;/code&gt;. The valid values are &lt;code&gt;fk_list[[.x]][[id]]&lt;/code&gt;, where &lt;code&gt;.x&lt;/code&gt; is the name of the &lt;code&gt;data.frame&lt;/code&gt; in &lt;code&gt;fk_list&lt;/code&gt; (which corresponds to the name of the code we&amp;rsquo;re looking up in &lt;code&gt;data&lt;/code&gt; and exists for sure, thanks to that &lt;code&gt;verify&lt;/code&gt; call) and &lt;code&gt;id&lt;/code&gt; is the name of the key in that &lt;code&gt;data.frame&lt;/code&gt; as stated earlier. I&amp;rsquo;ve replaced &lt;code&gt;error_fun&lt;/code&gt; and &lt;code&gt;success_fun&lt;/code&gt; so that the code does not exist &lt;code&gt;map&lt;/code&gt; as soon there are any problems. Instead, the data is returned for each assertion with the error attribute if one exists. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:memory&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:memory&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Immediately, &lt;code&gt;map&lt;/code&gt; is called on the resulting list of &lt;code&gt;data.frame&lt;/code&gt;s to collect the &lt;code&gt;assertr_errors&lt;/code&gt;, which are &lt;code&gt;reduce&lt;/code&gt;d using &lt;code&gt;append&lt;/code&gt; into a flattened list.&lt;/p&gt;

&lt;p&gt;If there are no errors accumulated, &lt;code&gt;accumulated_errors&lt;/code&gt; is &lt;code&gt;NULL&lt;/code&gt;, and the function exits early.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(is.null(accumulated_errors)) return(list())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I could have stopped here and returned all the messages in &lt;code&gt;accumulated_errors&lt;/code&gt;. But I don&amp;rsquo;t like all that text, I want something neater to work with later. The structure I decided on was a list of &lt;code&gt;data.frame&lt;/code&gt;s, with each element named for the column with the failed foreign key assertion and the contents being the index and value that failed the constraint.&lt;/p&gt;

&lt;p&gt;By calling &lt;code&gt;str&lt;/code&gt; on &lt;code&gt;data.frame&lt;/code&gt;s returned by assertion, I was able to see that the &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; tables printed in the failed &lt;code&gt;assert&lt;/code&gt; messages are contained in &lt;code&gt;error_df&lt;/code&gt;. So next I extract each of those &lt;code&gt;data.frame&lt;/code&gt;s into a single list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;reporter &amp;lt;- accumulated_errors %&amp;gt;%
            map(&#39;error_df&#39;) %&amp;gt;%
            map(~ map_df(.x, as.character)) # because factors suck
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m almost done. I have no way of identifying which column created each of those &lt;code&gt;error_df&lt;/code&gt; in &lt;code&gt;reporter&lt;/code&gt;. So to name each element based on the column that failed the foreign key contraint, I have to extract data from the &lt;code&gt;message&lt;/code&gt; attribute. Here&amp;rsquo;s what I came up with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(reporter) &amp;lt;- accumulated_errors %&amp;gt;%
                   map_chr(&#39;message&#39;) %&amp;gt;%
                   gsub(&amp;quot;^Column \&#39;([a-zA-Z]+)\&#39; .*$&amp;quot;, &#39;\\1&#39;, x = .)
reporter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So let&amp;rsquo;s create some fake data and run &lt;code&gt;all_valid_fk&lt;/code&gt; to see the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; df &amp;lt;- data.frame(functions = c(&#39;1001&#39;,&#39;1002&#39;, &#39;3001&#39;, &#39;3002&#39;),
                   objects = c(&#39;100&#39;,&#39;102&#39;, &#39;103&#39;, &#39;139&#39;),
                   actuals = c(10000, 2431, 809, 50000),
                   stringsAsFactors = FALSE)

&amp;gt; chart &amp;lt;- list(functions = data.frame(code = c(&#39;1001&#39;, &#39;1002&#39;, &#39;3001&#39;),
                                       name = c(&#39;Foo&#39;, &#39;Bar&#39;, &#39;Baz&#39;),
                                       stringsAsFactors = FALSE),
                objects =   data.frame(code = c(&#39;100&#39;, &#39;102&#39;, &#39;103&#39;),
                                       name = c(&#39;Mom&#39;, &#39;Dad&#39;, &#39;Baby&#39;),
                                       stringsAsFactors = FALSE))
&amp;gt; all_valid_fk(data = df, fk_list = chart, id = &#39;code&#39;)
$functions
# A tibble: 1 × 2
  index value
  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
1     4  3002

$objects
# A tibble: 1 × 2
  index value
  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
1     4   139
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Beautiful!&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s &lt;code&gt;all_valid_fk&lt;/code&gt; in one big chunk.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;all_valid_fk &amp;lt;- function(data, fk_list, id = &#39;code&#39;) {
  verify(data, do.call(has_all_names, as.list(names(fk_list))))

  accumulated_errors &amp;lt;- map(names(fk_list),
                            ~ is_valid_fk(data,
                                          key = .x,
                                          values = fk_list[[.x]][[id]],
                                          error_fun = error_append,
                                          success_fun = success_continue)) %&amp;gt;%
                        map(attr, &#39;assertr_errors&#39;) %&amp;gt;%
                        reduce(append)

  if(is.null(accumulated_errors)) return(list())

  reporter &amp;lt;- accumulated_errors %&amp;gt;%
              map(&#39;error_df&#39;) %&amp;gt;%
              map(~ map_df(.x, as.character))

  names(reporter) &amp;lt;- accumulated_errors %&amp;gt;%
                     map_chr(&#39;message&#39;) %&amp;gt;%
                     gsub(&#39;Column \&#39;(\\S*?)\&#39;.*$&#39;, &#39;\\1&#39;, x = .)
  reporter
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;My thanks to &lt;a href=&#34;https://jcarroll.com.au&#34;&gt;Jonathan Carroll&lt;/a&gt; who was kind enough to read this post closely and actually tried to run the code. As a result, I&amp;rsquo;ve fixed a couple of typos and now have an improved regex pattern above.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:missing&#34;&gt;I appear to have forgotten to build link post types into my Hugo blog, so the missing link from that post is &lt;a href=&#34;http://4dpiecharts.com/2015/07/03/the-state-of-assertions-in-r/&#34;&gt;here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:missing&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:memory&#34;&gt;I am a little concerned about memory here. Eight assertions would mean, at least briefly, eight copies of the same &lt;code&gt;data.frame&lt;/code&gt; copied here without the need for that actual data. There is probably a better way.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:memory&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Labeling Data with purrr</title>
      <link>http://www.json.blog/2017/03/labeling-data-with-purrr/</link>
      <pubDate>Fri, 03 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/03/labeling-data-with-purrr/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a fun common task. I have a data set that has a bunch of codes like:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Abbr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Alabama&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AL&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;01&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Alaska&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AK&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;02&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Arizona&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AZ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;04&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Arkansas&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AR&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;05&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;California&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;CA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;06&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Colorado&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;CO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;08&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Connecticut&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;CT&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;09&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Delaware&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;DE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;All of your data is labeled with the &lt;code&gt;code&lt;/code&gt; value. In this case, you want to do a &lt;code&gt;join&lt;/code&gt; so that you can use the actual names because it&amp;rsquo;s 2017 and we&amp;rsquo;re not animals.&lt;/p&gt;

&lt;p&gt;But what if your data, like the accounting data we deal with at &lt;a href=&#34;http://www.allovue.com&#34;&gt;Allovue&lt;/a&gt;, has lots of code fields. You probably either have one table that contains all of the look ups in &amp;ldquo;long&amp;rdquo; format, where there is a column that represents which column in your data the code is for like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;fips&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Alabama&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;fips&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Alaska&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Alternatively, you may have a lookup table per data element (so one called fips, one called account, one called function, etc).&lt;/p&gt;

&lt;p&gt;I bet most folks do the following in this scenario:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;account &amp;lt;- left_join(account, account_lookup)
account &amp;lt;- left_join(account, fips)

## Maybe this instead ##
account %&amp;lt;&amp;gt;%
  left_join(account_lookup) %&amp;gt;%
  left_join(fips)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to encourage you to do this a little different using &lt;code&gt;purrr&lt;/code&gt;. Here&amp;rsquo;s some annotated code that uses &lt;code&gt;reduce_right&lt;/code&gt; to make magic.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Load a directory of .csv files that has each of the lookup tables
lookups &amp;lt;- map(dir(&#39;data/lookups&#39;), read.csv, stringsAsFactors = FALSE)
# Alternatively if you have a single lookup table with code_type as your
# data attribute you&#39;re looking up
# lookups &amp;lt;- split(lookups, code_type)
lookups$real_data &amp;lt;- read.csv(&#39;data/real_data.csv&#39;, 
                              stringsAsFactors = FALSE)
real_data &amp;lt;- reduce_right(lookups, left_join)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Boom, now you went from data with attributes like &lt;code&gt;funds_code&lt;/code&gt;, &lt;code&gt;function_code&lt;/code&gt;, &lt;code&gt;state_code&lt;/code&gt; to data that also has &lt;code&gt;funds_name&lt;/code&gt;, &lt;code&gt;function_name&lt;/code&gt;, &lt;code&gt;state_name&lt;/code&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:namingconventions&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:namingconventions&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. What&amp;rsquo;s great is that this same code can be reused no matter how many fields require a hookup. I&amp;rsquo;m oftent dealing with accounting data where the accounts are defined by a different number of data fields, but my code doesn&amp;rsquo;t care at all.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:namingconventions&#34;&gt;My recommendation is to use consistent naming conventions like &lt;code&gt;_code&lt;/code&gt; and &lt;code&gt;_name&lt;/code&gt; so that knowing how to do the lookups is really straightforward. This is not unlike the convention with Microsoft SQL where the primary key of a table is named &lt;code&gt;Id&lt;/code&gt; and a foreign key to that table is named &lt;code&gt;TableNameId&lt;/code&gt;. Anything that helps you figure out how to put things together without thinking is worth it.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:namingconventions&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Contributing to vegalite</title>
      <link>http://www.json.blog/2017/03/contributing-to-vegalite/</link>
      <pubDate>Thu, 02 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/03/contributing-to-vegalite/</guid>
      <description>&lt;p&gt;One of my goals for 2017 is to contribute more to the R open source community. At the beginning of last year, I spent a little time helping to refactor &lt;a href=&#34;https://www.github.com/leeper/rio&#34;&gt;rio&lt;/a&gt;. It was one of the more rewarding things I did in all of 2016. It wasn&amp;rsquo;t a ton of work, and I feel like I gained a lot of confidence in writing R packages and using S3 methods. I wrote code that R users download and use thousands of times a month.&lt;/p&gt;

&lt;p&gt;I have been on the lookout for a Javascript powered interactive charting library since &lt;code&gt;ggvis&lt;/code&gt; was announced in 2014. But &lt;code&gt;ggvis&lt;/code&gt; seems to have stalled out in favor of other projects (for now) and the evolution of &lt;code&gt;rCharts&lt;/code&gt; into &lt;code&gt;htmlwidgets&lt;/code&gt; left me feeling like there were far too many options and no clear choices.&lt;/p&gt;

&lt;p&gt;What I was looking for was a plotting library to make clean, attractive graphics with tool tips that came with clear documentation and virtually no knowledge of Javascript required. Frankly, all of the &lt;code&gt;htmlwidgets&lt;/code&gt; stuff was very intimidating. From my vantage point skimming blog posts and watching stuff come by on Twitter, &lt;code&gt;htmlwidgets&lt;/code&gt;-based projects all felt very much directed at Javascript polyglots.&lt;/p&gt;

&lt;p&gt;Vega and Vega-Lite had a lot of the qualities I sought in a plotting library. Reading and writing JSON is very accessible compared to learning Javascript, especially with R&amp;rsquo;s excellent translation from lists to JSON. And although I know almost no Javascript, I found in both Vega and Vega-Lite easy to understand documents that felt a lot like building grammar of graphics &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:thegg&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:thegg&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; plots.&lt;/p&gt;

&lt;p&gt;So I decided to take the plunge&amp;ndash; there was a &lt;code&gt;vegalite&lt;/code&gt; &lt;a href=&#34;https://github.com/hrbrmstr/vegalite&#34;&gt;package&lt;/a&gt; and the examples didn&amp;rsquo;t look so bad. It was time to use my first &lt;code&gt;htmlwidgets&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;Things went great. I had some simple data and I wanted to make a bar chart. I wrote:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vegalite() %&amp;gt;%
add_data(my_df) %&amp;gt;%
encode_x(&#39;schools&#39;, type = &#39;nominal&#39;) %&amp;gt;%
encode_y(&#39;per_pupil&#39;, type = &#39;quantitative&#39;) %&amp;gt;%
mark_bar()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A bar chart was made! But then I wanted to use the font Lato, which is what we use at &lt;a href=&#34;http://www.allovue.com&#34;&gt;Allovue&lt;/a&gt;. No worries, Vega-Lite has a property called &lt;code&gt;titleFont&lt;/code&gt; for axes. So I went to do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vegalite() %&amp;gt;%
add_data(my_df) %&amp;gt;%
encode_x(&#39;schools&#39;, type = &#39;nominal&#39;) %&amp;gt;%
encode_y(&#39;per_pupil&#39;, type = &#39;quantitative&#39;) %&amp;gt;%
mark_bar() %&amp;gt;%
axis_x(titleFont = &#39;Lato&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bummer. It didn&amp;rsquo;t work. I almost stopped there, experiment over. But then I remembered my goal and I thought, maybe I need to learn to contribute to a package that is an &lt;code&gt;htmlwidget&lt;/code&gt; and not simply use an &lt;code&gt;htmlwidget&lt;/code&gt;-based package. I should at least &lt;em&gt;look&lt;/em&gt; at the code.&lt;/p&gt;

&lt;p&gt;What I found surprised me. Under the hood, all the R package does is build up lists. It makes so much sense&amp;ndash; pass JSON to Javascript to process and do what&amp;rsquo;s needed.&lt;/p&gt;

&lt;p&gt;So it turned out, &lt;code&gt;vegalite&lt;/code&gt; for R was a bit behind the current version of &lt;code&gt;vegalite&lt;/code&gt; and didn&amp;rsquo;t have the &lt;code&gt;titleFont&lt;/code&gt; property yet. And with that, I made my &lt;a href=&#34;https://github.com/hrbrmstr/vegalite/commit/8f4d4db057985bac4fc8c5743780b4746dd56c56&#34;&gt;first commit&lt;/a&gt;. All I had to do was update the function definition and add the new arguments to the axis data like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (!is.null(titleFont))    vl$x$encoding[[chnl]]$axis$titleFont &amp;lt;- titleFont
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But why stop there? I wanted to update all of &lt;code&gt;vegalite&lt;/code&gt; to use the newest available arguments. Doing so looked like a huge pain though. The original package author made these great functions like &lt;code&gt;axis_x&lt;/code&gt; and &lt;code&gt;axis_y&lt;/code&gt;. They both had the same arguments, the only difference was the &amp;ldquo;channel&amp;rdquo; was preset as &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; based on which function was called. Problem was that all of the arguments, all of the assignments, and all of the documentation had to be copied twice. It was worse with &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; which had many, many functions that are similar or identical in their &amp;ldquo;signature&amp;rdquo;. No wonder the package was missing so many Vega-Lite features&amp;ndash; they were a total pain to add.&lt;/p&gt;

&lt;p&gt;So as a final step, I decided I would do a light refactor across the whole package. In each of the core functions, like &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;axis&lt;/code&gt;, I would write a single generic function like &lt;code&gt;encode_vl()&lt;/code&gt; that would hold all of the possible arguments for the &lt;a href=&#34;https://vega.github.io/vega-lite/docs/encoding.html&#34;&gt;encoding portion&lt;/a&gt; of Vega-Lite. Then the specific functions like &lt;code&gt;encode_x&lt;/code&gt; could become wrapper functions that internally call &lt;code&gt;encode_vl&lt;/code&gt; like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;encode_x &amp;lt;- function(vl, ...) {
  vl &amp;lt;- encode_vl(vl, chnl = &amp;quot;x&amp;quot;, ...)
  vl
}

encode_y &amp;lt;- function(vl, ...) {
  vl &amp;lt;- encode_vl(vl, chnl =&amp;quot;y&amp;quot;, ...)
  vl
}

encode_color &amp;lt;- function(vl, ...) {
  vl &amp;lt;- encode_vl(vl, chnl = &amp;quot;color&amp;quot;, ...)
  vl
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, in order to update the documentation and the arguments for &lt;code&gt;encoding&lt;/code&gt;, I just have to update the &lt;code&gt;encode_vl&lt;/code&gt; function. It&amp;rsquo;s a really nice demonstration, in my opinion, of the power of R&amp;rsquo;s &lt;code&gt;...&lt;/code&gt; syntax. All of the wrapper functions can just pass whatever additional arguments the caller wants to &lt;code&gt;encode_vl&lt;/code&gt; without having to explicitly list them each time.&lt;/p&gt;

&lt;p&gt;This greatly reduced duplication in the code and made it far easier to update &lt;code&gt;vegalite&lt;/code&gt; to the newest version of Vega-Lite, which I also decided to do.&lt;/p&gt;

&lt;p&gt;Now Vega-Lite itself is embarking on a 2.0 release that I have a feeling will have some pretty big changes in store. I&amp;rsquo;m not sure if I&amp;rsquo;ll be the one to update &lt;code&gt;vegalite&lt;/code&gt;&amp;ndash; in the end, I think that Vega-Lite is too simple for the visualizations I need to do&amp;ndash; but I am certain whoever does the update will have a much easier go of it now than they would have just over a month ago.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/hrbrmstr&#34;&gt;Bob Rudis&lt;/a&gt; for making &lt;code&gt;vegalite&lt;/code&gt; and giving me free range after a couple of commits to go hog-wild on his package!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:thegg&#34;&gt;The &lt;code&gt;gg&lt;/code&gt; in &lt;code&gt;ggplot2&lt;/code&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:thegg&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My confrontation with the mainstream</title>
      <link>http://www.json.blog/2017/02/my-confrontation-with-the-mainstream/</link>
      <pubDate>Mon, 27 Feb 2017 13:02:58 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/02/my-confrontation-with-the-mainstream/</guid>
      <description>&lt;p&gt;You can check my &lt;a href=&#34;https://www.goodreads.com/user/show/5380958-jason&#34;&gt;Goodreads profile&lt;/a&gt;. I love science fiction and fantasy. And I know in 2017 and everyone has already observed the dominance of &amp;ldquo;geek culture&amp;rdquo;, with the dominance of Disney properties from Marvel and now Star Wars. Hell, &lt;a href=&#34;http://io9.gizmodo.com/suicide-squad-is-now-an-oscar-winning-movie-1792769922&#34;&gt;Suicide Squad won a goddamn Oscar&lt;/a&gt;.&lt;/p&gt;

&lt;div style= &#34;float:right;&#34;&gt;
  &lt;a href = &#34;#powell_best_selling&#34;&gt;   
      &lt;figure&gt;
          &lt;img src = &#34;/img/powell_best_selling.jpg&#34; height = &#34;800&#34;&gt;
          &lt;figcaption&gt; Powell&#39;s Best Selling Fiction, 2017-02-26&lt;/figcaption&gt;
      &lt;/figure&gt;
  &lt;/a&gt;
  &lt;a href=&#34;#_&#34; class=&#34;lightbox&#34; id = &#34;powell_best_selling&#34;&gt;
      &lt;figure&gt;
          &lt;img src = &#34;/img/powell_best_selling.jpg&#34;&gt;
          &lt;figcaption&gt;Powell&#39;s Best Selling Fiction, 2017-02-26&lt;/figcaption&gt;
      &lt;/figure&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;But I never felt like SFF was all that mainstream. SyFy might have made (and renewed) a TV series based on &lt;em&gt;The Magicians&lt;/em&gt;, but I still feel like the disaffected entitled shit that held onto his love of genre fiction too long when I crawl into bed and hide in speculative fiction (thank you Quentin, for so completely capturing what a shit I was at 14).&lt;/p&gt;

&lt;p&gt;Yesterday, I was confronted with the reality of SFF going mainstream at &lt;a href=&#34;http://www.powells.com&#34;&gt;Powell&amp;rsquo;s City of Books&lt;/a&gt;. I was fully unprepared to see the contents of their Best Selling Fiction shelf.&lt;/p&gt;

&lt;p&gt;By my count, at least 16 of the top 42 are SFF. &lt;em&gt;The Name of the Wind&lt;/em&gt;, &lt;em&gt;The Left Hand of Darkness&lt;/em&gt;, &lt;em&gt;The Fifth Season&lt;/em&gt;, &lt;em&gt;2312&lt;/em&gt;, and &lt;em&gt;Uprooted&lt;/em&gt; are some of the best books I&amp;rsquo;ve ready in the last four or five years. To think of these books as best sellers when they don&amp;rsquo;t have a TV show coming out (like &lt;em&gt;American Gods&lt;/em&gt;, &lt;em&gt;The Handmaid&amp;rsquo;s Tale&lt;/em&gt;, &lt;em&gt;The Man in the High Castle&lt;/em&gt;, and &lt;em&gt;The Magicians&lt;/em&gt;) and aren&amp;rsquo;t assigned in high school classrooms (&lt;em&gt;1984&lt;/em&gt;, &lt;em&gt;Slaughterhouse-Five&lt;/em&gt;) is just shocking. In my mind, these aren&amp;rsquo;t best sellers, they&amp;rsquo;re tiny nods between myself and other quiet bookshoppers that we are kin.&lt;/p&gt;

&lt;p&gt;I am not sad though. I am thrilled. I want to live in a world where I can just assume acquaintances are reading &lt;em&gt;The Fifth Season&lt;/em&gt; and &lt;em&gt;Uprooted&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Siri Won&#39;t Catch Alexa</title>
      <link>http://www.json.blog/2017/01/siri-wont-catch-alexa/</link>
      <pubDate>Sat, 14 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2017/01/siri-wont-catch-alexa/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I just got an Amazon Echo and surprisingly, I really love it.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In one form or another, &lt;a href=&#34;https://www.macstories.net/reviews/astra-brings-amazons-alexa-voice-assistant-to-the-iphone/&#34;&gt;this story&lt;/a&gt; has repeated &lt;a href=&#34;https://sixcolors.com/post/2015/12/my-favorite-gadget-of-2015-the-amazon-echo/&#34;&gt;again&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/marcoarment/status/715611484567511040&#34;&gt;again&lt;/a&gt; across the internet. So while the recent headline seemed to be &lt;a href=&#34;http://www.theverge.com/ces/2017/1/4/14169550/amazon-alexa-so-many-things-at-ces-2017&#34;&gt;&amp;ldquo;Amazon&amp;rsquo;s Alexa is everywhere at CES 2017&amp;rdquo;&lt;/a&gt;, it really feels like this year was the Amazon Alexa year.&lt;/p&gt;

&lt;p&gt;I have an Amazon Echo. I bought around a year ago during a sale as the buzz seemed to have peaked &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:peaked&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:peaked&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. My experience with the Amazon Echo has mostly been &amp;ldquo;I don&amp;rsquo;t get it.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I think I’m using the Echo wrong because despite what the whole internet is saying, it’s not doing much Siri doesn’t do.&lt;/p&gt;&amp;mdash; Jason Becker (@jsonbecker) &lt;a href=&#34;https://twitter.com/jsonbecker/status/736174308129705985?ref_src=twsrc%5Etfw&#34;&gt;May 27, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Month 3 of Echo ownership. I still use it for practically nothing.&lt;/p&gt;&amp;mdash; Jason Becker (@jsonbecker) &lt;a href=&#34;https://twitter.com/jsonbecker/status/747087351084683264?ref_src=twsrc%5Etfw&#34;&gt;June 26, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Apple Watch is doomed!&lt;br&gt;&lt;br&gt;But even my tepid explanations of what I like about mine are more complete than anything I could say about Echo&lt;/p&gt;&amp;mdash; Jason Becker (@jsonbecker) &lt;a href=&#34;https://twitter.com/jsonbecker/status/747087927247777793?ref_src=twsrc%5Etfw&#34;&gt;June 26, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;The Echo was kind of fun with Philip&amp;rsquo;s Hue lights or for a timer or unit conversion in the kitchen from time to time, but not much else. I was not much of a Siri user, and it turned out I was not much of an Amazon Echo user.&lt;/p&gt;

&lt;p&gt;But I just bought and Echo Dot.&lt;/p&gt;

&lt;h2 id=&#34;homekit-and-siri-can-t-compete&#34;&gt;HomeKit and Siri Can&amp;rsquo;t Compete&lt;/h2&gt;

&lt;p&gt;Siri and HomeKit should be a match made in heaven, but if I say &amp;ldquo;Hey Siri, turn off the bedroom lights,&amp;rdquo; the most common response is &amp;ldquo;Sorry, I cannot find device &amp;lsquo;bedroom lights&amp;rsquo;.&amp;rdquo; I then repeat the same command and the lights go off. This literally has happened once in almost a year of Echo ownership, but it happened nearly every time with Siri.&lt;/p&gt;

&lt;p&gt;Apple is miserable at sharing &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:sharing&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:sharing&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and that means that even if Siri worked perfectly, HomeKit is built on a bad model.&lt;/p&gt;

&lt;p&gt;My Philips Hue lights are HomeKit compatible. I use Family Sharing so that my partner Elsa can have access to all the apps I buy. Yet it took months to get the email to invite her to my home to send and work. And really, why should I have to invite someone to anything to turn on the lights in my home? Apple knows all about proximity, with it&amp;rsquo;s &lt;a href=&#34;http://appleinsider.com/articles/13/12/06/first-look-using-ibeacon-location-awareness-at-an-apple-store&#34;&gt;awesome use of iBeacons in its own stores&lt;/a&gt;. If being within reach of my light switches or thermostats were enough security to control my devices before, why is Apple making it so hard for people to access them via HomeKit? &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:security&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:security&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;a-better-model-for-homekit&#34;&gt;A Better Model for HomeKit&lt;/h2&gt;

&lt;p&gt;HomeKit&amp;rsquo;s insistence that all devices have the same security profile and complex approval has meant that devices are rarely HomeKit compatible while everything is compatible with Alexa&amp;rsquo;s simple skills program.&lt;/p&gt;

&lt;p&gt;Imagine if the HomeKit app and excellent Control Center interface
&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:controlcenter&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:controlcenter&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; existed at the launch of HomeKit. Imagine if instead of a complex encryption setup that required new hardware, Apple had tiered security requirements, where door locks and video surveillance lay on one side with heavy security, but lights and thermostats lay on the other. Imagine if HomeKit sharing was a simple check box interface that the primary account with Family Sharing could click on and off. Imagine if controlling low security profile devices with Siri worked for anyone within a certain proximity using iBeacons.&lt;/p&gt;

&lt;p&gt;This is a world where Apple&amp;rsquo;s Phil Schiller &lt;a href=&#34;https://backchannel.com/phil-schiller-on-iphones-launch-how-it-changed-apple-and-why-it-will-keep-going-for-50-years-e4412ad2c8f5#.8jw6uh7fe&#34;&gt;is right&lt;/a&gt; that &amp;ldquo;having my iPhone with me&amp;rdquo; could provide a better experience than a device designed to live in one place.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s not the product we have. And even if Apple gets into the &amp;ldquo;tower with microphones plugged into a wall&amp;rdquo; game, I don&amp;rsquo;t see them producing an Echo Dot like product that makes sure your voice assistant is everywhere. My iPhone might be with me. I may be able to turn on and off the lights with my voice. But without something like iBeacons in the picture, if someone comes to stay in my guest room they&amp;rsquo;re back to using the switch on the wall. If a family member uses Android, they are out of luck. If I have a child under the age where a cell phone is appropriate, they are back to living in a dumb home. The inexpensive Echo Dot means you can sprinkle Alexa everywhere the devices you want to control are for anyone to interact with.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:privilege&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:privilege&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; Apple doesn&amp;rsquo;t do inexpensive well.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not sure they can resolve the product decisions around HomeKit that make it less appealing to hardware manufacturers. Worse, some of Alexa&amp;rsquo;s best skills are entirely software. Alexa&amp;rsquo;s skills can seemingly shoot off requests to API endpoints all over the place. So instead of needing to buy a Logitech Harmony Hub with complex encryption and specialized SiriKit/HomeKit skills, and tight integration, my existing Harmony Hub that has an API used by Logitech&amp;rsquo;s own application supports Alexa skills. An Alexa skill can be built in a day. Apple is allergic to doing simple services like this well, even though the entire web runs on them.&lt;/p&gt;

&lt;h2 id=&#34;my-dot&#34;&gt;My Dot&lt;/h2&gt;

&lt;p&gt;Our new bedroom in Baltimore does not have recessed lighting much like our old bedroom. We&amp;rsquo;re using one of those inexpensive, black tower lamps in the bedroom. I don&amp;rsquo;t have a switch for the outlets in there. Philips doesn&amp;rsquo;t make any Hue bulbs that provides enough light to light the room with one lamp.&lt;/p&gt;

&lt;p&gt;I needed an instant way to get the light on and off. That&amp;rsquo;s when I remembered I had an old WeMo bought from before the days of HomeKit. I used that WeMo to have a simple nightly schedule (turn some lights on at sundown and off at midnight each night) and never really thought about it. The WeMo was perfect, and lo and behold, it works with Alexa. Our Echo is a bit far from the bedroom though, and I don&amp;rsquo;t want to shout across the house to turn off the lights.&lt;/p&gt;

&lt;p&gt;Not only was the inexpensive Echo Dot perfectly for sprinkling a little voice in the bedroom, it also meant our master bathroom can have Hue lights that are controlled with voice again. And, now I have a way to get Spotify onto my Kanto YU5 speakers in the bedroom without fussing with Bluetooth&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:dumpsterfire&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:dumpsterfire&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; from my phone by just connecting an &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;&amp;rdquo; phono plug in permanently.&lt;/p&gt;

&lt;p&gt;Now we say &amp;ldquo;Alexa, turn on the bedroom light&amp;rdquo; and &amp;ldquo;Alexa, play Jazz for Sleep&amp;rdquo;. It&amp;rsquo;s great. It always works. If we had a guest bedroom with the same setup, anyone who stayed there would be able to use it just as easily. No wonder why the Wynn is &lt;a href=&#34;http://www.cnbc.com/2016/12/14/wynn-las-vegas-to-add-amazon-alexa-to-all-hotel-rooms.html&#34;&gt;putting Amazon Echo in their hotel&lt;/a&gt;. Apple literally can&amp;rsquo;t do that.&lt;/p&gt;

&lt;h2 id=&#34;whither-voice-control&#34;&gt;Whither Voice Control&lt;/h2&gt;

&lt;p&gt;Amazon, Apple, and Google seemed locked in a battle over voice &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:microsoft&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:microsoft&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. I can think of four main times I want to use voice:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Walking with headphones&lt;/li&gt;
&lt;li&gt;Driving in the car&lt;/li&gt;
&lt;li&gt;Cooking in the kitchen&lt;/li&gt;
&lt;li&gt;Sharing an interface to shared devices&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For Phil Schiller, and by extension, Apple, the killer feature of Siri is you always have it. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:watch&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:watch&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; In a &lt;a href=&#34;https://backchannel.com/phil-schiller-on-iphones-launch-how-it-changed-apple-and-why-it-will-keep-going-for-50-years-e4412ad2c8f5#.8jw6uh7fe&#34;&gt;recent interview&lt;/a&gt;, Schiller is quoted as saying:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Personally, I still think the best intelligent assistant is the one that’s with you all the time. Having my iPhone with me as the thing I speak to is better than something stuck in my kitchen or on a wall somewhere.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Apple AirPods are all about maximizing (1). Siri works pretty well when I&amp;rsquo;m out with the dogs and need a quick bit of information or to make a call. But the truth is, I don&amp;rsquo;t really need to learn a lot from Siri on those dog walks. When I&amp;rsquo;m out walking, Siri is better than pulling out my phone, but once I&amp;rsquo;ve got a podcast or music going I don&amp;rsquo;t really need anything from Siri in those moments.&lt;/p&gt;

&lt;p&gt;Driving is another great context for voice control. I can&amp;rsquo;t look much at a screen and shouldn&amp;rsquo;t anyway. Ignoring CarPlay, Apple&amp;rsquo;s real move here is Bluetooth interfaces, which places Siri in most cars. But again, what is my real use for voice control in this scenario? Reading SMS and iMessages makes for a cool demo, but not really something I need. Getting directions to a location by name is probably the best use here, but Apple&amp;rsquo;s location database is shit for this. Plus, most of the time I choose where I am going when I get in the car, when I can still use my screen and would prefer to. The most important use of voice control in the car is calling a contact, which Voice Control, the on-device precursor to Siri, did just fine. &lt;a href=&#34;http://www.geekwire.com/2016/ford-working-on-amazon-echo-integration-connecting-cars-to-homes/&#34;&gt;And now Alexa is entering the car space too&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While cooking, it is great to be able to set timers, convert measurement units, and change music or podcast while my hands are occupied. This is why so many people place their Amazon Echo in the kitchen&amp;ndash; it works great for these simple task. &amp;ldquo;Hey Siri&amp;rdquo; and a Bluetooth speaker is a terrible solution in comparison. In fact, one thing that the Amazon Echo has done is cause me to wear my headphones less while cooking or doing the dishes, since the Amazon Echo works better and doesn&amp;rsquo;t mean I can&amp;rsquo;t hear Elsa in the other room. This isn&amp;rsquo;t a killer feature though. Early adopters may be all about the $180 kitchen timer with a modest speaker, but the Echo won&amp;rsquo;t be a mass product if this is its best value proposition.&lt;/p&gt;

&lt;h2 id=&#34;shared-interface-to-shared-devices&#34;&gt;Shared Interface to Shared Devices&lt;/h2&gt;

&lt;p&gt;There is a reason why home automation is where the Echo shines. Our homes are full of technology: light switches, appliances, televisions and all the boxes that plug in them, and everyone who enters the home has cause to control them. We expect that basically all home technology is reasonably transparent to anyone inside. Everyone knows how to lock and unlock doors, turn on the TV, turn on and off lights, or adjust the thermostat. Home automation has long been a geek honey pot that angers every cohabitant and visitor, but voice control offers an interface as easy and common as a light switch.&lt;/p&gt;

&lt;p&gt;Home automation is the early adopter use case that reveals how and why voice control is a compelling user interface. Turning on the bedroom lights means saying &amp;ldquo;Alexa, turn on the bedroom lights.&amp;rdquo; There is no pause for Siri to be listening. There is no taking out my phone or lifting up my watch. There is no invite process. There is no worrying about guests being confused. Anyone inside my home has always been able to hit a light switch. Anyone inside my home can say &amp;ldquo;Alexa, turn on the living room lights.&amp;rdquo; That&amp;rsquo;s why Apple erred by not having a lower security, proximity based way to control HomeKit devices.&lt;/p&gt;

&lt;p&gt;Voice control is great because it provides a shared interface to devices that are shared by multiple people. Computers, smartphones, and really most screen-based interfaces that we use are the aberration, pushing toward and suggesting that technology is becoming more personal. The world we actually live in is filled with artifacts that are communal, and as computer and information technology grow to infuse into the material technologies of our lives, we need communal interfaces. Amazon is well positioned to be a part of this future, but I don&amp;rsquo;t think Apple has a shot with its existing product strategy.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:peaked&#34;&gt;It hadn&amp;rsquo;t.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:peaked&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:sharing&#34;&gt;We still don&amp;rsquo;t have multi-user iPads or iPhones. I have a new AppleTV, but all the TV app recommendations don&amp;rsquo;t work because two people watch TV. Unlike Netflix, we can&amp;rsquo;t have separate profiles. And the Apple Watch is billed as &lt;a href=&#34;http://www.apple.com/pr/library/2014/09/09Apple-Unveils-Apple-Watch-Apples-Most-Personal-Device-Ever.html&#34;&gt;their most personal device yet&lt;/a&gt;. Where Amazon moves into the world of ever-present, open communal interfaces, Apple is looking toward individual, private worlds.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:sharing&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:security&#34;&gt;Ok, here comes the critiques about how HomeKit can be used to open door locks or activate video surveillance, etc. Great&amp;ndash; those are cool uses of technology that also have mostly proximity based security but fine, I can see a case for heavy encryption and complex sharing setups for those devices. But the truth is, most of the &lt;a href=&#34;https://twitter.com/internetofshit&#34;&gt;internet of things&lt;/a&gt; aren&amp;rsquo;t these kinds of devices. A good product would easily scale to different security profiles.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:security&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:controlcenter&#34;&gt;An unconscionable amount of the time I see &amp;ldquo;No Response&amp;rdquo; in Control Center under my devices. Worse, I have to sit and wait for Apple to realize those devices are there because eventually they pop on. Instant interfaces matter, and they matter even more when trying to replace a light switch.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:controlcenter&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:privilege&#34;&gt;There&amp;rsquo;s probably a good critique about privilege here, assuming that you have multiple rooms that would need a separate assistant device. But I would like to remind you that we&amp;rsquo;re talking about spending hundreds of dollars on cylinders plugged into walls that you talk to to control things that cost 4x their traditional counterparts. For the foreseeable future, we are addressing a market of rich people and this technology will succeed or fail there long before we get to ubiquity. Plus, who cares what Apple has to say about any of this if we&amp;rsquo;re not talking about rich people? Apple&amp;rsquo;s market is rich people and that isn&amp;rsquo;t going to change. Affordable luxury is the brand and target, where affordable in the global scheme means fairly well off.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:privilege&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:dumpsterfire&#34;&gt;Bluetooth is a dumpster fire when you have two phones, two sets of wireless headphones, a Bluetooth speaker in the bathroom, a Bluetooth speaker in the bedroom, and a shared car with Bluetooth. All of these things will choose at various times to ~conveniently~ connect to whatever phone they want if you&amp;rsquo;re not diligent about powering things down all the time. Bluetooth audio is a shit experience.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:dumpsterfire&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:microsoft&#34;&gt;Cortana isn&amp;rsquo;t anywhere that matters, so it doesn&amp;rsquo;t matter yet.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:microsoft&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:watch&#34;&gt;Apple Watch is about extending Siri wherever you are, but I don&amp;rsquo;t use Siri on my Watch much, because it&amp;rsquo;s not great in any of those four contexts. If I can raise my hand I have hands and I&amp;rsquo;d rather use my phone.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:watch&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Gateway Drug to purrr</title>
      <link>http://www.json.blog/2016/12/a-gateway-drug-to-purrr/</link>
      <pubDate>Thu, 29 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2016/12/a-gateway-drug-to-purrr/</guid>
      <description>&lt;p&gt;A lot of the data I work with uses numeric codes rather than text to describe features of each record. For example, financial data often has a fund code that represents the account&amp;rsquo;s source of dollars and an object code that signals what is bought (e.g. salaries, benefits, supplies). This is a little like the &lt;code&gt;factor&lt;/code&gt; data type in &lt;code&gt;R&lt;/code&gt;, which to the frustration of many modern analysts is internally an integer that mapped to a character label (which is a level) with a fixed number of possible values.&lt;/p&gt;

&lt;p&gt;I am often looking at data stored like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;fund_code&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;object_code&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;debit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;credit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2121&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2122&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;with the labels stored in another set of tables:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;fund_code&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;fund_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;General&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;object_code&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;object_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;2121&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Social Security&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;2122&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Life Insurance&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Before &lt;code&gt;purrr&lt;/code&gt;, I might have done a series of &lt;code&gt;dplyr::left_join&lt;/code&gt; or &lt;code&gt;merge&lt;/code&gt; to combine these data sets and get the labels in the same &lt;code&gt;data.frame&lt;/code&gt; as my data.&lt;/p&gt;

&lt;p&gt;But no longer!&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I just used purrr:reduce_right with left_join and cackled with joy. &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jason Becker (@jsonbecker) &lt;a href=&#34;https://twitter.com/jsonbecker/status/813793775260733441?ref_src=twsrc%5Etfw&#34;&gt;December 27, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Now, I can just create a &lt;code&gt;list&lt;/code&gt;, add all the data to it, and use &lt;code&gt;purrr:reduce&lt;/code&gt; to bring the data together. Incredibly convenient when up to 9 codes might exist for a single record!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Assume each code-name pairing is in a CSV file in a directory
data_codes &amp;lt;- lapply(dir(&#39;codes/are/here/&#39;, full.names = TRUE ), 
                     readr::read_csv)
data_codes$transactions &amp;lt;- readr::read_csv(&#39;my_main_data_table.csv&#39;)
transactions &amp;lt;- purrr:reduce_right(data_codes, dplyr::left_join)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introducing json.blog</title>
      <link>http://www.json.blog/2016/12/introducing-jsonblog/</link>
      <pubDate>Wed, 28 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2016/12/introducing-jsonblog/</guid>
      <description>&lt;p&gt;I have written a lot on the internet. This isn&amp;rsquo;t a surprise, I&amp;rsquo;ve been here since the mid-90s. But the truth is, most of what I write on the internet doesn&amp;rsquo;t make me proud. It hasn&amp;rsquo;t made the world any better. It certainly hasn&amp;rsquo;t made me feel any better. Most of this terrible writing is easy to associate with me, because a long time ago, I chose to use my real name as my online identity. Using my real name was supposed to make sure that I would stand by what I said, but the truth is that I am not always my better self on internet forums, Facebook, Twitter, or other places I get to write.&lt;/p&gt;

&lt;p&gt;My personal blog is a bit different. The writing I&amp;rsquo;ve done over the years at my own domains has been&amp;hellip; less embarrassing. I don&amp;rsquo;t mean to say that the quality of the writing is any better (it&amp;rsquo;s not); it&amp;rsquo;s just that the extra thought involved in opening up a text editor, writing something in Markdown, and taking the steps to post it has resulted in fewer emotional tirades. I do a much better job of deleting or never completing posts on my blog than I ever did writing somewhere someone else owned. It&amp;rsquo;s too bad the audience here is much smaller and harder to come by.&lt;/p&gt;

&lt;p&gt;My blog has always been a testing ground. It&amp;rsquo;s where I&amp;rsquo;ve learned how to use Wordpress, Pelican, and now Hugo. It&amp;rsquo;s been a place to think about templating, structure, CSS, shared hosting, Github pages, server management, nginx and the like. This is where I try different types of writing like link posts, lists, professional-ish informational posts, public versions of notes to myself, images, and more. This blog hasn&amp;rsquo;t had a topic or a format. I&amp;rsquo;m not convinced it ever will. For me, a self-hosted blog is meant to be a personal lab bench.&lt;/p&gt;

&lt;p&gt;I hope today I am starting what I consider to be the final version of this blog. I feel confident in the domain and name. I feel comfortable with Hugo and the flexiblility of my fully custom theme. I feel great about not having comments.&lt;/p&gt;

&lt;p&gt;The look and content will change many times again, but I feel good that from here forward I&amp;rsquo;ll be using and evolving &lt;a href=&#34;http://json.blog&#34;&gt;json.blog&lt;/a&gt;. This is my home on the web.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our New Home</title>
      <link>http://www.json.blog/image/our-new-home/</link>
      <pubDate>Sun, 11 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/image/our-new-home/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Strange Occurrence</title>
      <link>http://www.json.blog/image/a-strange-occurrence/</link>
      <pubDate>Sat, 26 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/image/a-strange-occurrence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Songs in the Key of Allovue</title>
      <link>http://www.json.blog/2016/07/songs-in-the-key-of-allovue/</link>
      <pubDate>Sat, 09 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2016/07/songs-in-the-key-of-allovue/</guid>
      <description>&lt;p&gt;When I entered high school, video games were beginning to lose their appeal. So I sold my four video game systems and all their games at a garage sale and that money, plus some Chanukkah money, bought me my first guitar and amp. I had just tried joined a band as a singer with a couple of guys I knew from school. I didn&amp;rsquo;t know anything about playing guitar. In fact, it took me a while to figure out what distortion was and why I my guitar didn&amp;rsquo;t sound like Kris Roe from The Ataris.&lt;/p&gt;

&lt;p&gt;In the beginning, being in a band was rough. We had a lot of fun playing together, but just keeping time and making it all the way through a song was a slog. We had agreed I wouldn&amp;rsquo;t try playing guitar with the band until I had been playing at least three months, self-taught. But this was 2001 and I lived on Long Island and we were playing pop-punk, so it didn&amp;rsquo;t take too long to catch up. Soon I was writing music, actual original music. To this day, I don&amp;rsquo;t really enjoy playing other people&amp;rsquo;s music, because from the moment I picked up a guitar it was an instrument for creating new music with other people.&lt;/p&gt;

&lt;p&gt;Writing music was a kind of awful torture I was addicted to. For years, I was absorbing how music sounded. I used to listen so intently that I memorized whole compositions. I wanted to hear every strike of a kick drum, every open string ringing out, every tapped bass note, and every quiet piano layered in deep below the mix. But now that I was writing music, I became become obsessed with its construction. All the nuances I worked hard to hear in music took on a whole new layer of depth as I tried unravel how the song was made. I never heard music the same way again. But my appreciation for the craft of songwriting far exceeded the meager results a few months of having a stratocaster glued to my hands could produce. I would meet and practice with just one of the other members of the band for hours long writing sessions where we would struggle to create something good enough to bring to the rest of the guys and flesh out into a full song. But eventually, within a couple of years, we wrote about 15 songs, at least six or seven of which I was pretty proud of. It was so difficult to write those first songs. It took so many hours at home alone, then working hard with one or two other guys to write new parts and determine a structure, and then eventually months of practice with four guys sweating in a basement practicing the same music over and over again.&lt;/p&gt;

&lt;p&gt;I wanted so badly to write my song.&lt;/p&gt;

&lt;p&gt;Every band has one. Their song. The real one. The song that every musician who hears your album recognizes immediately as the song that trancends the talent of the individuals involved and is just plain better. It&amp;rsquo;s not the most complex song. It may not even be the most overtly emotional. It&amp;rsquo;s probably not your single. But it&amp;rsquo;s the song that stands out as a proud announcement to the people like me, the musicians who absorb every sound and experience the very structure of the music. Transcendent, to repeat myself, is really the best explanation for it. These are the songs that shook my soul, and I wanted to find mine.&lt;/p&gt;

&lt;p&gt;I never did write my song. I ended up quitting that first band after two and a half years and playing with a different set of guys for a bit over year chasing &amp;ldquo;my song&amp;rdquo;. I hoped a different writing experience with different musicians might help. Throughout college, I still played guitar all the time, but I never got comfortable writing without collaborators and I never found the right people to fulfill that role. Nowadays I pick up a guitar so rarely. I hear a phrase in a song I love and immediately know I can play it and sometimes get the urge to actually prove that to myself. Once a year, the foggy edges of a song appears in the distance, enticing me to chase it for a short while, and I record a small phrase to add to the library of forgotten riffs and lyrics.&lt;/p&gt;

&lt;p&gt;I still listen to music, though not as often and not really the same kinds anymore. And I still can&amp;rsquo;t listen the way I used to, the way it was before I picked up a guitar and tried singing into a microphone. That part of me is permanently broken in a way I expect only musicians can understand.&lt;/p&gt;

&lt;p&gt;I learned something important about myself in my time as a musician. When I&amp;rsquo;m chasing something I truly love, I don&amp;rsquo;t feelsome great pleasure. Writing music was about throwing myself into an agonizing chase for the impossible. It was the euphoria of the small accomplishments&amp;ndash; a good song performed on stage in front of a crowd that actually responds to your creation, or cracking how to transition from a verse to a chorus&amp;ndash; that kept me going. And it was the imprint on my life, mind and soul, that brought me true joy from being a musician as time went on.&lt;/p&gt;

&lt;p&gt;Working on product at Allovue feels like writing music. I have never done something this hard, but I do know what it is like to experience a profound &lt;em&gt;need&lt;/em&gt; so deeply. There are moments of real euphoria, like when a user describes their experience with Balance in a way so perfectly aligns with our vision that I triple check they are not a plant. And there are moments of agony, like almost every time I start to &amp;ldquo;listen&amp;rdquo; to our product and deconstruct it, and feel the weight of a decade&amp;rsquo;s worth of ideas on what our product needs to match the vision I have had since the first time Jess told me what she&amp;rsquo;s trying to do.&lt;/p&gt;

&lt;p&gt;It feels like for the first time, I just might be writing my song. The real one. And I&amp;rsquo;m terrified I&amp;rsquo;m not good enough or strong enough or just plain enough to see it through.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A note on sequels</title>
      <link>http://www.json.blog/2016/04/a-note-on-sequels/</link>
      <pubDate>Sun, 24 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2016/04/a-note-on-sequels/</guid>
      <description>&lt;p&gt;I read a lot of science fiction and fantasy, genres filled with long running book series. Until the last couple of years, I mostly avoided any series that wasn&amp;rsquo;t already complete. First, I don&amp;rsquo;t like truly “epic” sci-fi fantasy. On-going series without an end in sight, or series that go beyond roughly 3,000 to 4,000 pages never end well for me&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I simply lose interest. Second, I worry that series won&amp;rsquo;t actually reach completion, either because the books are not successful enough or the author gets writer&amp;rsquo;s block&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, or even just getting caught up in waiting way too long between books&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Third, I like to actually remember what happened, especially in the kind of complex stories I like to read.&lt;/p&gt;

&lt;p&gt;Some series do really well with sequels. I recently read through Kelly McCullough&amp;rsquo;s &lt;em&gt;Fallen Blade&lt;/em&gt; series, and although it is complete and I did read the books in succession, they always made a clear attempt to reintroduce everything about the novel and the necessary bits of past events. In fact, McCullough was so good at this , it was almost obnoxious to read the series all in one go&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;But other books seem to provide no help at all. And I am now deeply invested in several series that have not yet completed. Right now I&amp;rsquo;m finally reading &lt;em&gt;Poseidon&amp;rsquo;s Wake&lt;/em&gt;, the third and final novel in Alastair Reynolds&amp;rsquo; &lt;em&gt;Poseidon&amp;rsquo;s Children&lt;/em&gt; trilogy. Because it had been so long, I had forgotten critical parts of the earlier two novels that I enjoyed so much. Now, nearly 40% through the book and thoroughly engrossed, most of the key information has miraculously come back to me. But I found it difficult to get through the first 5% or so of the novel if for no other reason than I was trying to remember what was in *Blue Remembered Earth*and what was in Kim Stanley Robinson&amp;rsquo;s &lt;em&gt;2312&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;I must admit, I am often impressed with my own ability to recall details of a story I read years earlier when encountering a sequel, because I seem to remember far more of it than expected.  But I wonder, what must the editing process on a sequel be like? How do authors and editors decide what can be assumed and what cannot?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;See &lt;em&gt;Wizard&amp;rsquo;s First Rule&lt;/em&gt;, &lt;em&gt;Dune&lt;/em&gt;, and &lt;em&gt;A Song of Ice and Fire&lt;/em&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;See Patrick Rothfuss.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;I think I really learned this waiting for the conclusion of &lt;em&gt;His Dark Materials&lt;/em&gt;, which felt like it took a goddamn life time.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;I assume these books must be geared toward young adults and that this impacted the “hand holding” involved in moving from book to book. I&amp;rsquo;m not sure if they&amp;rsquo;re considered YA fiction, but the writing certainly had that feel. Still, they were wonderfully fun quick reads. I read all six books from November 23rd through December 14th.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;A novel I did not enjoy nearly as much, but which seemed to have very similar themes and setting and which I read three months prior to &lt;em&gt;Blue Remembered Earth&lt;/em&gt; and &lt;em&gt;On the Steel Breeze&lt;/em&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Returning to Tumblr</title>
      <link>http://www.json.blog/2015/07/returning-to-tumblr/</link>
      <pubDate>Mon, 06 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/07/returning-to-tumblr/</guid>
      <description>&lt;p&gt;I have had a &lt;a href=&#34;http://tumblr.jsonbecker.com&#34;&gt;Tumblr site&lt;/a&gt; for a long time but never knew what to do with it. What is Tumblr exactly? Is it a hosted blog? Is it a hosted blog for hipsters? Is it a social network? Why should I put content into Tumblr?&lt;/p&gt;

&lt;p&gt;I have this &lt;a href=&#34;http://blog.jsonbecker.com&#34;&gt;blog&lt;/a&gt;, but I barely use it. I don&amp;rsquo;t have a Facebook page, because I don&amp;rsquo;t trust Facebook and how it repeatedly changed and confused privacy settings and after college, I rarely found that Facebook was a net positive in my life. Recently I crossed &lt;em&gt;1000&lt;/em&gt; followers on Twitter.&lt;/p&gt;

&lt;p&gt;I like the sense of control offered by &lt;a href=&#34;http://blog.jsonbecker.com/2015/01/taking-control.html&#34;&gt;owning where I put content&lt;/a&gt;. But the barrier to posting a blog post has always felt high to me. A blog feels somewhat permanent. It&amp;rsquo;s something I want my current and future employers and friends to read it. It&amp;rsquo;s a record of ideas that felt worthy of memorializing. I have tried over and over again to lower this perceived barrier to blogging and failed.&lt;/p&gt;

&lt;p&gt;At the same time, I find the quick ability to favorite/like, retweet/re-broadcast, and respond on Twitter to be addicting. It is so easy to give feedback and join a conversation. As a result, I&amp;rsquo;ve probably written more, 140 characters at a time, on Twitter than I ever have on this blog.&lt;/p&gt;

&lt;p&gt;For me, Twitter is an ephemeral medium. It is about instant conversation and access. What I dump into Twitter doesn&amp;rsquo;t have any lasting power, which is why it&amp;rsquo;s so easy to toss out thoughts. Twitter is my new &lt;a href=&#34;https://en.wikipedia.org/wiki/Internet_Relay_Chat&#34;&gt;IRC&lt;/a&gt;, not a &lt;a href=&#34;https://en.wikipedia.org/wiki/Microblogging&#34;&gt;microblog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Writing on Twitter in 140 characters often seems to attract the worst in people. It&amp;rsquo;s not just #gamergate, it&amp;rsquo;s me. My ideas are more sarcastic, more acerbic, and less well considered because Twitter feels like an off the cuff conversation among friends. But it&amp;rsquo;s not a conversation among friends. It&amp;rsquo;s not really even a conversation. It&amp;rsquo;s a bunch of people shouting at each other in the same room. Twitter is less a late night dorm room debate and more the floor of the New York Stock Exchange.&lt;/p&gt;

&lt;p&gt;Which brings me to Tumblr, a service I think I finally understand. Tumblr is Twitter, but for people who take a breath before shouting. It has the same rich post types that Twitter has implemented through Cards. It has the same ability to magnify posts I find interesting through its reblogging feature. It also has the same ability to send a bit of encouragement and acknowledgement through its hearts. But Tumblr also doesn&amp;rsquo;t have the limitation of 140 characters, so I can spread my thoughts out just a bit further. And Tumblr does have a reply/conversation mechanism, but it&amp;rsquo;s just slightly “heavier” feeling than a Twitter reply so I&amp;rsquo;m less likely to just shoot off my mouth with the first thoughts that come to mind. Though Tumblr is a hosted service, it also has a fairly good API that can be used to export posts and the ability to use a custom URL. I could generate more post types on my Pelican blog, but a self-hosted blog lacks some of the social features that are just fun. And the truth is, do I really want to just put a link to a song I&amp;rsquo;m listening to right now on my blog? Is that kind of ephemera really worthy of a blog post? Maybe, but that&amp;rsquo;s not the kind of blog I want.&lt;/p&gt;

&lt;p&gt;So I am going back to Tumblr. I have been experimenting for a couple of days and I really like having a place to dump a link or a funny picture. I don&amp;rsquo;t want Tumblr to host my blog, but I do want Tumblr to eat into some of my Twitter posting. I can easily syndicate Tumblr posts over to Twitter, so why not take a little more space and &lt;em&gt;breathe&lt;/em&gt; before deciding it is worth sharing something.&lt;/p&gt;

&lt;p&gt;Please &lt;a href=&#34;http://tumblr.jsonbecker.com&#34;&gt;follow me on Tumblr&lt;/a&gt;. I think it&amp;rsquo;s going to be really fun.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cross-posted on both my &lt;a href=&#34;http://blog.jsonbecker.com&#34;&gt;blog&lt;/a&gt; and my &lt;a href=&#34;http://tumblr.jsonbecker.com&#34;&gt;Tumblr&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>News on Assertions in R</title>
      <link>http://www.json.blog/2015/07/news-on-assertions-in-r/</link>
      <pubDate>Sun, 05 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/07/news-on-assertions-in-r/</guid>
      <description>&lt;p&gt;How many times have you written R functions that start with a bunch of code that looks like this?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;my_funct &amp;lt;- function(dob, enddate = &amp;quot;2015-07-05&amp;quot;){
if (!inherits(dob, &amp;quot;Date&amp;quot;) | !inherits(enddate, &amp;quot;Date&amp;quot;)){
    stop(&amp;quot;Both dob and enddate must be Date class objects&amp;quot;)
  } 
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because R was designed to be interactive, it is incredibly tolerant to bad user input. Functions are not &lt;em&gt;type safe&lt;/em&gt;, meaning function arguments do not have to conform to specified data types. But most of my R code is not run interactively. I have to trust my code to run on servers on schedules or on demand as a part of production systems. So I find myself frequently writing code like the above&amp;ndash; manually writing type checks for safety.&lt;/p&gt;

&lt;p&gt;There has been some great action in the R community around &lt;em&gt;assertive programming&lt;/em&gt;, as you can see in the link. My favorite development, by far, are type-safe functions in the &lt;a href=&#34;https://github.com/smbache/ensurer&#34;&gt;ensurer package&lt;/a&gt;. The above function definition can now be written like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;my_funct &amp;lt;- function_(dob ~ Date, enddate ~ Date: as.Date(&amp;quot;2015-07-05&amp;quot;), {
  ...
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All the type-checking is done.&lt;/p&gt;

&lt;p&gt;I really like the reuse of the formula notation &lt;code&gt;~&lt;/code&gt; and the use of &lt;code&gt;:&lt;/code&gt; to indicate default values.&lt;/p&gt;

&lt;p&gt;Along with packages like &lt;a href=&#34;https://github.com/hadley/testthat&#34;&gt;testthat&lt;/a&gt;, R is really growing up and modernizing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rhode Island Disease</title>
      <link>http://www.json.blog/2015/06/rhode-island-disease/</link>
      <pubDate>Mon, 15 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/06/rhode-island-disease/</guid>
      <description>&lt;p&gt;When discussing policy in Rhode Island, I almost always encounter two bizarre arguments.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Rhode Island is completely unique. Ideas from other places don&amp;rsquo;t adequately take into account our local context. What is working there either won&amp;rsquo;t work here or isn&amp;rsquo;t really comparable to our situation here.&lt;/li&gt;
&lt;li&gt;What is happening nationally is directly applicable to Rhode Island. We can make broad sweeping statements about a set of policies, ideas, or institutions currently in play in Rhode Island without any knowledge of how things are going locally and how it&amp;rsquo;s different from other places. We can simply graft a broader national narrative onto Rhode Island regardless of whether it makes any sense with our facts on the ground.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These seemingly in conflict points of view are often employed by the same actors.&lt;/p&gt;

&lt;p&gt;It is probably not unique to Rhode Island, but that won&amp;rsquo;t stop me from calling it &lt;strong&gt;Rhode Island Disease&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Acceptable Terms</title>
      <link>http://www.json.blog/2015/04/acceptable-terms/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/04/acceptable-terms/</guid>
      <description>

&lt;p&gt;An &lt;a href=&#34;http://www.gcpvd.org/2015/04/15/pawsox-providence-stadium-proposal/&#34;&gt;initial proposal&lt;/a&gt; has been made to the city of Providence and state of Rhode Island to keep the PawSox in Rhode Island and move them to a new stadium along the river in Providence.&lt;/p&gt;

&lt;p&gt;The team is proposing that they privately finance all of the construction costs of the stadium while the land remains state (or city? I am not clear) owned. The state will lease the land underneath the stadium (the real value) with an option to buy for 30 years at $1 a year. The state will also pay $5,000,000 rent for the stadium itself annually for 30 years. The PawSox will then lease back the stadium at $1,000,000 per year. The net result will be the stadium is built and Rhode Island pays the PawSox owners $4,000,000 a year for 30 years.&lt;/p&gt;

&lt;h3 id=&#34;the-good&#34;&gt;The Good&lt;/h3&gt;

&lt;p&gt;Privately financing the upfront cost of the stadium puts risks of construction delays and cost overruns on the PawSox. Already they are underestimating the cost of moving a gas line below the park grounds. Whatever the cost of construction, whatever the impact on the team of a late opening, the costs to the state are fixed. There is essentially no risk in this plan for taxpayers, defining risk as a technical term for uncertainty. We know what this deal means: $120,000,000 over 30 years.&lt;/p&gt;

&lt;p&gt;The interest rate is pretty low. Basically, although the risk is privatized, we should view this stadium as the PawSox providing the state of Rhode Island a loan of $85,000,000 which we will pay back at a rate of approximate 1.15% &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:interest&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:interest&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Now just because the interest is low doesn&amp;rsquo;t mean we should buy&amp;hellip;&lt;/p&gt;

&lt;p&gt;The stadium design is largely attractive, even if the incorporated lighthouse is drawing ire. I don&amp;rsquo;t mind it, but I do like the idea of replacing it with an Anchor has some Greater City Providence commenters have recommended. Overall, I think the design fits with the neighborhood. It&amp;rsquo;s easy to get caught up in pretty renderings.&lt;/p&gt;

&lt;p&gt;The pedestrian bridge remains and is accessible. As someone who lives in Downcity, I am very much looking forward to this dramatic improvement to my personal transit. I think the bridge&amp;rsquo;s importance for transit is underrated, although admittedly we could make Point Street Bridge friendlier to pedestrians and bike riders instead.&lt;/p&gt;

&lt;p&gt;Brown University seems interested in hosting events, like football games, at the stadium. The plan also seems to give the state a lot of leeway in holding various events in the space when it&amp;rsquo;s not used for the baseball season. It could really be a great event space from mid-April until early November each year.&lt;/p&gt;

&lt;h3 id=&#34;the-bad&#34;&gt;The Bad&lt;/h3&gt;

&lt;p&gt;Even the team&amp;rsquo;s own economic estimates only foresee $2,000,000 in increased tax revenues. Although they claim this estimate is conservative, I would take that with a huge grain of salt. You do not lead with a plan that straight up says the taxpayers will be out $60,000,000 over 30 years unless you don&amp;rsquo;t have a better foot to put forward. I am going to go ahead and assume this estimate is about right. It&amp;rsquo;s certainly in the ballpark. (Ugh.) But what that means is that Rhode Islanders should understand this is &lt;em&gt;not an investment&lt;/em&gt;. This is not like building transit infrastructure or tax stabilization agreements to spur private construction. This deal is more akin to building schools. We do not, in fact cannot, expect that the economic impact makes this project a net positive for revenues. With $12,000,000 expected in direct spending, the project could be net positive for GDP, but even then it is obvious this is not the best annual investment to grow the economy. It is easy to come up with a laundry list of projects that cost less than this that could create more economic activity and/or more revenue to the state and city. Therefore, the project should be viewed primarily on use value. Will Rhode Islanders get $4,000,000 a year in value from the pleasure of using (and seeing) this stadium and its surrounding grounds? In school construction, we expect the benefits to be short term job creation, long term impacts on student health and well-being, ability to learn, and our ability to attract quality teachers. But most of those benefits are diffuse and hard to capture. Ultimately, we mostly support school construction because of the use benefits the kids and teachers see each year.&lt;/p&gt;

&lt;p&gt;The time line is crazy. If they&amp;rsquo;re serious about a June decision, they&amp;rsquo;re nuts. We have a complicated budget process ongoing right now. We have a teacher contract in Providence to negotiate. We have a brand new I-195 Commission trying to make their mark and get cranes in the sky. There&amp;rsquo;s no way a negotiation in good faith can be completed in 60 days unless they agree to every counter. If this is a &amp;ldquo;final best offer&amp;rdquo;, essentially, due to time line, then it is disingenuous.&lt;/p&gt;

&lt;p&gt;What happens in 30 years? We don&amp;rsquo;t have any guarantees of being whole in 30 years, and the same threats and challenges posed by the PawSox today will come up again in 30 years. Are we committed to a series of handouts until the team is of no monetary or cultural value?&lt;/p&gt;

&lt;p&gt;Other cities are likely going to come into play. The PawSox don&amp;rsquo;t have to negotiate a deal that&amp;rsquo;s fair for Rhode Island. They just have to negotiate to a deal that&amp;rsquo;s comparable to an offer they think someone else will make. Rhode Island&amp;rsquo;s position is weak, provided that anyone else is willing to make a deal.&lt;/p&gt;

&lt;h3 id=&#34;the-strange&#34;&gt;The Strange&lt;/h3&gt;

&lt;p&gt;The PawSox are asking for a 30-year property tax exemption. There&amp;rsquo;s a lot to think through here. First, there are at least two parcels that were meant to be tax generating that are a part of this plan&amp;ndash; the land Brown&amp;rsquo;s Continuing Education building currently sits on and the small develop-able parcel that was cut out from the park for a high value hotel or similar use. The stadium wants both of these parcels in addition to the park. I think City Council President Aponte is being a bit silly talking about being &amp;ldquo;made whole&amp;rdquo; over this deal, unless he&amp;rsquo;s talking about those two parcels. The park land was never going to generate city tax revenue and was actually going to cost the city money to maintain. Part of my openness to any proposal on this park land is my lack of confidence that the city will invest appropriately to maintain a world-class park space along the waterfront. There&amp;rsquo;s very little &amp;ldquo;whole&amp;rdquo; to be made.&lt;/p&gt;

&lt;p&gt;It is also possible that Providence will have to designate additional park space if the stadium is built. If that&amp;rsquo;s true and it&amp;rsquo;s coming off the tax roles than the PawSox absolutely should have to pay property taxes, period. There&amp;rsquo;s one possible exception I&amp;rsquo;ll address below&amp;hellip;&lt;/p&gt;

&lt;p&gt;I also feel very strongly about having a single process for tax stabilization across all I-195 land that is not politically driven but instead a matter of administrative decision. Exceptions for a big project breaks the major benefit of a single tax stabilization agreement ruling all the I-195 land, which is our need to send a signal that all players are equal, all developers are welcome, and political cronyism is not the path required to build. While some of those $2,000,000 in tax benefits will accrue to Providence through increased surrounding land value, many costs associated with the stadium will as well. There are police details, road wear and tear, fire and emergency services, and more to consider.&lt;/p&gt;

&lt;h3 id=&#34;my-counter&#34;&gt;My Counter&lt;/h3&gt;

&lt;p&gt;I don&amp;rsquo;t think this deal is dead, but I am not sure that the PawSox, city, or state would accept my counter. I have struggled with whether I should share what I &lt;em&gt;want&lt;/em&gt; to happen versus what I think a deal that &lt;em&gt;would&lt;/em&gt; happen looks like. I would be tempted to personally just let the PawSox walk. But if Rhode Island really wants them to stay, here&amp;rsquo;s a plausible counter:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The PawSox receive the same tax stabilization agreement all other developers get from the city of Providence. Terms for a fair valuation of the property are agreed upon up front that are derived from some portion of an average of annual revenues.&lt;/li&gt;
&lt;li&gt;The lease terms should be constructed such that the net cost (excluding the anticipated increase in tax receipts) is equal to the tax dollars owed to the city of Providence. Therefore, the state essentially pays for the $85,000,000 of principal and the city taxes. This could be through a PILOT, but I&amp;rsquo;d prefer that amount go to the PawSox and the PawSox transfer the dollars to the city. It&amp;rsquo;s just accounting, but I prefer the symbol of them paying property taxes. I don&amp;rsquo;t think it&amp;rsquo;s a terrible precedent for the state to offer PILOT payments to cover a gap between the city TSA in I-195 with a developer&amp;rsquo;s ask, if the state sees there is substantial public interest in that investment, but still better to actually get developers used to writing a check to the city.&lt;/li&gt;
&lt;li&gt;If the city has to make additional green space equivalent to the park we are losing, I foresee two options. First is the PawSox paying full load on whatever that land value is. The second is probably better, but harder to make happen. Brown should give up the Brown Stadium land to the city. They can make it into a park without reducing the foot print of taxable property in the city. If they did this, Brown should essentially get free use of the stadium with no fees (except police detail or similar that they would pay for their games on the East Side) in perpetuity. They should get first rights after the PawSox games themselves.&lt;/li&gt;
&lt;li&gt;The stadium itself will be reverted to ownership by the Rhode Island Convention Center Authority if the option to buy the land is not exercised in 30 years. This way the whole stadium and its land are state owned, since the state paid for it. The possible exception would be if Brown has to give up its stadium to park land, in which case I might prefer some arrangement be made with them.&lt;/li&gt;
&lt;li&gt;The PawSox ownership agrees to pay a large penalty to the state and the city if they move the team out of Rhode Island in the next 99 years.&lt;/li&gt;
&lt;li&gt;PawSox maintenance staff will be responsible for maintaining the Riverwalk park, stadium grounds, and the green-way that has been proposed for the I-195 district. Possible we can expand this to something like the Downcity Improvement District (or perhaps just have them pay to expand the DID into the Knowledge District). This will help ensure this creates more permanent jobs and reduces costs to the city for maintaining its public spaces that contribute to the broader attractiveness of the stadium.&lt;/li&gt;
&lt;li&gt;There should be a revenue share deal for any non-PawSox game events with the city and/or state for concession purchases and parking receipts.&lt;/li&gt;
&lt;li&gt;The stadium should not be exempt from future TIF assessments for infrastructure in the area.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am not sure that I would pay even that much for the stadium, but this would be a far better deal overall. I can absolutely think of better ways to spend state dollars, but I also realize that the trade-off is not that simple. Rhode Island is not facing a windfall of $85,000,000 and trying to decide what to do with it. A stadium that keeps the PawSox in Rhode Island inspires emotion. The willingness to create these dollars for this purpose may be far higher than alternative uses. The correct counterfactual is not necessarily supporting 111 Westminster (a better plan for less). It is not necessarily better school buildings. It is not necessarily meaningful tax benefits for rooftop solar power. It is not lowering taxes, building a fund to provide seed capital to local startups, a streetcar, dedicated bus and/or bike lanes, or tax benefits to fill vacant properties and homes. The correct counterfactual could be nothing. It could be all of these things, but in much smaller measure. It is very hard to fully evaluate this proposal because we are not rational actors with a fixed budget line making marginal investment decisions. Ultimately, with big flashy projects like this, I lean toward evaluating them on their own merits. Typically, and I think this case is no exception, even evaluating a stadium plan on its own merits without considering alternative investments makes it clear these projects are bad deals. Yet cities and states make them over and over again. We would be wise to look at this gap in dollars and cents and our collective, repeated actions not as fits of insanity but instead as stark reminders of our inability to simply calculate the total benefits that all people receive.&lt;/p&gt;

&lt;p&gt;In my day job, I get to speak to early stage investors. There I learned an important tidbit&amp;ndash; a company can name whatever valuation they want if an investor can control the terms. That&amp;rsquo;s my feeling with the PawSox. The cash is important, it&amp;rsquo;s not nothing. But any potential plan should be judged by the terms.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s hoping Rhode Island isn&amp;rsquo;t willing to accept bad terms at a high cost.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:interest&#34;&gt;$A = P(1 + \frac{r}{n})^{nt}$ where $A = $120,000,000$, $P = $85,000,000$, $n = 1$, and $t = 30$. I&amp;rsquo;ll leave you to the algebra.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:interest&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A PostgreSQL Cheat Sheet for OSX and R</title>
      <link>http://www.json.blog/2015/01/a-postgresql-cheat-sheet-for-osx-and-r/</link>
      <pubDate>Fri, 23 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/01/a-postgresql-cheat-sheet-for-osx-and-r/</guid>
      <description>&lt;p&gt;I keep this on my desktop.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install postgresql
initdb /usr/local/var/postgres -E utf8
gem install lunchy
### Start postgres with lunchy
mkdir -p ~/Library/LaunchAgents
cp /usr/local/Cellar/postgresql/9.3.3/homebrew.mxcl.postgresql.plist ~/Library/LaunchAgents/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Setup DB from SQL file:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;### Setup DB
lunchy start postgres
created $DBNAME
psql -d $DBNAME -f &#39;/path/to/file.sql&#39;
lunchy stop postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Starting and Stopping PostgreSQL&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;lunchy start postgres
lunchy stop postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;may run into trouble with local socket… try this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm /usr/local/var/postgres/postmaster.pid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Connecting with R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make sure lunch start postgres in terminal first)
require(dplyr)
db &amp;lt;- src_postgres(dbname=$DBNAME)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inspired by seeing &lt;a href=&#34;http://altons.github.io/r/2015/01/22/an-easy-way-of-installing-rpostgresql-on-mac/&#34;&gt;this post&lt;/a&gt; and thought I should toss out what I do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Taking Control</title>
      <link>http://www.json.blog/2015/01/taking-control/</link>
      <pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2015/01/taking-control/</guid>
      <description>

&lt;h2 id=&#34;severing-my-daemon&#34;&gt;Severing My Daemon&lt;/h2&gt;

&lt;p&gt;When I was in high school, I piggy-backed on a friend&amp;rsquo;s website to host a page for my band. We could post pictures, show locations and dates, lyrics, and pretend like we produced music people cared about. It was mostly a fun way for me to play with the web and something to show folks when I said I played guitar and sang in a band. One day, my friend canceled his hosting. He wasn&amp;rsquo;t using his site for anything and he forgot that I had been using the site. I was 18, I never thought about backups, and I had long deleted all those pesky photos taking up space on my memory cards and small local hard drive.&lt;/p&gt;

&lt;p&gt;Four years of photos from some of the best experiences of my life are gone. No one had copies. Everyone was using the site. In the decade since, no set of pictures has ever been as valuable as the ones I lost that day.&lt;/p&gt;

&lt;h2 id=&#34;who-controls-the-past&#34;&gt;Who controls the past&amp;hellip;&lt;/h2&gt;

&lt;p&gt;As you can imagine, this loss has had a profound effect on how I think about both my data and the permanence of the internet. Today, I have a deep system of backups for any digital data I produce, and I am far more likely to err on keeping data than discarding it. Things still sometimes go missing. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:stage&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:stage&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Perhaps the more lasting impact is my desire to maintain some control over all of my data. I use &lt;a href=&#34;http://www.fastmail.fm/?STKI=11467093&#34;&gt;Fastmail&lt;/a&gt; for my email, even after over 10 years of GMail use. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:early&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:early&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; I like knowing that I am storing some of my most important data in a standard way that easily syncs locally and backs up. I like that I pay directly for such an important service so that all of the incentive for my email provider is around making email work better for me. I am the customer. I use &lt;a href=&#34;http://www.getsync.com&#34;&gt;Bittorrent Sync&lt;/a&gt; for a good chunk of my data. I want redundancy across multiple machines and syncing, but I don&amp;rsquo;t want all of my work and all of my data to depend on being on a third party server like it is with Dropbox. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:db&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:db&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. I also use a &lt;a href=&#34;http://www.esn.fm/sponsors/&#34;&gt;Transporter&lt;/a&gt; so that some of my files are stored on a local hard drive.&lt;/p&gt;

&lt;h2 id=&#34;raison-d-être&#34;&gt;Raison D&amp;rsquo;être&lt;/h2&gt;

&lt;p&gt;Why does this blog exist? I have played with &lt;a href=&#34;http://tumblr.jsonbecker.com&#34;&gt;Tumblr&lt;/a&gt; in the past and I like its social and discovery tools, but I do not like the idea of pouring my thoughts into someone else&amp;rsquo;s service with no guarantee of easy or clean exit. I tried using Wordpress on a self-hosted blog for a while, but I took one look at the way my blog posts were being stored in the Wordpress database and kind of freaked out. All those convenient plugins and short codes were transforming the way my actual text was stored in hard to recover way. Plus, I didn&amp;rsquo;t really understand how my data was stored well enough to be comfortable I had solid back ups. I don&amp;rsquo;t want to lose my writing like I lost those pictures.&lt;/p&gt;

&lt;p&gt;This blog exists, built on &lt;a href=&#34;http://www.getpelican.com&#34;&gt;Pelican&lt;/a&gt;, because I needed to a place to write my thoughts in plain text that was as easy to back up as it was to share with the world. I don&amp;rsquo;t write often, and I feel I rarely write the &amp;ldquo;best&amp;rdquo; of my thoughts, but if I am going to take the time to put something out in the world I want to be damn sure that I control it.&lt;/p&gt;

&lt;h2 id=&#34;bag-end&#34;&gt;Bag End&lt;/h2&gt;

&lt;p&gt;I recently began a journey that I thought was about simplifying tools. I began using &lt;code&gt;vim&lt;/code&gt; a lot more for text editing, including writing prose like this post. But I quickly found that my grasping for new ways to do work was less about simplifying and more about better control. I want to be able to work well, with little interruption, on just about any computer. I don&amp;rsquo;t want to use anything that&amp;rsquo;s overly expensive or available only on one platform if I can avoid it. I want to strip away dependencies as much as possible. And while much of what I already use is free software, I didn&amp;rsquo;t feel like I was in &lt;em&gt;control&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For example, &lt;code&gt;git&lt;/code&gt; has been an amazing change for how I do all my work since about 2011. Github is a major part of my daily work and has saved me a bunch of money by allowing me to host this site for free. But I started getting frustrated with limitations of not having an actual server and not really having access to the power and control that a real server provides. So I recently moved this site off of Github and on to a &lt;a href=&#34;https://www.digitalocean.com/?refcode=7377c1fcbe67&#34;&gt;Digital Ocean&lt;/a&gt; droplet. This is my first experiment with running a Linux VPS. Despite using desktop Linux for four years full time, I have never administered a server. It feels like a skill I should have and I really like the control.&lt;/p&gt;

&lt;h2 id=&#34;quentin-s-land&#34;&gt;Quentin&amp;rsquo;s Land&lt;/h2&gt;

&lt;p&gt;This whole blog is about having a place I control where I can write things. I am getting better at the control part, but I really need to work on the writing things part.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I hope to do in the next few months. I am going to choose (or write) a new theme for the site that&amp;rsquo;s responsive and has a bit more detail. I am probably going to write a little bit about the cool, simple things I learned about &lt;code&gt;nginx&lt;/code&gt; and how moving to my own server is helping me run this page (and other experiments) with a lot more flexibility. I am also going to try and shift some of my writing from tweetstorms to short blog posts. If I am truly trying to control my writing, I need to do a better job of thinking out loud in this space versus treating them as disposable and packing them on to Twitter. I will also be sharing more code snippets and ideas and less thoughts on policy and local (Rhode Island) politics. The code/statistics/data stuff feels easier to write and has always gotten more views and comments.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s the plan for 2015. Time to execute.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:stage&#34;&gt;I recently found some rare music missing that I had to retrieve through some heroic efforts that included [Archive.org]() and stalking someone from an online forum that no longer exists (successfully).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:stage&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:early&#34;&gt;I was a very early adopter of Gmail.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:early&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:db&#34;&gt;I still use Dropbox. I&amp;rsquo;m not an animal. But I like having an alternative.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:db&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Creating A Services Marketplace Online</title>
      <link>http://www.json.blog/2014/11/amazon-creating-a-services-marketplace-online/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/11/amazon-creating-a-services-marketplace-online/</guid>
      <description>&lt;p&gt;A few thoughts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;This is a very interesting way to take advantage of a number of existing Amazon technologies&amp;ndash;primarily their payment processing and review system.&lt;/li&gt;
&lt;li&gt;Services are an increasingly important part of the economy and is less subject to commoditization. This is Amazon dipping into a massive growth area by commoditizing discovery and payment. It also offloads some of the risk from both sides of the transaction. It&amp;rsquo;s very bold, possibly brilliant.&lt;/li&gt;
&lt;li&gt;If you have tried to find a reliable carpenter, electrician, plumber, house cleaning service, etc lately, it should be obvious the value that Amazon can provide. Even as a subscriber to Angie&amp;rsquo;s List, which has been invaluable, finding reliable, affordable, and quality services is still a frustrating experience.&lt;/li&gt;
&lt;li&gt;This is why technology companies get huge valuations. It is hard to anticipate just how technologies to become the first online booksellers will lead to a massive number of accounts with credit cards and a strongly trusted brand. It is hard to anticipate how book reviews and powerful search andfiltering become the way you find people to come into your home and fix a toilet. But truly, it&amp;rsquo;s hard to anticipate the limits of a company with massive reach into people&amp;rsquo;s wallets that scales.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>The Power Of Sound</title>
      <link>http://www.json.blog/2014/11/the-power-of-sound/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/11/the-power-of-sound/</guid>
      <description>&lt;p&gt;It has been said a thousand times before, but I feel the need to say it again. So much of what Star Wars got right was creating a fully realized, fascinating world. As much as stunning visual effects that have largely stood the test of time were a part of that story, it was how Star Wars &lt;em&gt;sounded&lt;/em&gt; that is most remarkable.&lt;/p&gt;

&lt;p&gt;Watch that trailer. It has moments that &lt;em&gt;look&lt;/em&gt; an awful lot like Star Wars&amp;ndash; vast dunes in the middle of the desert, the Millenium Falcon speeding along, flipping at odd angles emphasizing its unique flat structure. But it also has a lot of elemetns that are decidedly modern and not Star Wars like. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:shakes&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:shakes&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; I think what&amp;rsquo;s most remarkable is I can close my eyes and just listen. Immediately I can &lt;em&gt;hear&lt;/em&gt; Star Wars. The sounds of Star Wars are not just iconic, they are deeply embedded in my psyche and embued with profound meaning.&lt;/p&gt;

&lt;p&gt;I first had the opportunity to see Star Wars on the big screen it was during the release of the &amp;ldquo;Special Editions&amp;rdquo;. There is nothing like hearing Star Wars in a theater.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:shakes&#34;&gt;Shakey-cam is the primary culprit.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:shakes&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>School Funding Reform, Hard Work and Fraught with Potential</title>
      <link>http://www.json.blog/2014/11/school-funding-reform-hard-work-and-fraught-with-potential/</link>
      <pubDate>Wed, 19 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/11/school-funding-reform-hard-work-and-fraught-with-potential/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Because of the primacy of equity as a goal in school finance system design, the formulas disproportionately benefit less wealthy districts and those with high concentrations of needier students.
&amp;hellip;
because of the universal impact on communities, school finance legislation requires broad political buy-in.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think it is worth contrasting the political realities of constructing school finance law with the need and justification for state funding of education in the first place.&lt;/p&gt;

&lt;p&gt;The state is the in the business of funding schools for redistributive purposes. If that wasn&amp;rsquo;t required, there&amp;rsquo;s little reason to not trade an inefficient pass through of sales and income tax dollars through to communities that could have lower sales and income taxes (or state sales and income taxes) replaced with local sales, income, property taxes , and fees. We come together as states to solve problems that extend beyond parochial boundaries, and our political unions exist to tackle problems we&amp;rsquo;re not better off tackling alone.&lt;/p&gt;

&lt;p&gt;There are limits to redistributive policy. Support for the needs of other communities might wane, leading to challenging and reducing the rights of children with new law or legal battles, serious political consequences for supporters of redistirbution, and decreased in economic activity (in education, property value). These are real pressures that need to be combatted both by convincing voters and through policy success &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:lack&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:lack&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. There are also considerations around the ethics of &amp;ldquo;bailing out&amp;rdquo; communities that made costly mistakes like constructing too many buildings or offering far too generous rights to staff in contracts that they cannot afford to maintain. We struggle as policy experts to not create the opportunity for moral hazards as we push to support children who need our help today.&lt;/p&gt;

&lt;p&gt;Policy experts and legal experts cannot excuse the needs of children today, nor can they fail to face the limits of support for redistribution or incentivizing bad adult behavior.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:lack&#34;&gt;I don&amp;rsquo;t doubt that support for redistributive policy goes south when it appears that our efforts to combat poverty and provide equal opportunities appear to fail, over and over again, and in many cases may actually make things worse.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:lack&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How do we attract the best teachers to struggling schools?</title>
      <link>http://www.json.blog/2014/11/how-do-we-attract-the-best-teachers-to-struggling-schools/</link>
      <pubDate>Thu, 13 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/11/how-do-we-attract-the-best-teachers-to-struggling-schools/</guid>
      <description>&lt;p&gt;There are some basic facts about the teacher labor market that are &lt;em&gt;inconvenient&lt;/em&gt; for many folks working to improve education. I am going to go through a few premises that I think should be broadly accepted and several lemma and contentions that I hope clarifies my own view on education resources and human capital management.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Teaching in low performing schools is challenging.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If I am looking for a job, all else being equal, I will generally not choose the more challenging one.&lt;/p&gt;

&lt;p&gt;Some may object to the idea that teachers would not accept a position that offers a greater opportunity to make a difference, for example, teaching at an inner city school, over one that was less likely to have an impact, like teaching in a posh, suburban neighborhood. It is certainly true that some teachers, if not most teachers place value on making a greater impact. However, the question is how great is that preference? How much less compensation (not just wage) would the median teacher be willing to take to work in a more challenging environment?&lt;/p&gt;

&lt;p&gt;I contend that it is atypical for teachers to accept lower compensation for a more challenging job. I would further suggest that even if there were a sufficient number of teachers to staff all urban schools with those that would accept lower compensation for a position in those schools, the gap in compensation that they would accept is low.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There are large gaps in non-pecuniary compensation between high performing school and low performing schools that is difficult to overcome.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let us supposed that it&amp;rsquo;s true there are large parts of the teacher workforce that would accept lower compensation (wage and non-wage) to teach in urban schools. There are real benefits to taking on a role where the potential for impact is great.&lt;/p&gt;

&lt;p&gt;However, we can consider this benefit as part of the &lt;em&gt;hedonic wages&lt;/em&gt; supplied by a teaching role. Other forms of non-monetary compensation that teachers may experience include: a comfortable physical work environment with sufficient space, lighting, and climate control; sufficient supplies to teach effectively; support and acceptance of their students, their families, and the broader school communities; a safe work environment; job security; alignment to a strong, unified school culture; and strong self-efficacy.&lt;/p&gt;

&lt;p&gt;Some of these features &lt;em&gt;could&lt;/em&gt; be easily replicated in many low performing schools. It is possible to have better quality physical schools and sufficient funding for supplies. Other features can be replicated, but not nearly as easily. Low performing schools where students have complex challenges inside and outside of the classroom are not environments where everyone has a strong sense of self-efficacy. Even the initial sense that making a difference is within reach erodes for many after facing a challenging environment day after day, year after year. A safe environment and a strong school culture are well within reach, but hardly easy and hardly universal. These things should be universal. They require funding, leadership, and broadly successful organizations.&lt;/p&gt;

&lt;p&gt;The key is not that all high performing schools always have these features and no low performing schools can or do have these features. What is important is that many of these features are less often found in low performing, particularly urban schools.&lt;/p&gt;

&lt;p&gt;I contend that the typical gap in non-pecuniary compensation between high and low performing schools is large enough to wipe out any negative compensating wage differential that may exist due to a desire for greater impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The primary mechanism to get &amp;ldquo;more&amp;rdquo; education is increasing the quality or quantity of teaching.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let us take the leap of suggesting that teaching is a key part of the production of education. If we want to improve educational equity and address the needs of low performing schools, we need some combination of more and higher quality teaching. This is a key driver of policies like extended learning time (more), smaller class sizes (more), professional development (better), and teacher evaluation and support systems (better). It is what is behind improving teacher preparation programs (better), alternative certification (better), and progressive support programs like RTI (more and better).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NaNo(Blo)WriMo</title>
      <link>http://www.json.blog/2014/11/nanoblowrimo/</link>
      <pubDate>Sun, 02 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/11/nanoblowrimo/</guid>
      <description>&lt;p&gt;November marks the start of National Novel Writing Month (NaNoWriMo). The quick version is folks band together and support each other to write 50,000 words in November.&lt;/p&gt;

&lt;p&gt;I would love to write a novel one day. I am not sure I could do it well, but I am pretty sure I could hit 50,000-80,000 words if I dedicated time to tell a story.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t have a story to tell.&lt;/p&gt;

&lt;p&gt;So this year, I have decided to not feel guilty about skipping out on another NaNoWriMo (&lt;em&gt;always the reader, never the author&lt;/em&gt;), and instead I am modifying it to meet my needs. With no story to tell and no experience tackling a single project the size of a novel, I am going to tackle a smaller problem&amp;ndash; this blog.&lt;/p&gt;

&lt;p&gt;Instead of 50,000 words in 30 days, I am going to try and write 1000 words a day for the next four weeks. I will not hold myself to a topic. I will not even hold myself to non-fiction. I will not hold myself to a number of posts or the size of the posts I write. I will not even hold myself to true daily count, instead reviewing where I stand at the end of each week.&lt;/p&gt;

&lt;p&gt;I am hoping that the practice of simply writing will grease my knuckles and start the avalanche that leads to writing more. A small confession&amp;ndash; I write two or three blog posts every week that never leave my drafts. I find myself unable to hit publish because the ideas tend to be far larger or far smaller than I anticipate when I set out to write and share my frustrations. I also get nervous, particularly when writing about things I do professionally, about not writing the perfect post that&amp;rsquo;s clear, heavily researched, and expresses my views definitively and completely. This month, I say goodbye to that anxiety and start simply hitting publish.&lt;/p&gt;

&lt;p&gt;I will leave you with several warnings.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Things might get topically wacky. I might suddenly become a food blogger, or write about more personal issues, or write a short story and suddenly whiplash to talking about programming, education policy, or the upcoming election. If high volume, random topics aren&amp;rsquo;t your thing, you should probably unsubscribe from my RSS feed and check back in a month.&lt;/li&gt;
&lt;li&gt;I might write terrible arguments that are poorly supported and don&amp;rsquo;t reflect my views. This month, I will not accept my most common excuses for not publishing, which boil down to fear people will hold me to the views I express in my first-draft thinking. I am going to make mistakes this month in public and print the dialog I am having with myself. The voices I allow room to speak as I struggle with values, beliefs, and opinions may be shock and offend. This month, this blog is my internal dialog. Please read it as a struggle, however definitive the tone.&lt;/li&gt;
&lt;li&gt;I am often disappointed that the only things I publish are smaller ideas written hastily with poor editing. Again, this month I embrace the reality that almost everything I write that ends up published is the result of 20 minutes of furious typing with no looking back, rather than trying to be a strong writer with a strong view point and strong support.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope that the end of this month I will have written at least a couple of pieces I feel proud of, and hopefully, I will have a little less fear of hitting publish in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Growing Up in the Internet Hate Machine</title>
      <link>http://www.json.blog/2014/10/growing-up-in-the-internet-hate-machine/</link>
      <pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/10/growing-up-in-the-internet-hate-machine/</guid>
      <description>&lt;p&gt;A terrible thing is happening this year. Women all across the internet are finding themselves the target of violence, simply for existing. Women are being harassed for talking about video games, women are being harassed for talking about the technology industry, women are being harassed for talking, women are being harassed.&lt;/p&gt;

&lt;p&gt;A terrible thing is happening. Women are finding themselves the target of violence.&lt;/p&gt;

&lt;p&gt;A terrible thing has always happened.&lt;/p&gt;

&lt;p&gt;&lt;hr&gt;&lt;/hr&gt;&lt;/p&gt;

&lt;p&gt;I remember being a 16 year old posting frequently on internet forums. One in particular focused on guitar equipment. I loved playing in a band, and I loved the technology of making guitar sounds. Many people on the forum were between 16 and 24, although it was frequented by quite a few &amp;ldquo;adults&amp;rdquo; in their 30s, 40s, and 50s. It was a wonderful opportunity to interact as an adult, with adults.&lt;/p&gt;

&lt;p&gt;Every week members created a new thread where they posted hundreds of photos of women. Most of them were professional photographs taken at various night clubs as patrons entered. Some were magazine clippings or fashion modeling. I remember taking part, both in gazing and supplying the occasional photograph from the internet. We were far from the early days of the world wide web, this being around 2003, but this was also before social media matured and online identity was well understood by the general public.&lt;/p&gt;

&lt;p&gt;This thread became controversial. A change from private to corporate ownership of this forum led to increased moderation, and the weekly post with photos of women was one of the targets.&lt;/p&gt;

&lt;p&gt;I did not understand.&lt;/p&gt;

&lt;p&gt;In the debates about the appropriateness of the content and its place within our online community, I took the side of those who wanted the post to remain alive. I was not its most ardent supporter, nor was I moved to some of the extremes in language and entitlement that typically surround these conversations. However, my views were clear and easy. These were public photographs, largely taken with permission (often for compensation). And, of course, none of the pictures were &lt;em&gt;pornographic&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Appropriateness for me at 16 was defined by &lt;em&gt;pornography&lt;/em&gt;. I did not understand.&lt;/p&gt;

&lt;p&gt;&lt;hr&gt;&lt;/hr&gt;&lt;/p&gt;

&lt;p&gt;My parents did not raise me to be misogynist. One of the most influential moments in my life came on a car ride to the dentist. I was also around 16 or 17. I think it was on my way to get my wisdom teeth removed. I had been dating the same girl for a while, and it was time for my father to give me &lt;em&gt;the talk&lt;/em&gt;. All he said to me was, &amp;ldquo;Women deserve your respect.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;That was it.&lt;/p&gt;

&lt;p&gt;&lt;hr&gt;&lt;/hr&gt;&lt;/p&gt;

&lt;p&gt;We were in college, and my friends and I were all &lt;em&gt;internet natives&lt;/em&gt;. We had used the web for over ten years. We grew up in AOL chatrooms and forums. The backwaters of the internet at this time shifted from Something Awful to 4Chan. This was the height of some of the most prolific and hilarious memes: lolcats, &lt;a href=&#34;http://knowyourmeme.com/memes/xzibit-yo-dawg&#34;&gt;Xzibit&lt;/a&gt;, &lt;a href=&#34;http://knowyourmeme.com/memes/advice-dog&#34;&gt;advice dogs&lt;/a&gt; (a favorite was &lt;a href=&#34;http://knowyourmeme.com/memes/foul-bachelor-frog&#34;&gt;bachelor frog&lt;/a&gt;, which seemed to understand our worst impulses expressed in only modest exaggeration).&lt;/p&gt;

&lt;p&gt;There was also violence.&lt;/p&gt;

&lt;p&gt;It was not uncommon to see names, phone numbers, and addresses that 4chan was supposed to harass because someone said so. Various subcultures seemed to be alternatively mocked and harassed endlessly in the very place that had first embraced, supported, and connected people under the guise of radical anonymity. The most famous of the &amp;ldquo;&lt;a href=&#34;http://knowyourmeme.com/memes/rules-of-the-internet&#34;&gt;Rules of the Internet&lt;/a&gt;&amp;rdquo; was Rule 34 &amp;ndash; if you can think of it, there is a porn of it&amp;ndash; and its follow up, Rule 35 &amp;ndash; if you can not find porn of it, you should make it. 4chan seemed determined to make this a reality. But really the most troublesome thing was the attitude toward women. Nothing was as unacceptable to 4chan as suggesting that women are anything but objects for male gaze. In a place sometimes filled with radically liberal (if more left-libertarian than left-progressive) politics that would spawn groups like &lt;a href=&#34;http://knowyourmeme.com/memes/subcultures/anonymous&#34;&gt;Anonymous&lt;/a&gt;, nothing brought out as much criticism as suggesting our culture has a problem with women.&lt;/p&gt;

&lt;p&gt;My response was largely to fade from this part of the internet. I had only reached the point of being &lt;em&gt;uncomfortable&lt;/em&gt; with this behavior. It would take more time for me to understand. It still felt like this was a problem of ignorant people.&lt;/p&gt;

&lt;p&gt;&lt;hr&gt;&lt;/hr&gt;&lt;/p&gt;

&lt;p&gt;I am rarely jealous of intelligence. I am rarely jealous of wealth. I am rarely jealous of experiences. What I am most often jealous of is what seems to me to be a preternatural maturity of others, particularly around issues of ethics and human rights.&lt;/p&gt;

&lt;p&gt;Fully grappling with privilege is not something that happens over a moment, it is a sensitivity to be developed over a lifetime. We are confronted with media that builds and reinforces a culture that is fundamentally intolerant and conservative. There are countless microaggressions that are modeled everywhere for our acceptance as normal. It has taken me a decade of maturation, hard conversations, and self-examination to only begin to grow from fully complicit and participating in objectification of women to what I would now consider to be the most basic level of human decency.&lt;/p&gt;

&lt;p&gt;The internet has gone from enabling my own aggression toward women to exposing me to a level of misogyny and violence that deeply disturbs and disgusts me, shattering any notion that my past offenses were harmless or victimless. The ugly underside of our culture is constantly on display, making it all the more obvious how what felt like isolated events on the &amp;ldquo;ok&amp;rdquo; side of the line were actaully creating a space that supported and nurtured the worst compulsions of men.&lt;/p&gt;

&lt;p&gt;&lt;hr&gt; &lt;/hr&gt;&lt;/p&gt;

&lt;p&gt;I often think about my own journey when I see disgusting behavior on the internet. I wonder whether I am facing a deeply, ugly person or myself at 16. I try to parse the difference between naïvety, ignorance, and hate and to understand if they require a unique response.&lt;/p&gt;

&lt;p&gt;Mostly, I struggle with what would happen if Jason Today spoke to Jason 16.&lt;/p&gt;

&lt;p&gt;Jason 16 could not skip over a decade of growth simply for having met Jason Today. It took me conversations with various folks playing the role of Jason Today over and over again, year after year. I wish I believed there was another way to reach the Jason 16s out there. I wish I knew how to help them become preternaturally aware of their actions. All I know how to do is try to be compassionate to those who hate while firmly correcting, try to meet the heightened expectations I place on myself, try to apologize when I need to, and try to support those that seem more equipped to push the conversation forward.&lt;/p&gt;

&lt;p&gt;Along this path, I never lept to agreement so much as paused. Each time I heard a convincing point, I paused and considered. Growth came in a series of all too brief pauses.&lt;/p&gt;

&lt;p&gt;Pauses are often private and quiet, its discoveries never on direct display.&lt;/p&gt;

&lt;p&gt;If pauses are the best anyone can expect, then working to change our culture of violence toward women will rarely feel like much more than shouting at the void.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>When We Legislate and Ajudicate Our World View</title>
      <link>http://www.json.blog/2014/06/when-we-legislate-and-ajudicate-our-world-view/</link>
      <pubDate>Fri, 13 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/06/when-we-legislate-and-ajudicate-our-world-view/</guid>
      <description>&lt;p&gt;The &lt;em&gt;Vergara v. California&lt;/em&gt; case has everyone in education talking. Key teacher tenure provisions in California are on the ropes, presumably because of the disparate impact on teacher, annd therefore education, quality for students who are less fortunate.&lt;/p&gt;

&lt;p&gt;I have fairly loosely held views about the practice of tenure itself and the hiring and firing of teachers. However, I have strongly held views that unions made mistake with their efforts to move a lot of rules about the teaching labor market into state laws across the country. Deep rules and restrictions are better left to contracts, even from a union perpsective. At worst, these things should be a part of regulation, which can be more easily adapted and waived.&lt;/p&gt;

&lt;p&gt;That said, here are a collection of interesting thoughts on tenure post-&lt;em&gt;Vergara&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;*&lt;a href=&#34;http://takingnote.learningmatters.tv/?p=7032&#34;&gt;John Merrow, reacting to &lt;em&gt;Vergara&lt;/em&gt;&lt;/a&gt;*:
&amp;gt;Tenure and due process are essential, in my view, but excessive protectionism (70+ steps to remove a teacher?) alienates the general public and the majority of effective teachers, particularly young teachers who are still full of idealism and resent seeing their union spend so much money defending teachers who probably should have been counseled out of the profession years ago.
&amp;gt;
&amp;gt; With the modal ‘years of experience&amp;rsquo; of teachers dropping dramatically, from 15 years in 1987 to 1 or 2 years today, young teachers are a force to be reckoned with. If a significant number of them abandon the familiar NEA/AFT model, or if they develop and adopt a new form of teacher unionism, public education and the teaching profession will be forever changed.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.mercurynews.com/education/ci_25724972/longer-pre-tenure-probationary-period-san-jose-teachers&#34;&gt;San Jose Mercury News reporting on the state thwarting a locally negotiated change to tenure&lt;/a&gt;&lt;/em&gt;:
&amp;gt;With little discussion, the board rejected the request, 7 to 2. The California Teachers Association, one of the most powerful lobbies in Sacramento, had opposed granting a two-year waiver from the state Education Code &amp;ndash; even though one of the CTA&amp;rsquo;s locals had sought the exemption&amp;hellip;
&amp;gt;&amp;hellip;San Jose Teachers Association President Jennifer Thomas, whose union had tediously negotiated with the district an agreement to improve teacher evaluations and teaching quality, called the vote frustrating&amp;hellip;
&amp;gt;San Jose Unified and the local teachers association sought flexibility to grant teachers tenure after one year or to keep a teacher on probation for three years.
&amp;gt;
&amp;gt;The district argued that 18 months &amp;ndash; the point in a teacher&amp;rsquo;s career at which districts must make a tenure decision &amp;ndash; sometimes doesn&amp;rsquo;t allow time to fairly evaluate a candidate for what can be a lifetime job.
&amp;gt;
&amp;gt;Now, Thomas said, when faced with uncertainty over tenure candidates, administrators will err on the side of releasing them, which then leaves a stain on their records.&lt;/p&gt;

&lt;p&gt;*[Kevin Welner summarzing some of the legal implications of &lt;em&gt;Vergara&lt;/em&gt;]()*:
&amp;gt;Although I can&amp;rsquo;t help but feel troubled by the attack on teachers and their hard-won rights, and although I think the court&amp;rsquo;s opinion is quite weak, legally as well as logically, my intent here is not to disagree with that decision. In fact, as I explain below, the decision gives real teeth to the state&amp;rsquo;s Constitution, and that could be a very good thing. It&amp;rsquo;s those teeth that I find fascinating, since an approach like that used by the Vergara judge could put California courts in a very different role —as a guarantor of educational equality—than we have thus far seen in the United States&amp;hellip;
&amp;gt;&amp;hellip;To see why this is important, consider an area of education policy that I have researched a great deal over the years: tracking (aka “ability grouping”). There are likely hundreds of thousands of children in California who are enrolled in low-track classes, where the expectations, curricula and instruction are all watered down. These children are denied equal educational opportunities; the research regarding the harms of these low-track classes is much stronger and deeper than the research about teachers Judge Treu found persuasive in the Vergara case. That is, plaintiffs&amp;rsquo; attorneys would easily be able to show a “real and appreciable impact” on students&amp;rsquo; fundamental right to equality of education. Further, the harm from enrollment in low-track classes falls disproportionately on lower-income students and students of color. (I&amp;rsquo;ll include some citations to tracking research from myself and others at the end of this post.)&lt;/p&gt;

&lt;p&gt;Welner also repeats a common refrain from the education-left that tenure and insulating teachers from evaluations is critical for attracting quality people into the teaching profession. This is an argument that the &lt;em&gt;general equilibrium&lt;/em&gt; impact on the broader labor market is both larger in magnitude and in the opposite direction of any assumed positive impacts from easier dismissal of poor performing teachers:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This more holistic view is important because the statutes are central to the larger system of teacher employment. That is, one would expect that a LIFO statute or a due process statute or tenure statute would shape who decides to become a teacher and to stay in the profession. These laws, in short, influence the nature of teaching as a profession. The judge here omits any discussion of the value of stability and experience in teaching that tenure laws, however imperfectly, were designed to promote in order to attract and retain good teachers. By declining to consider the complexity of the system, the judge has started to pave a path that looks more narrowly at defined, selected, and immediate impact—which could potentially be of great benefit to future education rights plaintiffs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.forbes.com/sites/modeledbehavior/2014/06/13/should-everyone-have-tenure/&#34;&gt;Adam Ozimek of Modeled Behavior&lt;/a&gt;&lt;/em&gt;:
&amp;gt;I can certainly imagine it is possible in some school districts they will find it optimal to fire very few teachers. But why isn&amp;rsquo;t it enough for administrators to simply rarely fire people, and for districts to cultivate reputations as places of stable employment? One could argue that administrators can&amp;rsquo;t be trusted to actually do this, but such distrust of administrators brings back a fundamental problem with this model of public education: if your administrators are too incompetent to cultivate a reputation that is optimal for student outcomes then banning tenure is hardly the problem, and imposing tenure is hardly a solution. This is closely related to a point I made yesterday: are we supposed to believe administrators fire sub-optimally but hire optimally&lt;/p&gt;

&lt;p&gt;His piece from today (and &lt;a href=&#34;http://www.forbes.com/sites/modeledbehavior/2014/06/12/putting-teacher-tenure-in-context/&#34;&gt;this one from yesterday&lt;/a&gt;) argues that Welner&amp;rsquo;s take could be applied to just about any profession, and furthermore, requires accepting a far deeper, more fundamental structural problem in education that should be unacceptable. If administrators would broadly act so foolishly as to decimate the market for quality teaching talent and be wholly unable to successfully staff their schools, we have far bigger problems. And, says Ozimek, there is no reason to believe that tenure is at all a response to this issue.&lt;/p&gt;

&lt;p&gt;Dana Goldstein would likely take a more &lt;a href=&#34;http://www.amazon.com/dp/038553695X/?tag=jasonpbeckerc-20&#34;&gt;historical view&lt;/a&gt; on the usefulness of tenure against adminstrator abuse.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.theatlantic.com/education/archive/2014/06/california-rules-teacher-tenure-laws-unconstitutional/372536/&#34;&gt;But, writing for The Atlantic&lt;/a&gt;&lt;/em&gt;, she focuses instead on tenure as a red herring:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The lesson here is that California&amp;rsquo;s tenure policies may be insensible, but they aren&amp;rsquo;t the only, or even the primary, driver of the teacher-quality gap between the state&amp;rsquo;s middle-class and low-income schools. The larger problem is that too few of the best teachers are willing to work long-term in the country&amp;rsquo;s most racially isolated and poorest neighborhoods. There are lots of reasons why, ranging from plain old racism and classism to the higher principal turnover that turns poor schools into chaotic workplaces that mature teachers avoid. The schools with the most poverty are also more likely to focus on standardized test prep, which teachers dislike. Plus, teachers tend to live in middle-class neighborhoods and may not want a long commute.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>They really don&#39;t make them like they used to...</title>
      <link>http://www.json.blog/2014/05/they-really-dont-make-them-like-they-used-to.../</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/05/they-really-dont-make-them-like-they-used-to.../</guid>
      <description>&lt;p&gt;I have never found dictionaries or even a thesaurus particularly useful as part of the writing process. I like to blame this on my lack of &lt;del&gt;creative&lt;/del&gt; careful writing.&lt;/p&gt;

&lt;p&gt;But just maybe, I have simply been using the wrong dictionaries. It is hard not to be seduced by the seeming superiority of Webster&amp;rsquo;s original style. A dictionary that is one-part explanatory and one-part &lt;em&gt;exploratory&lt;/em&gt; provides a much richer experience of English as an enabler of ideas that transend meager vocabulary.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pindown: Failed Dreams</title>
      <link>http://www.json.blog/2014/05/pindown-failed-dreams/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/05/pindown-failed-dreams/</guid>
      <description>&lt;p&gt;I had never thought of a use for &lt;a href=&#34;http://www.brettterpstra.com&#34;&gt;Brett Terpstra&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;http://heckyesmarkdown.com&#34;&gt;Marky the Markdownifier&lt;/a&gt; before listening today&amp;rsquo;s &lt;a href=&#34;http://5by5.tv/systematic/96&#34;&gt;Systematic&lt;/a&gt;. Why would I want to turn a webpage into Markdown?&lt;/p&gt;

&lt;p&gt;When I heard that Marky has an API, I was inspired. &lt;a href=&#34;http://pinboard.in&#34;&gt;Pinboard&lt;/a&gt; has a &amp;ldquo;description&amp;rdquo; field that allows up to 65,000 characters. I never know what to put in this box. Wouldn&amp;rsquo;t it be great to put the full content of the page in Markdown into this field?&lt;/p&gt;

&lt;p&gt;I set out to write a quick Python script to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grab recent Pinboard links.&lt;/li&gt;
&lt;li&gt;Check to see if the URLs still resolve.&lt;/li&gt;
&lt;li&gt;Send the link to Marky and collect a Markdown version of the content.&lt;/li&gt;
&lt;li&gt;Post an updated link to Pinboard with the Markdown in the description field.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If all went well, I would release this script on Github as &lt;strong&gt;Pindown&lt;/strong&gt;, a great way to put Markdown page content into your Pinboard links.&lt;/p&gt;

&lt;p&gt;The script below is far from well-constructed. I would have spent more time cleaning it up with things like better error handling and a more complete CLI to give more granular control over which links receive Markdown content.&lt;/p&gt;

&lt;p&gt;Unfortunately, I found that Pinboard consistently returns a &lt;a href=&#34;http://www.checkupdown.com/status/E414.html&#34;&gt;414 error code&lt;/a&gt; because the URLs are too long. Why is this a problem? Pinboard, in an attempt to &lt;a href=&#34;https://pinboard.in/api/&#34;&gt;maintain compatibility&lt;/a&gt; with the &lt;a href=&#34;http://del.ico.us&#34;&gt;del.ico.us&lt;/a&gt; API uses only GET requests, whereas this kind of request would typically use a POST end point. As a result, I cannot send along a data payload.&lt;/p&gt;

&lt;p&gt;So I&amp;rsquo;m sharing this just for folks who are interested in playing with Python, RESTful APIs, and Pinboard. I&amp;rsquo;m also posting for my own posterity since a &lt;a href=&#34;https://groups.google.com/d/msg/pinboard-dev/Od6sCzREeBU/L-WKgX6vUDoJ&#34;&gt;non-Del.ico.us compatible version 2 of the Pinboard API is coming&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
import json
import yaml


def getDataSet(call):
  r = requests.get(&#39;https://api.pinboard.in/v1/posts/recent&#39; + call)
  data_set = json.loads(r._content)
  return data_set

def checkURL(url=&amp;quot;&amp;quot;):
  newurl = requests.get(url)
  if newurl.status_code==200:
    return newurl.url
  else:
    raise ValueError(&#39;your message&#39;, newurl.status_code)

def markyCall(url=&amp;quot;&amp;quot;):
  r = requests.get(&#39;http://heckyesmarkdown.com/go/?u=&#39; + url)
  return r._content

def process_site(call):
  data_set = getDataSet(call)
  processed_site = []
  errors = []
  for site in data_set[&#39;posts&#39;]:
    try:
      url = checkURL(site[&#39;href&#39;])
    except ValueError:
      errors.append(site[&#39;href&#39;])
    description = markyCall(url)
    site[&#39;extended&#39;] = description
    processed_site.append(site)
  print errors
  return processed_site

def write_pinboard(site, auth_token):
  stem = &#39;https://api.pinboard.in/v1/posts/add?format=json&amp;amp;auth_token=&#39;
  payload = {}
  payload[&#39;url&#39;] = site.get(&#39;href&#39;)
  payload[&#39;description&#39;] = site.get(&#39;description&#39;, &#39;&#39;)
  payload[&#39;extended&#39;] = site.get(&#39;extended&#39;, &#39;&#39;)
  payload[&#39;tags&#39;] = site.get(&#39;tags&#39;, &#39;&#39;)
  payload[&#39;shared&#39;] = site.get(&#39;extended&#39;, &#39;no&#39;)
  payload[&#39;toread&#39;] = site.get(&#39;toread&#39;, &#39;no&#39;)           
  r = requests.get(stem + auth_token, params = payload)
  print(site[&#39;href&#39;] + &#39;\t\t&#39; + r.status_code)

def main():
  settings = file(&#39;AUTH.yaml&#39;, &#39;rw&#39;)
  identity = yaml.load(AUTH.yaml)
  auth_token = identity[&#39;user_name&#39;] + &#39;:&#39; + identity[&#39;token&#39;]
  valid_sites = process_site(&#39;?format=json&amp;amp;auth_token=&#39; + auth_token)
  for site in valid_sites:
    write_pinboard(site, auth_token)

if __name__ == &#39;__main__&#39;:
  main()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Symlinking Your Data</title>
      <link>http://www.json.blog/2014/04/symlinking-your-data/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/04/symlinking-your-data/</guid>
      <description>&lt;p&gt;I frequently work with private data. Sometimes, it lives on my personal machine rather than on a database server. Sometimes, even if it lives on a remote database server, it is better that I use locally cached data than query the database each time I want to do analysis on the data set. I have always dealt with this by &lt;a href=&#34;http://support.apple.com/kb/HT1578?viewlocale=en_US&amp;amp;locale=en_US&#34;&gt;creating encrypted disk images&lt;/a&gt; with secure passwords (stored in &lt;a href=&#34;https://agilebits.com&#34;&gt;1Password&lt;/a&gt;). This is a nice extra layer of protection for private data served on a laptop, and it adds little complication to my workflow. I just have to remember to mount and unmount the disk images.&lt;/p&gt;

&lt;p&gt;However, it can be inconvenient from a project perspective to refer to data in a distant location like &lt;code&gt;/Volumes/ClientData/Entity/facttable.csv&lt;/code&gt;. In most cases, I would prefer the data &amp;ldquo;reside&amp;rdquo; in &lt;code&gt;data/&lt;/code&gt; or &lt;code&gt;cache/&lt;/code&gt; &amp;ldquo;inside&amp;rdquo; of my project directory.&lt;/p&gt;

&lt;p&gt;Luckily, there is a great way that allows me to point to &lt;code&gt;data/facttable.csv&lt;/code&gt; in my R code without actually having &lt;code&gt;facttable.csv&lt;/code&gt; reside there: symlinking.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;http://en.wikipedia.org/wiki/Symbolic_link&#34;&gt;symlink&lt;/a&gt; is a &lt;strong&gt;symbolic link&lt;/strong&gt; file that sits in the preferred location and references the file path to the actual file. This way, when I refer to &lt;code&gt;data/facttable.csv&lt;/code&gt; the file system knows to direct all of that activity to the actual file in &lt;code&gt;/Volumes/ClientData/Entity/facttable.csv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From the command line, a symlink can be generated with a simple command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ln -s target_path link_path
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R offers a function that does the same thing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;file.symlink(target_path, link_path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;target_path&lt;/code&gt; and &lt;code&gt;link_path&lt;/code&gt; are both strings surrounded by quotation marks.&lt;/p&gt;

&lt;p&gt;One of the first things I do when setting up a new analysis is add common data storage file extensions like &lt;code&gt;.csv&lt;/code&gt; and &lt;code&gt;.xls&lt;/code&gt; to my &lt;code&gt;.gitignore&lt;/code&gt; file so that I do not mistakenly put any data in a remote repository. The second thing I do is set up symlinks to the mount location of the encrypted data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expressiveness Counts</title>
      <link>http://www.json.blog/2014/03/expressiveness-counts/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/03/expressiveness-counts/</guid>
      <description>

&lt;p&gt;Education data often come in annual snapshots. Each year, students are able to identify anew, and while student identification numbers may stay the same, names, race, and gender can often change. Sometimes, even data that probably should not change, like a date of birth, is altered at some point. While I could spend all day talking about data collection processes and automated validation that should assist with maintaining clean data, most researchers face multiple characteristics per student, unsure of which one is accurate.&lt;/p&gt;

&lt;p&gt;While it is true that identity is fluid, and sex/gender or race identifications are not inherently stable overtime, it is often necessary to &amp;ldquo;choose&amp;rdquo; a single value for each student when presenting data. The &lt;a href=&#34;http://www.strategicdataproject.com&#34;&gt;Strategic Data Project&lt;/a&gt; does a great job of defining the business rules for these cases in its &lt;a href=&#34;http://www.gse.harvard.edu/sdp/resources/toolkit.php&#34;&gt;diagnostic toolkits&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If more than one [attribute value is] observed, report the modal [attribute value]. If multiple modes are observed, report the most recent [attribute value] recorded.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is their rule for all attributes considered &lt;em&gt;time-invariant&lt;/em&gt; for analysis purposes. I think it is a pretty good one.&lt;/p&gt;

&lt;p&gt;Implementing this rule turned out to be more complex than it appeared using R, especially with performant code. In fact, it was this business rule that led me to learn how to use the &lt;code&gt;data.table&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;First, I developed a small test set of data to help me make sure my code accurately reflected the expected results based on the business rule:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Generate test data for modal_attribute().
modal_test &amp;lt;- data.frame(sid = c(&#39;1000&#39;, &#39;1001&#39;, &#39;1000&#39;, &#39;1000&#39;, &#39;1005&#39;, 
                                 &#39;1005&#39;, rep(&#39;1006&#39;,4)),
                         race = c(&#39;Black&#39;, &#39;White&#39;, &#39;Black&#39;, &#39;Hispanic&#39;,
                                  &#39;White&#39;, &#39;White&#39;, rep(&#39;Black&#39;,2), 
                                  rep(&#39;Hispanic&#39;,2)),
                         year = c(2006, 2006, 2007, 2008,
                                  2010, 2011, 2007, 2008,
                                  2010, 2011))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test data generated by that code looks like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sasid&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;race&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Black&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1001&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;White&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Black&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2007&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Hispanic&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2008&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1005&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;White&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2010&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1005&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;White&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1006&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Black&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2007&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1006&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Black&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2008&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1006&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Hispanic&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2010&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1006&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Hispanic&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2011&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And the results should be:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sasid&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;race&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Black&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1001&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;White&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1005&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;White&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1006&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Hispanic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;My first attempts at solving this problem using &lt;code&gt;data.table&lt;/code&gt; resulted in a pretty complex set of code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Calculate the modal attribute using data.table
modal_person_attribute_dt &amp;lt;- function(df, attribute){
  # df: rbind of all person tables from all years
  # attribute: vector name to calculate the modal value
  # Calculate the number of instances an attributed is associated with an id
  dt &amp;lt;- data.table(df, key=&#39;sasid&#39;)
  mode &amp;lt;- dt[, rle(as.character(.SD[[attribute]])), by=sasid]
  setnames(mode, c(&#39;sasid&#39;, &#39;counts&#39;, as.character(attribute)))
  setkeyv(mode, c(&#39;sasid&#39;, &#39;counts&#39;))
  # Only include attributes with the maximum values. This is equivalent to the
  # mode with two records when there is a tie.
  mode &amp;lt;- mode[,subset(.SD, counts==max(counts)), by=sasid]
  mode[,counts:=NULL]
  setnames(mode, c(&#39;sasid&#39;, attribute))
  setkeyv(mode, c(&#39;sasid&#39;,attribute))
  # Produce the maximum year value associated with each ID-attribute 
  # pairing    
  setkeyv(dt, c(&#39;sasid&#39;,attribute))
  mode &amp;lt;- dt[,list(schoolyear=max(schoolyear)), by=c(&amp;quot;sasid&amp;quot;, attribute)][mode]
  setkeyv(mode, c(&#39;sasid&#39;, &#39;schoolyear&#39;))
  # Select the last observation for each ID, which is equivalent to the highest
  # schoolyear value associated with the most frequent attribute.
  result &amp;lt;- mode[,lapply(.SD, tail, 1), by=sasid]
  # Remove the schoolyear to clean up the result
  result &amp;lt;- result[,schoolyear:=NULL]
  return(as.data.frame(result))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This approached seemed &amp;ldquo;natural&amp;rdquo; in &lt;code&gt;data.table&lt;/code&gt;, although it took me a while to refine and debug since it was my first time using the package &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2012&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2012&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Essentially, I use &lt;code&gt;rle&lt;/code&gt;, a nifty function I used in the past for my &lt;a href=&#34;{filename}/ranked-likert-scale-visualization.md&#34;&gt;Net-Stacked Likert&lt;/a&gt; code to count the number of instances of an attribute each student had in their record. I then subset the data to only the max count value for each student and merge these values back to the original data set. Then I order the data by student id and year in order to select only the last observation per student.&lt;/p&gt;

&lt;p&gt;I get a quick, accurate answer when I run the test data through this function. Unfortunately, when I ran the same code on approximately 57,000 unique student IDs and 211,000 total records, the results were less inspiring. My Macbook Air&amp;rsquo;s fans spin up to full speed and timings are terrible:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; system.time(modal_person_attribute(all_years, &#39;sex&#39;))
 user  system elapsed 
 40.452   0.246  41.346 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Data cleaning tasks like this one are often only run a few times. Once I have the attributes I need for my analysis, I can save them to a new table in a database, CSV, or similar and never run it again. But ideally, I would like to be able to build a document presenting my data completely from the raw delivered data, including all cleaning steps, accurately. So while I may use a cached, clean data set for some the more sophisticated analysis while I am building up a report, in the final stages I begin running the entire analyses process, including data cleaning, each time I produce the report.&lt;/p&gt;

&lt;p&gt;With the release of &lt;code&gt;dplyr&lt;/code&gt;, I wanted to reexamine this particular function because it is one of the slowest steps in my analysis. I thought with fresh eyes and a new way of expressing R code, I may be able to improve on the original function. Even if its performance ended up being fairly similar, I hoped the &lt;code&gt;dplyr&lt;/code&gt; code would be easier to maintain since I frequently use &lt;code&gt;dplyr&lt;/code&gt; and only turn to &lt;code&gt;data.table&lt;/code&gt; in specific, sticky situations where performance matters.&lt;/p&gt;

&lt;p&gt;In about a tenth the time it took to develop the original code, I came up with this new function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modal_person_attribute &amp;lt;- function(x, sid, attribute, year){
  grouping &amp;lt;- lapply(list(sid, attribute), as.symbol)
  original &amp;lt;- x
  max_attributes &amp;lt;- x %.% 
                    regroup(grouping) %.%
                    summarize(count = n()) %.%
                    filter(count == max(count))
  recent_max &amp;lt;- left_join(original, max_attributes) %.%
                regroup(list(grouping[[1]])) %.%
                filter(!is.na(count) &amp;amp; count == max(count))
  results &amp;lt;- recent_max %.% 
             regroup(list(grouping[[1]])) %.%
             filter(year == max(year))
  return(results[,c(sid, attribute)])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At least to my eyes, this code is far more expressive and elegant. First, I generate a &lt;code&gt;data.frame&lt;/code&gt; with only the rows that have the most common attribute per student by grouping on student and attribute, counting the size of those groups, and filtering to most common group per student. Then, I do join on the original data and remove any records without a count from the previous step, finding the maximum count per student ID. This recovers the year value for each of the students so that in the next step I can just choose the rows with the highest year.&lt;/p&gt;

&lt;p&gt;There are a few funky things (note the use of regroup and grouping, which are related to &lt;code&gt;dplyr&lt;/code&gt;&amp;rsquo;s poor handling of strings as arguments), but for the most part I have shorter, clearer code that closely resembles the plain-English stated business rule.&lt;/p&gt;

&lt;p&gt;But was this code more performant? Imagine my glee when this happened:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; system.time(modal_person_attribute_dplyr(all_years, sid=&#39;sasid&#39;, 
&amp;gt; attribute=&#39;sex&#39;, year=&#39;schoolyear&#39;))
Joining by: c(&amp;quot;sasid&amp;quot;, &amp;quot;sex&amp;quot;)
   user  system elapsed 
  1.657   0.087   1.852 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is a remarkable increase in performance!&lt;/p&gt;

&lt;p&gt;Now, I realize that I may have cheated. My &lt;code&gt;data.table&lt;/code&gt; code isn&amp;rsquo;t very good and could probably follow a pattern closer to what I did in &lt;code&gt;dplyr&lt;/code&gt;. The results might be much closer in the hands of a more adept developer. But the take home message for me was that &lt;code&gt;dplyr&lt;/code&gt; enabled me to write the more performant code naturally because of its expressiveness. Not only is my code faster and easier to understand, it is also simpler and took far less time to write.&lt;/p&gt;

&lt;p&gt;It is not every day that a tool provides powerful &lt;strong&gt;expressiveness&lt;/strong&gt; and yields greater &lt;strong&gt;performance&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;update&#34;&gt;Update&lt;/h3&gt;

&lt;p&gt;I have made some improvements to this function to simplify things. I will be maintaining this code in my &lt;a href=&#34;https://github.com/jasonpbecker/PPSDCollegeReadiness/blob/master/R/modal_person_attribute.R&#34;&gt;PPSDCollegeReadiness repository&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;modal_person_attribute &amp;lt;- function(x, sid, attribute, year){
  # Select only the important columns
  x &amp;lt;- x[,c(sid, attribute, year)]
  names(x) &amp;lt;- c(&#39;sid&#39;, &#39;attribute&#39;, &#39;year&#39;)
  # Clean up years
  if(TRUE %in% grepl(&#39;_&#39;, x$year)){
    x$year &amp;lt;- gsub(pattern=&#39;[0-9]{4}_([0-9]{4})&#39;, &#39;\\1&#39;, x$year)
  }  
  # Calculate the count for each person-attribute combo and select max
  max_attributes &amp;lt;- x %.% 
                    group_by(sid, attribute) %.%
                    summarize(count = n()) %.%
                    filter(count == max(count)) %.%
                    select(sid, attribute)
  # Find the max year for each person-attribute combo
  results &amp;lt;- max_attributes %.% 
             left_join(x) %.%
             group_by(sid) %.%
             filter(year == max(year)) %.%
             select(sid, attribute)
  names(results) &amp;lt;- c(sid, attribute)
  return(results)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:2012&#34;&gt;It was over a year ago that I first wrote this code.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2012&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latinos in Rhode Island Face Housing Burden</title>
      <link>http://www.json.blog/2014/02/latinos-in-rhode-island-face-housing-burden/</link>
      <pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/02/latinos-in-rhode-island-face-housing-burden/</guid>
      <description>&lt;p&gt;We burden Latinos (and other traditionally underserved communities) with expensive housing because of the widespread practice of using &lt;a href=&#34;https://www.providenceri.com/efile/3620&#34;&gt;homestead exemptions&lt;/a&gt; in Rhode Island. By lowering the real estate tax rate, typically by 50%, for owner occupied housing, we dramatically inflate the tax rate paid by Rhode Islanders who are renting.&lt;/p&gt;

&lt;p&gt;Echoing a newly filed lawsuit in New York City over &lt;a href=&#34;http://www.bkreader.com/2014/02/class-action-law-suit-charges-black-and-hispanic-renters-pay-more-in-property-taxes-than-some-owners/&#34;&gt;discriminatory real estate tax regimes&lt;/a&gt;, this new report emphasizes the racist incentives built into our property tax.&lt;/p&gt;

&lt;p&gt;Homestead exemptions are built on the belief that renters are non-permanent residents of communities, care less for the properties they occupy and neighborhoods they live in, and are worse additions than homeowners. Frankly, it is an anti-White flight measure meant to assure people that only those with the means to purchase and the intent to stay will join their neighborhoods. Wealthy, largely White, property owners see homestead exemptions as fighting an influx of &amp;ldquo;slum lords&amp;rdquo;, which is basically the perception of anyone who purchases a home or builds apartments and rents them out.&lt;/p&gt;

&lt;p&gt;Rather than encouraging denser communities with higher land utilization and more housing to reduce the cost of living in dignity, we subsidize low value (per acre) construction that maintain inflated housing costs.&lt;/p&gt;

&lt;p&gt;Full disclosure: I own a condo in Providence and receive a 50% discount on my taxes. In fact, living in a condo Downcity, my home value is depressed because of the limited ways that I can use it. I could rent my current condo at market rate and lose money because of the doubling in taxes that I would endure versus turning a small monthly profit at the same rent with higher taxes. The flexibility to use my property as my own residence or as a rental unit more than pays for higher taxes.&lt;/p&gt;

&lt;p&gt;So while I do have personal reasons to support removing the homestead exemption, even if I lived in a single family home on the East Side that was not attractive as a rental property, I would still think this situation is absurd. Homeowners&amp;rsquo; taxes should easily be 20% higher to tax renters 30% less. Maybe some of our hulking, vacant infrastructure could be more viably converted into housing stock and lower the cost for all residents. Maybe we could even see denser development because there will actually be a market for renters at the monthly rates that would need to be charged to recuperate expenses. At least the rent wouldn&amp;rsquo;t be so damn high for too many people of color and people living in or near poverty.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appreciating the Beauty of dplyr</title>
      <link>http://www.json.blog/2014/02/appreciating-the-beauty-of-dplyr/</link>
      <pubDate>Tue, 18 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/02/appreciating-the-beauty-of-dplyr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://had.co.nz&#34;&gt;Hadley Wickham&lt;/a&gt; has &lt;a href=&#34;https://github.com/hadley&#34;&gt;once again&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:seriously&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:seriously&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; made R ridiculously better. Not only is &lt;code&gt;dplyr&lt;/code&gt; incredibly fast, but the new syntax allows for some really complex operations to be expressed in a ridiculously beautiful way.&lt;/p&gt;

&lt;p&gt;Consider a data set, &lt;code&gt;course&lt;/code&gt;, with a student identifier, &lt;code&gt;sid&lt;/code&gt;, a course identifier, &lt;code&gt;courseno&lt;/code&gt;, a quarter, &lt;code&gt;quarter&lt;/code&gt;, and a grade on a scale of 0 to 4, &lt;code&gt;gpa&lt;/code&gt;. What if I wanted to know the number of a courses a student has failed over the entire year, as defined by having an overall grade of less than a 1.0?&lt;/p&gt;

&lt;p&gt;In dplyr:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;course %.% 
group_by(sid, courseno) %.%
summarise(gpa = mean(gpa)) %.%
filter(gpa &amp;lt;= 1.0) %.%
summarise(fails = n())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I refuse to even sully this post with the way I would have solved this problem in the past.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:seriously&#34;&gt;Seriously, how many of the packages he has managed/written are indispensable to using R today? It is no exaggeration to say that the world would have many more Stata, SPSS, and SAS users if not for Hadleyverse.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:seriously&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Freedom Should Be Reserved for the Wealthy</title>
      <link>http://www.json.blog/2014/02/freedom-should-be-reserved-for-the-wealthy/</link>
      <pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/02/freedom-should-be-reserved-for-the-wealthy/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;These quotes are absolutely striking, in that they give a clear glimpse into the ideological commitments of the Republican Party. From Sen. Blunt and Rep. Cole, we get the revelation that— for conservatives— the only “work” worth acknowledging is wage labor. To myself, and many others, someone who retires early to volunteer— or leaves a job to care for their children— is still working, they&amp;rsquo;re just outside the formal labor market. And indeed, their labor is still valuable— it just isn&amp;rsquo;t compensated with cash.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the greatest benefits of wealth is that it can liberate people to pursue happiness. When we tie a basic need for living complete lives of dignity to full time employment, people will find themselves willing to make many sacrifices to ensure this need. In our nation of great wealth with liberty and freedom as core values, it is hard to believe that the GOP would decry the liberating effect of ending the contingency of health care on work.&lt;/p&gt;

&lt;p&gt;There is no work rule, regulation, or union that empowers workers more in their relationship with their employers than removing the threat of losing health care from the table. An increasingly libertarian right should be celebrating this as a key victory, rather than celebrate the existing coercive impact that health care has in our lives.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Republicans aren&amp;rsquo;t as worried as the idle rich, who— I suppose— have earned the right to avoid a life of endless toil. Otherwise— if Republicans really wanted everyone to work as much as possible— they&amp;rsquo;d support confiscatory tax rates. After all, nothing will drive an investment banker back to the office like the threat of losing 70 percent of her income to Uncle Sam.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Oh yeah, I forgot. For all their claims to loving liberty and freedom, what the GOP really stands for is protecting liberty and freedom for the existing &amp;ldquo;deserving&amp;rdquo; wealthy. They will fight tooth and nail to remove estate taxes because inheritance is a legitimate source of liberty. Removing the fear of entering a hospital uninsured after being unable to access preventive care is what deprives folks of &amp;ldquo;dignity&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dreamschooling</title>
      <link>http://www.json.blog/2014/02/dreamschooling/</link>
      <pubDate>Thu, 06 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/02/dreamschooling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;My Democracy Prep colleague Lindsay Malanga and I often say we should start an organization called the Coalition of Pretty Good Schools.  We&amp;rsquo;d start with the following principles.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Every child must have a safe, warm, disruption-free classroom as a non-negotiable, fundamental right.&lt;/li&gt;
&lt;li&gt;All children should be taught to read using phonics-based instruction.&lt;/li&gt;
&lt;li&gt;All children must master basic computational skills with automaticity before moving on to higher mathematics.&lt;/li&gt;
&lt;li&gt;Every child must be given a well-rounded education that includes science, civics, history, geography, music, the arts, and physical education.&lt;/li&gt;
&lt;li&gt;Accountability is an important safeguard of public funds, but must not drive or dominate a child&amp;rsquo;s education. Class time must not be used for standardized test preparation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;We have no end of people ready to tell you about their paradigmatic shift that will fix education overnight. There has been plenty of philosophizing about the goals, purpose, and means of education. Everyone is ready to pull out tropes about the &amp;ldquo;factory model&amp;rdquo; of education our system is built on.&lt;/p&gt;

&lt;p&gt;The reality is that the education system too often fails at very basic delivery, period. I would love to see more folks draw a line in the sand of their minimum basic requirements, and not in an outrageous, political winky-wink where they are wrapping thier ideal in the language of the minimum. Lets have a deep discussion right now about the minimum basic requirements and lets get relentless about making that happen without the distraction of the dream. Frankly, whatever your dream is, so long as it involves kids going somewhere to learn &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:yeahyeah&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:yeahyeah&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, if we can&amp;rsquo;t deliver on the basics it will be dead on
arrival.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:yeahyeah&#34;&gt;Of course, for a group of folks who are engaged in Dreamschooling, we cannot take for granted that schools will be places or that children will be students in any traditional sense of the word. However, I believe that if we have a frank conversation about the minimum expectations for education I suspect this will not be a particularly widely held sentiment. If our technofuturism does complete its mindmeld with the anarcho-____ movements on the left and right to lead to a dramatically different conceptualization of childhood in the developed world in my lifetime&amp;hellip;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:yeahyeah&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Garrahy Complex: Rules for Public Investment in Parking</title>
      <link>http://www.json.blog/2014/01/garrahy-complex-rules-for-public-investment-in-parking/</link>
      <pubDate>Tue, 07 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2014/01/garrahy-complex-rules-for-public-investment-in-parking/</guid>
      <description>&lt;p&gt;James over at &lt;a href=&#34;http://www.transportprovidence.blogspot.com&#34;&gt;TransportPVD&lt;/a&gt; has a great &lt;a href=&#34;http://www.transportprovidence.blogspot.com/2014/01/how-do-you-prevent-surface-parking-lot.html&#34;&gt;post today&lt;/a&gt; talking about a Salt Lake City ordinance that makes property owners responsible for providing a bond that funds the landscaping and maintenance of vacant lots left after demolition. I love this as much as he does and would probably add several other provisions (like forfeiting any tax breaks on that property or any other property in the city and potentially forfeiture of the property itself if a demolition was approved based on site plans that are not adhered to within a given time frame). Ultimately, I do think the best solution to surface parking where it doesn&amp;rsquo;t belong, of either the temporary or permanent (and isn&amp;rsquo;t it all actually permanent?) kind, is a &lt;a href=&#34;http://streetsblog.net/2012/12/10/want-to-end-the-scourge-of-surface-parking-tax-land-not-buildings/&#34;&gt;land value tax&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;James goes one step further and suggests that we should adopt some similar rules around ALL parking developments and proposes a few. His hopes were that a mayoral candidate would chime in. For now, he will have to do with me.&lt;/p&gt;

&lt;p&gt;His recommendations are built somewhat specific to the commission looking at building a state-funded parking garage in front of the &lt;a href=&#34;http://www.gcpvd.org/2013/10/24/projo-commission-meets-to-discuss-possible-garrahy-complex-parking-garage/&#34;&gt;Garrahy Complex&lt;/a&gt; in Downcity, about which many urbanists and transit advocates have expressed reservations or outright rejection. They are:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;The garage is parking neutral. As many spots need to be removed from the downtown as are added.&lt;/li&gt;
&lt;li&gt;An added bonus would be if some of the spots removed were on-street ones, to create protected bike lanes or transit lanes with greenery separating them from car traffic.&lt;/li&gt;
&lt;li&gt;The garage has the proposed bus hub.&lt;/li&gt;
&lt;li&gt;There are ground-level shops.&lt;/li&gt;
&lt;li&gt;The garage is left open 24-hours so that it can limit the need for other lots (this happens when a garage is used only during the day, or only at night, instead of letting it serve both markets).&lt;/li&gt;
&lt;li&gt;Cars pay full market price to park.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Note: I&amp;rsquo;ve numbered rather than kept the bullets of the original to make responding easier.)&lt;/p&gt;

&lt;p&gt;I disagree with the first and second point, which are really one and the same. We are in a district that has tremendously underutilized land. We want that space to be developed and as a result of that development we expect their to be much increased need for transit capacity. The goal should be &lt;em&gt;both&lt;/em&gt; to increase accessibility &lt;em&gt;and&lt;/em&gt; increase the share of transit capacity offered by walking, biking, or riding a bus or light rail. This does not require that we demand a spot-for-spot when building a public garage. I agree with the sentiment but disagree with the degree. Part of building rules and policies like this is to ensure comprehensive consideration of the transit context when developing parking. I see no reason to a priori assume that garages should only be permitted if they eliminate the same number of spaces they create.&lt;/p&gt;

&lt;p&gt;The reason I combine these two points is because the city does not have the ability to remove off-street parking that is not publicly owned. Investing in smaller garages by footprint that have to be built taller and provide no change in capacity probably make no sense at all. If we&amp;rsquo;re going to build any kind of public garage at all, it should be with the goal of consolidating parking into infrastructure with reasonable land utilization. We would rather 3 or 4 large garages properly located than all of the current lots. Limiting their size because of the flexibility available due to reducing on-street parking or the footprint on existing lots doesn&amp;rsquo;t achieve that and doesn&amp;rsquo;t factor in orders-of-magnitude changes in capacity we should need for all transit modes in the next 20 years.&lt;/p&gt;

&lt;p&gt;On point three, I am skeptical. I like the idea of improving bus infrastructure when building parking infrastructure in general. In fact, I voted against the \$40M Providence road paving bond even though that was much needed maintenance. My rationale was purely ideological&amp;ndash; we should not use debt to pay for car maintenance without also investing in ways to reduce future maintenance costs through better utilization of those roads. However, I have a hard time believing that the Garrahy location is any good as a bus hub. If RIPTA did a great job identifying the need for an additional bus hub that the Garrahy location met the criteria for, I think it&amp;rsquo;s a reasonable idea. Short of that, it feels like throwing the transit community a wasteful bone.&lt;/p&gt;

&lt;p&gt;I mostly agree on point four, but I doubt at the scale James would like to see. I think an appropriate level is probably not that different from the recently erected Johnson and Wales garage. The reality is that street-level retail is the right form, but there isn&amp;rsquo;t sufficient foot traffic to support it right now and won&amp;rsquo;t be for some time. There has to be street-level activation of any garage built in this area, but the square footage is likely fairly timid.&lt;/p&gt;

&lt;p&gt;I absolutely agree with point five, without qualification. Not a dime should be spent on a public parking spot that is closed at any point in time, anywhere in the city. I would actually ditto this for surface parking lots on commercial properties of any kind after business hours. Not only should they have to be open, they should have to provide signs indicating the hours of commercial activity when parking is restricted and the hours when parking is available to the public. These hours of operations should require board approval. Owners could choose to charge during these off hours, but cars must be able to access the lot.&lt;/p&gt;

&lt;p&gt;And point six should be a given for any public parking.&lt;/p&gt;

&lt;p&gt;The real problem with Garrahy, in my opinion, is the cost is absurd, likely to be at least \$35,000 per space. There is plenty of existing parking, suggesting the demand right now is illusory and market rate for those spots right now means the investment is unlikely to ever be recovered. In a world with limited capacity for government spending on transit as a public good, I would rather subsidize transit infrastructure that benefits the poor and directly impacts the share of non-car transit as it increases capacity. Spending limited funds on parking infrastructure is ludicrous when demand isn&amp;rsquo;t sufficient to recover the investment. We already more than sufficiently subsidize parking in the area. And of course, the &amp;ldquo;study commission&amp;rdquo; is not really a study&amp;ndash; it&amp;rsquo;s a meeting convened by those who want the project to happen putting the required usual suspects in the room to tepidly rubber stamp it. At least that&amp;rsquo;s my cynical take.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Did public schools build economies, or did economies build public schools?</title>
      <link>http://www.json.blog/2013/12/did-public-schools-build-economies-or-did-economies-build-public-schools/</link>
      <pubDate>Tue, 10 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/12/did-public-schools-build-economies-or-did-economies-build-public-schools/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;We find that public schools offered practically zero return education on the margin, yet they did enjoy significant political and financial support from local political elites, if they taught in the “right” language of instruction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One thing that both progressives and libertarians agree upon are that social goals of education are woefully underappreciated and considered in the current school reform discussion. Both school choice and local, democratic control of schools are reactions to centralization resulting in &amp;ldquo;elites&amp;hellip; [selecting] the &amp;lsquo;right&amp;rsquo; language of instruction.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;I am inclined to agree with neither.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculating Age with Precision in R</title>
      <link>http://www.json.blog/2013/12/calculating-age-with-precision-in-r/</link>
      <pubDate>Wed, 04 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/12/calculating-age-with-precision-in-r/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;
&amp;gt; Turns out the original code below was pretty messed up. All kinds of little errors I didn&amp;rsquo;t catch. I&amp;rsquo;ve updated it below. There are a lot of options to refactor this further that I&amp;rsquo;m currently considering. Sometimes it is really hard to know just how flexible something this big really should be. I think I am going to wait until I start developing tests to see where I land. I have a feeling moving toward a more test-driven work flow is going to force me toward a different structure.&lt;/p&gt;

&lt;p&gt;I recently updated the function I &lt;a href=&#34;http://www.json.blog/2013/06/calculating-age-in-r/&#34;&gt;posted&lt;/a&gt; about back in June that calculates the difference between two dates in days, months, or years in R. It is still surprising to me that &lt;code&gt;difftime&lt;/code&gt; can only return units from seconds up until weeks. I suspect this has to do with the challenge of properly defining a &amp;ldquo;month&amp;rdquo; or &amp;ldquo;year&amp;rdquo; as a unit of time, since these are variable.&lt;/p&gt;

&lt;p&gt;While there was nothing wrong with the original function, it did irk me that it always returned an integer. In other words, function returned only complete months or years. If the start date was on &lt;code&gt;2012-12-13&lt;/code&gt; and the end date was on &lt;code&gt;2013-12-03&lt;/code&gt;, the function would return &lt;code&gt;0&lt;/code&gt; years. Most of the time, this is the behavior I expect when calcuating age. But it is completely reasonable to want to include partial years or months, e.g. in the aforementioned example returning &lt;code&gt;0.9724605&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So after several failed attempts because of silly errors in my algorithm, here is the final code. It will be released as part of &lt;a href=&#34;https://github.com/jknowles/eeptools/&#34;&gt;eeptools 0.3&lt;/a&gt; which should be avialable on CRAN soon &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:moves&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:moves&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;age_calc &amp;lt;- function(dob, enddate=Sys.Date(), units=&#39;months&#39;, precise=TRUE){
  if (!inherits(dob, &amp;quot;Date&amp;quot;) | !inherits(enddate, &amp;quot;Date&amp;quot;)){
    stop(&amp;quot;Both dob and enddate must be Date class objects&amp;quot;)
  }
  start &amp;lt;- as.POSIXlt(dob)
  end &amp;lt;- as.POSIXlt(enddate)
  if(precise){
    start_is_leap &amp;lt;- ifelse(start$year %% 400 == 0, TRUE, 
                        ifelse(start$year %% 100 == 0, FALSE,
                               ifelse(start$year %% 4 == 0, TRUE, FALSE)))
    end_is_leap &amp;lt;- ifelse(end$year %% 400 == 0, TRUE, 
                        ifelse(end$year %% 100 == 0, FALSE,
                               ifelse(end$year %% 4 == 0, TRUE, FALSE)))
  }
  if(units==&#39;days&#39;){
    result &amp;lt;- difftime(end, start, units=&#39;days&#39;)
  }else if(units==&#39;months&#39;){
    months &amp;lt;- sapply(mapply(seq, as.POSIXct(start), as.POSIXct(end), 
                            by=&#39;months&#39;, SIMPLIFY=FALSE), 
                     length) - 1
    # length(seq(start, end, by=&#39;month&#39;)) - 1
    if(precise){
      month_length_end &amp;lt;- ifelse(end$mon==1, 28,
                                 ifelse(end$mon==1 &amp;amp; end_is_leap, 29,
                                        ifelse(end$mon %in% c(3, 5, 8, 10), 
                                               30, 31)))
      month_length_prior &amp;lt;- ifelse((end$mon-1)==1, 28,
                                     ifelse((end$mon-1)==1 &amp;amp; start_is_leap, 29,
                                            ifelse((end$mon-1) %in% c(3, 5, 8, 
                                                                      10), 
                                                   30, 31)))
      month_frac &amp;lt;- ifelse(end$mday &amp;gt; start$mday,
                           (end$mday-start$mday)/month_length_end,
                           ifelse(end$mday &amp;lt; start$mday, 
                            (month_length_prior - start$mday) / 
                                month_length_prior + 
                                end$mday/month_length_end, 0.0))
      result &amp;lt;- months + month_frac
    }else{
      result &amp;lt;- months
    }
  }else if(units==&#39;years&#39;){
    years &amp;lt;- sapply(mapply(seq, as.POSIXct(start), as.POSIXct(end), 
                            by=&#39;years&#39;, SIMPLIFY=FALSE), 
                     length) - 1
    if(precise){
      start_length &amp;lt;- ifelse(start_is_leap, 366, 365)
      end_length &amp;lt;- ifelse(end_is_leap, 366, 365)
      year_frac &amp;lt;- ifelse(start$yday &amp;lt; end$yday,
                          (end$yday - start$yday)/end_length,
                          ifelse(start$yday &amp;gt; end$yday, 
                                 (start_length-start$yday) / start_length +
                                end$yday / end_length, 0.0))
      result &amp;lt;- years + year_frac
    }else{
      result &amp;lt;- years
    }
  }else{
    stop(&amp;quot;Unrecognized units. Please choose years, months, or days.&amp;quot;)
  }
  return(result)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:moves&#34;&gt;I should note that my mobility function will also be included in eeptools 0.3. I know I still owe a post on the actual code, but it is such a complex function I have been having a terrible time trying to write clearly about it.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:moves&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Different Angle on PISA</title>
      <link>http://www.json.blog/2013/12/a-different-angle-on-pisa/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/12/a-different-angle-on-pisa/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/pisaengagement.png&#34; alt=&#34;PISA Results&#34; title=&#34;PISA Student Engagement US&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to call attention to these interesting PISA results. &lt;a href=&#34;http://www.json.blog/img/turnsout.mp3&#34;&gt;Turns out&lt;/a&gt; that student anxiety in the United States is lower than the OECD average and belief in ability is higher &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:definitions&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:definitions&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I thought that all of the moves in education since the start of standard&amp;rsquo;s based reform were supposed to be generating tremendous anxiety and failing to produce students who had high sense of self-efficacy?&lt;/p&gt;

&lt;p&gt;It is also worth noting that students in the United States were more likely to skip out on school dand this had a higher than typical impact on student performance. One interpretation of this could be that students are less engaged, but also that schooling activities do have a large impact on students rather than schools being of lesser importance than student inputs.&lt;/p&gt;

&lt;p&gt;I have always had a hard time reconciling the calls for higher teacher pay and better work conditions and evidence that missing even just 10% of schooling has a huge impact on student outcomes with the belief that addressing other social inequities is the key way to achieve better outcomes for kids.&lt;/p&gt;

&lt;p&gt;This is all an exercise in nonsense. It is incredibly difficult to transfer findings from surveys across dramatical cultural differences. It is also hard to imagine what can be learned about the delivery of education in the dramatically different contexts that exists. The whole international comparison game seems like one big Rorschach test where the price of admission is leaving any understanding of culture, context, and external validity at the door.&lt;/p&gt;

&lt;p&gt;P.S.: The use of color in this visualization is awful. There is a sense that they are trying to be &amp;ldquo;value neutral&amp;rdquo; with data that is ordinal in nature (above, same, or below), and in doing so chose two colors that are very difficult to distinguish between. Yuck.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:definitions&#34;&gt;The site describes prevalence of anxiety as, &amp;ldquo;proportion of students who feel helpless when faced with math problems&amp;rdquo; and belief in ability as, &amp;ldquo;proportion of students who feel confident in their math abilitites&amp;rdquo;. Note, based on these defitions, one might also think that either curricula were not so misaligned with international benchmarks or that we are already seeing the fruits of partial transition to Common Core. Not knowing the trend for this data, or some of the specifics about the collection instrument, makes that difficult to assess.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:definitions&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Good Long Read on Assessment and Accountability</title>
      <link>http://www.json.blog/2013/11/a-good-long-read-on-assessment-and-accountability/</link>
      <pubDate>Sat, 23 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/a-good-long-read-on-assessment-and-accountability/</guid>
      <description>&lt;p&gt;Although it clocks in at 40+ pages, this is a worthwhile and relatively fast read for anyone in education policy on the future of assessment if we&amp;rsquo;re serious about college and career readiness. There is a ton to unpack, with a fair amount it agree with and a lot I am quite a bit less sure on.&lt;/p&gt;

&lt;p&gt;I think this paper is meant for national and state level policy-makers, and so my major quibble is I think this is much more valuable for a district-level audience. I am less bullish on the state&amp;rsquo;s role in building comprehensive assessment systems. That&amp;rsquo;s just my initial reaction.&lt;/p&gt;

&lt;p&gt;The accountability section is both less rich and less convincing than the assessment portion. I have long heard cries for so-called reciprocal accountability, but it is still entirely unclear to me what this means and looks like and the implications for current systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bilingual Education at Providence Public Schools</title>
      <link>http://www.json.blog/2013/11/bilingual-education-at-providence-public-schools/</link>
      <pubDate>Thu, 21 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/bilingual-education-at-providence-public-schools/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;“We are trying to work towards late-exit ELL programs so (students) can learn the concepts in (their) native language,” Lusi said. Administrative goals have recently shifted to a focus on proficiency in both languages because bilingual education is preferred, she added.&lt;/p&gt;

&lt;p&gt;But instituting district-wide bilingual education would require funding to hire teachers certified in both languages and to buy dual-language materials, she said.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I am pretty sure this is new. I am surprised there has not been a stronger effort to pass a legislative package in Rhode Island that provides both the policy framework and funding necessary to achieve universal bilinguage education for English language learners in RI schools.&lt;/p&gt;

&lt;p&gt;One of the great advantages of transitioning to common standards&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:CCSS&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:CCSS&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is there should be greater availability of curricular materials in languages other than English. I suspect most of what is needed for bilingual education is start up money for materials, curriculum supports and developments, and assessment materials. There are a few policy things that need to be in place, possibly around state exams, but also rules around flexible teacher assignment, hiring, and dismissal staffing needs dramatically change.&lt;/p&gt;

&lt;p&gt;Someone should be putting this package together. I suspect there would be broad support.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:CCSS&#34;&gt;Note, this is not necessarily a feature of the Common Core State Standards, just having standards in common with many other states.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:CCSS&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>DeBlasio: Weak on Implementation</title>
      <link>http://www.json.blog/2013/11/deblasio-weak-on-implementation/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/deblasio-weak-on-implementation/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;De Blasio and his advisers are still figuring out how much rent to charge well-funded charter schools, his transition team told me. “It would depend on the resources of the charter school or charter network,” he told WNYC, in early October. “Some are clearly very, very well resourced and have incredible wealthy backers. Others don&amp;rsquo;t. So my simple point was that programs that can afford to pay rent should be paying rent.” (In an October debate with the Republican candidate Joseph Lhota, he put it more bluntly: “I simply wouldn&amp;rsquo;t favor charters the way Mayor Bloomberg did because, in the end, our city rises or falls on our traditional public schools.”)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My impression of DeBlasio was that he went around collecting every plausible complaint from every interest group that was mad at Bloomberg and promised whatever they wanted. There didn&amp;rsquo;t really seem to be a coherent theory or any depth whatsoever to his policy prescriptions.&lt;/p&gt;

&lt;p&gt;Already working hard to confirm this impression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More evidence for &#34;mere facts&#34;</title>
      <link>http://www.json.blog/2013/11/more-evidence-for-mere-facts/</link>
      <pubDate>Tue, 19 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/more-evidence-for-mere-facts/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;To recap, the first study discussed above established that children from disadvantaged backgrounds know less about a topic (i.e., birds) than their middle-class peers. Next, in study two, the researchers showed that differences in domain knowledge influenced children&amp;rsquo;s ability to understand words out of context, and to comprehend a story. Moreover, poor kids — who also had more limited knowledge — perform worse on these tasks than did their middle class peers. But could additional knowledge be used to level the playing field for children from less affluent backgrounds? &lt;br&gt;&lt;br&gt;
In study three, the researchers held the children&amp;rsquo;s prior knowledge constant by introducing a fictitious topic — i.e., a topic that was sure to be unknown to both groups. When the two groups of children were assessed on word learning and comprehension related to this new domain, the researchers found no significant differences in how poor and middle-class children learned words, comprehended a story or made inferences.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the &amp;ldquo;old&amp;rdquo; divides in education, from before the current crop of &amp;ldquo;edreform&amp;rdquo;, is whether or not content matters. Broadly, there are two camps, let&amp;rsquo;s call them the &amp;ldquo;Facts&amp;rdquo; and &amp;ldquo;Skills&amp;rdquo;, with the &amp;ldquo;Skills&amp;rdquo; camp clearly ahead in terms of mind share.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Skills&amp;rdquo; is based on a fundamentally intuitive insight&amp;ndash; students need to know how to do things not about the things themselves. In many ways it is built on our common experience of forgetting facts over time. We need &lt;em&gt;21st century skills&lt;/em&gt;, not an accumulation of specific, privileged knowledge that fades over time. Whatever the latest technology, from encyclopedias to calculators through to Google, each generation decides that the tools that adults use end the necessity of knowing &lt;em&gt;about&lt;/em&gt; things rather than knowing how to &lt;em&gt;find&lt;/em&gt; things.&lt;/p&gt;

&lt;p&gt;This is very attractive. It seems to match our adult experiences accumulating knowledge and using it in our work. It seems to address students&amp;rsquo; boredom with learning &lt;em&gt;irrelevant&lt;/em&gt; information. It leaves space for groups to advocate for teaching whatever content they want since everyone can argue that content is fundamentally limited in value.&lt;/p&gt;

&lt;p&gt;In classic &lt;strong&gt;&lt;a href=&#34;http://www.json.blog/img/turnsout.mp3&#34;&gt;turns out&lt;/a&gt;&lt;/strong&gt; sense, however, the evidence keeps mounting that one must teach from the &amp;ldquo;Facts&amp;rdquo; approach to achieve the goals of the &amp;ldquo;Skills&amp;rdquo; position.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Turns out:&lt;/strong&gt; skills and knowledge do not transfer well across domains. There is little evidence that learning how to read literary fiction translates to reading technical manuals with comprehension. In other words, critical thinking is not really an independent ability free of domain context &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:critthinking&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:critthinking&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In fact, experts are able to learn more quickly, but only in their domain and only when they have prior knowledge to use as scaffolding &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:scaffold&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:scaffold&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Turns out:&lt;/strong&gt; reading comprehension is strongly connected to whether or not students have prior knowledge (&amp;ldquo;Facts&amp;rdquo;) about the topic of the passage &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:priorknowledge&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:priorknowledge&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Reading techniques only provide modest assistance for comprehension.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Turns out:&lt;/strong&gt; privileging skills over content may have a serious differential impact on disadvantaged children. A well-intentioned goal of achieving equity through equality has led many to advocate that we do a disservice to children of color and children in poverty because their schools have not as completely embraced a &amp;ldquo;Skills&amp;rdquo; world and are too focused on &amp;ldquo;Facts&amp;rdquo;. The problem is that deep disparities we see when these students enter schooling point to having less prior knowledge than their peers &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:culturalliteracy&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:culturalliteracy&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;What is remarkable, and tragic, is that the &amp;ldquo;Skills&amp;rdquo; camp has maintained its dominance through the demonization of &amp;ldquo;Facts&amp;rdquo;, with dramatic misinterpretations like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &amp;ldquo;Facts&amp;rdquo; folks are just White colonialists seeking to maintain existing power structures through teaching the information of privilege.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;Facts&amp;rdquo; folks privilege memorization, rote learning, and recall-based assessment over other pedagogy that is more engaging and authentic.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;Facts&amp;rdquo; folks can only ever teach what was important yesterday; &amp;ldquo;Skills&amp;rdquo; camp can teach what matters to become a lifelong learner for tomorrow&amp;rsquo;s world.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;None of these are true.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is largely brought to you by: E.D. Hirsch, Dan T. Willingham, and Malcolm Gladwell via Merlin Mann.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:critthinking&#34;&gt;[&lt;a href=&#34;http://www.aft.org/pdfs/americaneducator/summer2007/Crit_Thinking.pdf](&#34;&gt;http://www.aft.org/pdfs/americaneducator/summer2007/Crit_Thinking.pdf](&lt;/a&gt;)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:critthinking&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:scaffold&#34;&gt;[&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/11550744](&#34;&gt;http://www.ncbi.nlm.nih.gov/pubmed/11550744](&lt;/a&gt;)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:scaffold&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:priorknowledge&#34;&gt;[&lt;a href=&#34;http://www.aft.org/newspubs/periodicals/ae/spring2006/willingham.cfm](&#34;&gt;http://www.aft.org/newspubs/periodicals/ae/spring2006/willingham.cfm](&lt;/a&gt;)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:priorknowledge&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:culturalliteracy&#34;&gt;This has pretty much been the thrust behind E.D. Hirsch&amp;rsquo;s work, who has been accused of being on the far right in education, despite his consistent belief that education equity is one of the most important goals to achieve. His firm belief, and I am mostly convinced, is that explicit factual content is the key tool for how teaching can dramatically improve educational equity.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:culturalliteracy&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The four ways to really fix education</title>
      <link>http://www.json.blog/2013/11/the-four-ways-to-really-fix-education/</link>
      <pubDate>Tue, 19 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/the-four-ways-to-really-fix-education/</guid>
      <description>&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;More schooling, reoriented calendar&lt;/li&gt;
&lt;li&gt;Wider range of higher education&lt;/li&gt;
&lt;li&gt;Cheaper four-year degrees&lt;/li&gt;
&lt;li&gt;Eliminate property tax-based public education&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is an interesting list. I don&amp;rsquo;t agree with number four. There are several benefits to using property taxes not the least of which is their stability and lagged response during traditional economic downturns. However, there are many things we should do to reform our revenue system for education. I am keen on &lt;em&gt;more&lt;/em&gt; taxes on &amp;ldquo;property&amp;rdquo;, using land value taxes that are levvied either statewide or regionally to address some of the inequities traditional, highly localized property taxes can lead to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s Poverty Stupid... or is it?</title>
      <link>http://www.json.blog/2013/11/its-poverty-stupid...-or-is-it/</link>
      <pubDate>Mon, 18 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/11/its-poverty-stupid...-or-is-it/</guid>
      <description>&lt;p&gt;If I had to point to the key fissure in the education policy and research community it would be around poverty. Some seem to view it as an inexorable obstacle, deeply believing that the key improvement strategy is to decrease inequity of inputs. Some seem to view it as an obstacle that can be overcome by systems functioning at peak efficacy, deeply believing the great challenge is achieving that efficacy sustainably at scale. Both positions seem to grossly simplify causes and suggest policy structures and outcomes that are unachievable.&lt;/p&gt;

&lt;p&gt;Paraphrasing &lt;a href=&#34;http://www.kungfugrippe.com&#34;&gt;Merlin Mann&lt;/a&gt;, &lt;em&gt;always be skeptical of &amp;ldquo;turns out&amp;rdquo; research&lt;/em&gt;. In this case, are the results really that surprising? If they are, I might suggest that you have been focusing too much on the partial equilibrium impact of poverty and ignoring the bigger picture.&lt;/p&gt;

&lt;p&gt;Not that I think integration is likely, easy, quick, or magically fixes things.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incomplete Evidence</title>
      <link>http://www.json.blog/2013/10/incomplete-evidence/</link>
      <pubDate>Tue, 08 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/10/incomplete-evidence/</guid>
      <description>&lt;p&gt;I spent most of high school writing, practicing, and performing music. I played guitar in two separate bands, and was the lead vocalist in one of those bands, and played trumpet in various wind ensembles and the jazz band at school. When I wasn&amp;rsquo;t a part of the creation process myself, there is a pretty good chance I was listening to music. Back then, it seemed trivial to find a new artist or album to obsess over.&lt;/p&gt;

&lt;p&gt;Despite being steeped in music, I have always found it hard to write about. The truth is, I have limited ability to use words to explain just what makes a particular piece of music so wonderful. Oh sure, I could discuss structure, point out a particular hook in a particular section and how it sits in the mix. I could talk about the tone of the instrument or about quality of the performance or any number of other things. The problem with this language is it reduces what is great about &lt;em&gt;this&lt;/em&gt; piece of music to a description that could easily fit some other piece of music. Verbalizing the experience of music projects a woefully flattened artifact of something breathtaking.&lt;/p&gt;

&lt;p&gt;Now it might seem that recorded music has greatly diminished this challenge. After all, the experience of recorded music can scale&amp;ndash; anyone can listen. Unfortunately, I found this to be completely untrue. When I play music for other people, it actually sounds different than when I experience it for myself. Little complexities that seem crucial to the mix seem to cower and hide rather than loom large in the presence of others. It is not really feasible to point out what makes the song so great while listening, because it disrupts the experience. Worst of all, no one else seems to experience what I experience when I listen.&lt;/p&gt;

&lt;p&gt;Of course, all of this may seem obvious to someone who has read about aesthetics. I have not.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using R to Calculate Student Moblity</title>
      <link>http://www.json.blog/2013/09/using-r-to-calculate-student-moblity/</link>
      <pubDate>Mon, 23 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/09/using-r-to-calculate-student-moblity/</guid>
      <description>

&lt;p&gt;In a couple of previous posts, I outlined the importance of &lt;a href=&#34;filename|documentation-of-business-rules-and-analysis.md&#34;&gt;documenting business rules for common education statistics&lt;/a&gt; and described my take on how to &lt;a href=&#34;new-calculation-for-student-mobility.md&#34;&gt;best calculate student mobility&lt;/a&gt;. In this post, I will be sharing two versions of R function I wrote to implement this mobility calculation, reviewing their different structure and methods to reveal how I achieved an order of magnitude speed up between the two versions. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:optimization&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:optimization&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; At the end of this post, I will propose several future routes for optimization that I believe should lead to the ability to handle millions of student records in seconds.&lt;/p&gt;

&lt;h2 id=&#34;version-0-where-do-i-begin&#34;&gt;Version 0: Where Do I Begin?&lt;/h2&gt;

&lt;p&gt;The first thing I tend to do is whiteboard the rules I want to use through careful consideration and constant referal back to real data sets. By staying grounded in the data, I am less likely to encounter unexpected situations during my quality control. It also makes it much easier to develop test data, since I seek out outlier records in actual data during the business rule process.&lt;/p&gt;

&lt;p&gt;Developing test data is a key part of the development process. Without a compact, but sufficiently complex, set of data to try with a newly developed function, there is no way to know whether or not it does what I intend.&lt;/p&gt;

&lt;p&gt;Recall the &lt;a href=&#34;filename|new-calculation-for-student-mobility.md&#34;&gt;business rules for mobility&lt;/a&gt; that I have proposed, all of which came out of this whiteboarding process:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Entering the data with an enroll date after the start of the year counts as one move.&lt;/li&gt;
&lt;li&gt;Leaving the data with an exit date before the end of the year counts as one move.&lt;/li&gt;
&lt;li&gt;Changing schools sometime during the year without a large gap in enrollment counts as one move.&lt;/li&gt;
&lt;li&gt;Changing schools sometime during the year with a large gap in enrollment counts as two moves.&lt;/li&gt;
&lt;li&gt;Adjacent enrollment records for the same student in the same school without a large gap in enrollment does not count as moving.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Test data needs to represent each of these situations so that I can confirm the function is properly implementing each rule.&lt;/p&gt;

&lt;p&gt;Below is a copy of my test data. As an exercise, I recommend determining the number of &amp;ldquo;moves&amp;rdquo; each of these students should be credited with after applying the above stated business rules.&lt;/p&gt;

&lt;p&gt;| Unique Student ID   | School Code   | Enrollment Date   | Exit Date    |  |
| &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; |  |
| 1000000             | 10101         | 2012-10-15        | 2012-11-15   |  |
| 1000000             | 10103         | 2012-01-03        | 2013-03-13   |  |
| 1000000             | 10103         | 2012-03-20        | 2013-05-13   |  |
| 1000001             | 10101         | 2012-09-01        | 2013-06-15   |  |
| 1000002             | 10102         | 2012-09-01        | 2013-01-23   |  |
| 1000003             | 10102         | 2012-09-15        | 2012-11-15   |  |
| 1000003             | 10102         | 2013-03-15        | 2013-06-15   |  |
| 1000004             | 10103         | 2013-03-15        | NA           |  |&lt;/p&gt;

&lt;h2 id=&#34;version-1-a-naïve-implementation&#34;&gt;Version 1: A Naïve Implementation&lt;/h2&gt;

&lt;p&gt;Once I have developed business rules and a test data set, I like to quickly confirm that I can produce the desired results. That&amp;rsquo;s particularly true when it comes to implementing a new, fairly complex business rules. My initial implementation of a new algorithm does not need to be efficient, easily understood, or maintainable. My goal is simply to follow my initial hunch on how to accomplish a task and get it working. Sometimes this &lt;em&gt;naïve implementation&lt;/em&gt; turns out to be pretty close to my final implementation, but sometimes it can be quite far off. The main things I tend to improve with additional work are extensibility, readability, and performance.&lt;/p&gt;

&lt;p&gt;In the case of this mobility calculation, I knew almost immediately that my initial approach was not going to have good performance characteristics. Here is a step by step discussion of Version 1.&lt;/p&gt;

&lt;h3 id=&#34;function-declaration-parameters&#34;&gt;Function Declaration: Parameters&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;moves_calc &amp;lt;- function(df, 
                       enrollby,
                       exitby,
                       gap=14,
                       sid=&#39;sid&#39;, 
                       schid=&#39;schid&#39;,
                       enroll_date=&#39;enroll_date&#39;,
                       exit_date=&#39;exit_date&#39;)){
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I named my function &lt;code&gt;moves_calc()&lt;/code&gt; to match the style of &lt;code&gt;age_calc()&lt;/code&gt; which was submitted and accepted to the eeptools package. This new function has eight parameters.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df&lt;/code&gt;: a &lt;code&gt;data.frame&lt;/code&gt; containing the required data to do the mobility calculation.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;enrollby&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; or &lt;code&gt;Date&lt;/code&gt; in the format &lt;code&gt;YYYY-MM-DD&lt;/code&gt;. This parameter signifies the start of the school year. Students whose first enrollment is after this date will have an additional  &lt;code&gt;move&lt;/code&gt; under the assumption that they enrolled somewhere prior to the first enrollment record in the data. This does not (and likely should not) match the actual first day of the school year.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;exitby&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; or &lt;code&gt;Date&lt;/code&gt; in the format  &lt;code&gt;YYYY-MM-DD&lt;/code&gt;. This parameter signifies the end of the school year. Students whose last exit is before this date will have an additional &lt;code&gt;move&lt;/code&gt; under the assumption that they enrolled somewhere after this exit record that is excluded in the data. This date does not (and likely should not) match the actual last day of the school year.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gap&lt;/code&gt;: an atomic vector of type &lt;code&gt;numeric&lt;/code&gt; that signifies how long a gap must exist between student records to record an additional move for that student under the assumption that they enrolled somewhere in between the two records in the data that is not recorded.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sid&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; that represents the name of the vector in &lt;code&gt;df&lt;/code&gt; that contains the unique student identifier. The default value is &lt;code&gt;&#39;sid&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;schid&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; that represents the name of the vector in &lt;code&gt;df&lt;/code&gt; that contains the unique school identifier. The default value is &lt;code&gt;schid&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;enroll_date&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; that represents the name of the vector in &lt;code&gt;df&lt;/code&gt; that contains the enrollment date for each record. The default value is &lt;code&gt;enroll_date&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;exit_date&lt;/code&gt;: an atomic vector of type &lt;code&gt;character&lt;/code&gt; that represents the name of the vector in &lt;code&gt;df&lt;/code&gt; that contains the exit date for each record. The default value is &lt;code&gt;exit_date&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Most of these parameters are about providing flexibility around the naming of attributes in the data set. Although I often write functions for my own work which accept &lt;code&gt;data.frames&lt;/code&gt;, I can not help but to feel this is a bad practice. Assuming particular data attributes of the right name and type does not make for generalizable code. To make up for my shortcoming in this area, I have done my best to allow other users to enter whatever data column names they want, so long as they contain the right information to run the algorithm.&lt;/p&gt;

&lt;p&gt;The next portion of the function loads some of the required packages and is common to many of my custom functions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(&amp;quot;data.table&amp;quot; %in% rownames(installed.packages()) == FALSE){
    install.packages(&amp;quot;data.table&amp;quot;)
  } 
require(data.table)

if(&amp;quot;plyr&amp;quot; %in% rownames(installed.packages()) == FALSE){
    install.packages(&amp;quot;plyr&amp;quot;)
  } 
require(plyr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;type-checking-and-programmatic-defaults&#34;&gt;Type Checking and Programmatic Defaults&lt;/h3&gt;

&lt;p&gt;Next, I do extensive type-checking to make sure that &lt;code&gt;df&lt;/code&gt; is structured the way I expect it to be in order to run the algorithm. I do my best to supply humane &lt;code&gt;warning()&lt;/code&gt; and &lt;code&gt;stop()&lt;/code&gt; messages when things go wrong, and in some cases, set default values that may help the function run even if function is not called properly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if (!inherits(df[[enroll_date]], &amp;quot;Date&amp;quot;) | !inherits(df[[exit_date]], &amp;quot;Date&amp;quot;))
    stop(&amp;quot;Both enroll_date and exit_date must be Date objects&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;enroll_date&lt;/code&gt; and &lt;code&gt;exit_date&lt;/code&gt; both have to be &lt;code&gt;Date&lt;/code&gt; objects. I could have attempted to coerce those vectors into &lt;code&gt;Date&lt;/code&gt; types using &lt;code&gt;as.Date()&lt;/code&gt;, but I would rather not assume something like the date format. Since &lt;code&gt;enroll_date&lt;/code&gt; and &lt;code&gt;exit_date&lt;/code&gt; are the most critical attributes of each student, the function will &lt;code&gt;stop()&lt;/code&gt; if they are the incorrect type, informing the analyst to clean up the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(missing(enrollby)){
   enrollby &amp;lt;- as.Date(paste(year(min(df$enroll_date, na.rm=TRUE)),
                              &#39;-09-15&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
}else{
  if(is.na(as.Date(enrollby, format=&amp;quot;%Y-%m-%d&amp;quot;))){
     enrollby &amp;lt;- as.Date(paste(year(min(df$enroll_date, na.rm=TRUE)),
                               &#39;-09-15&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;
     warning(paste(&amp;quot;enrollby must be a string with format %Y-%m-%d,&amp;quot;,
                   &amp;quot;defaulting to&amp;quot;, 
                   enrollby, sep=&#39; &#39;))
  }else{
    enrollby &amp;lt;- as.Date(enrollby, format=&amp;quot;%Y-%m-%d&amp;quot;)
  }
}
if(missing(exitby)){
  exitby &amp;lt;- as.Date(paste(year(max(df$exit_date, na.rm=TRUE)),
                          &#39;-06-01&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
}else{
  if(is.na(as.Date(exitby, format=&amp;quot;%Y-%m-%d&amp;quot;))){
    exitby &amp;lt;- as.Date(paste(year(max(df$exit_date, na.rm=TRUE)),
                              &#39;-06-01&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
    warning(paste(&amp;quot;exitby must be a string with format %Y-%m-%d,&amp;quot;,
                  &amp;quot;defaulting to&amp;quot;, 
                  exitby, sep=&#39; &#39;))
  }else{
    exitby &amp;lt;- as.Date(exitby, format=&amp;quot;%Y-%m-%d&amp;quot;)
  }
}
if(!is.numeric(gap)){
  gap &amp;lt;- 14
  warning(&amp;quot;gap was not a number, defaulting to 14 days&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For maximum flexibility, I have parameterized the &lt;code&gt;enrollby&lt;/code&gt;, &lt;code&gt;exitby&lt;/code&gt;, and &lt;code&gt;gap&lt;/code&gt; used by the algorithm to determine student moves. An astute observer of the function declaration may have noticed I did not set default values for &lt;code&gt;enrollby&lt;/code&gt; or &lt;code&gt;exitby&lt;/code&gt;. This is because these dates are naturally going to be different which each year of data. As a result, I want to enforce their explicit declaration.&lt;/p&gt;

&lt;p&gt;However, we all make mistakes. So when I check to see if &lt;code&gt;enrollby&lt;/code&gt; or &lt;code&gt;exitby&lt;/code&gt; are &lt;code&gt;missing()&lt;/code&gt;, I do not stop the function if it returns &lt;code&gt;TRUE&lt;/code&gt;. Instead, I set the value &lt;code&gt;enrollby&lt;/code&gt; to September 15 in the year that matches the minimum (first) enrollment record and &lt;code&gt;exitby&lt;/code&gt; to June 1 in the year that matches the maximum (last) exit record. I then pop off a &lt;code&gt;warning()&lt;/code&gt; that informs the user the expected values for each parameter and what values I have defaulted them to. I chose to use &lt;code&gt;warning()&lt;/code&gt; because many R users set their environment to halt at &lt;code&gt;warnings()&lt;/code&gt;. They are generally not good and should be pursued and fixed. No one should depend upon the defaulting process I use in the function. But the defaults that can be determined programmatically are sensible enough that I did not feel the need to always halt the function in its place.&lt;/p&gt;

&lt;p&gt;I also check to see if &lt;code&gt;gap&lt;/code&gt; is, in fact, defined as a number. If not, I also throw a &lt;code&gt;warning()&lt;/code&gt; after setting &lt;code&gt;gap&lt;/code&gt; equal to default value of &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Is this all of the type and error-checking I could have included? Probably not, but I think this represents a very sensible set that make this function much more generalizable outside of my coding environment. This kind of checking may be overkill for a project that is worked on independently and with a single data set, but colleagues, including your future self, will likely be thankful for their inclusion if any of your code is to be reused.&lt;/p&gt;

&lt;h3 id=&#34;initializing-the-results&#34;&gt;Initializing the Results&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output &amp;lt;- data.frame(id = as.character(unique(df[[sid]])),
                     moves = vector(mode = &#39;numeric&#39;, 
                                    length = length(unique(df[[sid]]))))
output &amp;lt;- data.table(output, key=&#39;id&#39;)
df &amp;lt;- arrange(df, sid, enroll_date)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My naïve implementation uses a lot of &lt;code&gt;for&lt;/code&gt; loops, a no-no when it comes to R performance. One way to make &lt;code&gt;for&lt;/code&gt; loops a lot worse, and this is true in any language, is to reassign a variable within the loop. This means that each iteration has the overhead of creating and assigning that object. Especially when we are building up results for each observation, it is silly to do this. We know exactly how big the data will be and therefore only need to create the object once. We can then assign a much smaller part of that object (in this case, one value in a vector) rather than the whole object (a honking &lt;code&gt;data.table&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Our &lt;code&gt;output&lt;/code&gt; object is what the function returns. It is a simple &lt;code&gt;data.table&lt;/code&gt; containing all of the unique student identifiers and the number of &lt;code&gt;moves&lt;/code&gt; recorded for each student.&lt;/p&gt;

&lt;p&gt;The last line in this code chunk ensures that the data are arranged by the unique student identifier and enrollment date. This is key since the &lt;code&gt;for&lt;/code&gt; loops assume that they are traversing a student&amp;rsquo;s record sequentially.&lt;/p&gt;

&lt;h3 id=&#34;business-rule-1-the-latecomer&#34;&gt;Business Rule 1: The Latecomer&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for(i in 1:(length(df[[sid]])-1)){
  if(i&amp;gt;1 &amp;amp;&amp;amp; df[sid][i,]!=df[sid][(i-1),]){
    if(df[[&#39;enroll_date&#39;]][i]&amp;gt;enrollby){
      output[as.character(df[[sid]][i]), moves:=moves+1L]
    }
  }else if(i==1){
    if(df[[&#39;enroll_date&#39;]][i]&amp;gt;enrollby){
    output[as.character(df[[sid]][i]), moves:=moves+1L]
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first bit of logic checks if &lt;code&gt;sid&lt;/code&gt; in row &lt;code&gt;i&lt;/code&gt; is not equal to the &lt;code&gt;sid&lt;/code&gt; in the &lt;code&gt;i-1&lt;/code&gt; row. In other words, is this the first time we are observing this student? If it is, then row &lt;code&gt;i&lt;/code&gt; is the first observation for that student and therefore has the minimum enrollment date. The &lt;code&gt;enroll_date&lt;/code&gt; is checked against &lt;code&gt;enrollby&lt;/code&gt;. When &lt;code&gt;enroll_date&lt;/code&gt; is after &lt;code&gt;enrollby&lt;/code&gt;, then the &lt;code&gt;moves&lt;/code&gt; attribute for that &lt;code&gt;sid&lt;/code&gt; is incremented by 1. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:increments&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:increments&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Now, I didn&amp;rsquo;t really mention the conditional that &lt;code&gt;i&amp;gt;1&lt;/code&gt;. This is needed because there is no &lt;code&gt;i-1&lt;/code&gt; observation for the very first row of the &lt;code&gt;data.table&lt;/code&gt;. Therefore, &lt;code&gt;i==1&lt;/code&gt; is a special case where we once again perform the same check for &lt;code&gt;enroll_date&lt;/code&gt; and &lt;code&gt;enrollby&lt;/code&gt;. The &lt;code&gt;i&amp;gt;1&lt;/code&gt; condition is before the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; operator, which ensures the statement after the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; is not evaluated when the first conditional is &lt;code&gt;FALSE&lt;/code&gt;. This avoids an &amp;ldquo;out of bounds&amp;rdquo;-type error where R tries to check &lt;code&gt;df[0]&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;business-rule-5-the-feint&#34;&gt;Business Rule 5: The Feint&lt;/h3&gt;

&lt;p&gt;Yeah, yeah&amp;ndash; the business rule list above doesn&amp;rsquo;t match the order of my function. That&amp;rsquo;s ok. Remember, sometimes giving instructions to a computer does not follow the way you would organize instructions for humans.&lt;/p&gt;

&lt;p&gt;Remember, the function is traversing through our &lt;code&gt;data.frame&lt;/code&gt; one row at a time. First I checked to see if the function is at the first record for a particular student. Now I check to see if there are any records after the current record.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  if(df[sid][i,]==df[sid][(i+1),]){
    if(as.numeric(difftime(df[[&#39;enroll_date&#39;]][i+1], 
                           df[[&#39;exit_date&#39;]][i], units=&#39;days&#39;)) &amp;lt; gap &amp;amp;
       df[schid][(i+1),]==df[schid][i,]){
        next
    }else if ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the case where the &lt;code&gt;i+1&lt;/code&gt; record has the same &lt;code&gt;sid&lt;/code&gt;, then the &lt;code&gt;enroll_date&lt;/code&gt; of &lt;code&gt;i+1&lt;/code&gt; is subtracted from the &lt;code&gt;exit_date&lt;/code&gt; of &lt;code&gt;i&lt;/code&gt; and checked against &lt;code&gt;gap&lt;/code&gt;. If it is both less than &lt;code&gt;gap&lt;/code&gt; and the &lt;code&gt;schid&lt;/code&gt; of &lt;code&gt;i+1&lt;/code&gt; is the same as &lt;code&gt;i&lt;/code&gt;, then &lt;code&gt;next&lt;/code&gt;, which basically breaks out of this conditional and moves on without altering moves. In other words, students who are in the same school with only a few days between the time they exited are not counting has having moved.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;...&lt;/code&gt; above is not the special &lt;code&gt;...&lt;/code&gt; in R, rather, I&amp;rsquo;m continuing that line below.&lt;/p&gt;

&lt;h3 id=&#34;business-rule-3-the-smooth-mover&#34;&gt;Business Rule 3: The Smooth Mover&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  }else if(as.numeric(difftime(df[[&#39;enroll_date&#39;]][i+1], 
                               df[[&#39;exit_date&#39;]][i], 
                               units=&#39;days&#39;)) &amp;lt; gap){
    output[as.character(df[[sid]][i]), moves:=moves+1L] 
  }else{ ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we have the simple case where a student has moved to another school (recall, this is still within the &lt;code&gt;if&lt;/code&gt; conditional where the next record is the same student as the current record) with a very short period of time between the &lt;code&gt;exit_date&lt;/code&gt; at the current record and the &lt;code&gt;enroll_date&lt;/code&gt; of the next record. This is considered a &amp;ldquo;seamless&amp;rdquo; move from one school to another, and therefore that student&amp;rsquo;s moves are incremented by 1.&lt;/p&gt;

&lt;h3 id=&#34;business-rule-4-the-long-hop&#34;&gt;Business Rule 4: The Long Hop&lt;/h3&gt;

&lt;p&gt;Our final scenario for a student moving between schools is when the gap between the &lt;code&gt;exit_date&lt;/code&gt; at the &lt;code&gt;i&lt;/code&gt; school and the &lt;code&gt;enroll_date&lt;/code&gt; at the &lt;code&gt;i+1&lt;/code&gt; school is large, defined as &lt;code&gt;&amp;gt; gap&lt;/code&gt;. In this scenario, the assumption is that the student moved to a jurisdiction outside of the data set, such as out of district for district-level data or out of state for state level data, and enrolled in at least one school not present in their enrollment record. The result is these students receive &lt;code&gt;2 moves&lt;/code&gt;&amp;ndash; one out from the &lt;code&gt;i&lt;/code&gt; school to a missing school and one in to the &lt;code&gt;i+1&lt;/code&gt; school from the missing school.&lt;/p&gt;

&lt;p&gt;The code looks like this (again a repeat from the &lt;code&gt;else{...&lt;/code&gt; above which was not using the &lt;code&gt;...&lt;/code&gt; character):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  }else{
    output[as.character(df[[sid]][i]), moves:=moves+2L] 
  }
}else...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This ends with a &lt;code&gt;}&lt;/code&gt; which closes the &lt;code&gt;if&lt;/code&gt; conditional that checked if the &lt;code&gt;i+1&lt;/code&gt; student was the same as the &lt;code&gt;i&lt;/code&gt; student, leaving only one more business rule to check.&lt;/p&gt;

&lt;h3 id=&#34;business-rule-2-the-early-summer&#34;&gt;Business Rule 2: The Early Summer&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;}else{
  if(is.na(df[[&#39;exit_date&#39;]][i])){
    next
  }else if(df[[&#39;exit_date&#39;]][i] &amp;lt; exitby){
        output[as.character(df[[sid]][i]), moves:=moves+1L]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that this &lt;code&gt;else&lt;/code&gt; block is only called if &lt;code&gt;sid&lt;/code&gt; of the &lt;code&gt;i+1&lt;/code&gt; record is not the same as &lt;code&gt;i&lt;/code&gt;. This means that this is the final entry for a particular student. First, I check to see if that student has a missing &lt;code&gt;exit_date&lt;/code&gt; and if so charges no &lt;code&gt;move&lt;/code&gt; to the student implementing the &lt;code&gt;next&lt;/code&gt; statement to break out of this iteration of the loop. Students never have missing &lt;code&gt;enroll_date&lt;/code&gt; for any of the data I have seen over 8 years. This is because most systems minimally autogenerate the &lt;code&gt;enroll_date&lt;/code&gt; for the current date when a student first enters a student information system. However, sometimes districts forget to properly exit a student and are unable to supply an accurate &lt;code&gt;exit_date&lt;/code&gt;. In a very small number of cases I have seen these missing dates. So I do not want the function to fail in this scenario. My solution here was simply to break out and move to the next iteration of the loop.&lt;/p&gt;

&lt;p&gt;Finally, I apply the last rule, which compares the final &lt;code&gt;exit_date&lt;/code&gt; for a student to &lt;code&gt;exitby&lt;/code&gt;, incrementing &lt;code&gt;moves&lt;/code&gt; if the student left prior to the end of the year and likely enrolled elsewhere before the summer.&lt;/p&gt;

&lt;p&gt;The last step is to close the &lt;code&gt;for&lt;/code&gt; loop and &lt;code&gt;return&lt;/code&gt; our result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  }
  return(output)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;version-2-10x-speed-and-more-readable&#34;&gt;Version 2: 10x Speed And More Readable&lt;/h2&gt;

&lt;p&gt;The second version of this code is vastly quicker.&lt;/p&gt;

&lt;p&gt;The opening portion of the code, including the error checking is essentially a repeat of before, as is the initialization of the output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;moves_calc &amp;lt;- function(df, 
                       enrollby,
                       exitby,
                       gap=14,
                       sid=&#39;sasid&#39;, 
                       schid=&#39;schno&#39;,
                       enroll_date=&#39;enroll_date&#39;,
                       exit_date=&#39;exit_date&#39;){
  if(&amp;quot;data.table&amp;quot; %in% rownames(installed.packages()) == FALSE){
    install.packages(&amp;quot;data.table&amp;quot;)
  } 
  require(data.table)
  if (!inherits(df[[enroll_date]], &amp;quot;Date&amp;quot;) | !inherits(df[[exit_date]], &amp;quot;Date&amp;quot;))
      stop(&amp;quot;Both enroll_date and exit_date must be Date objects&amp;quot;)
  if(missing(enrollby)){
    enrollby &amp;lt;- as.Date(paste(year(min(df[[enroll_date]], na.rm=TRUE)),
                              &#39;-09-15&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
  }else{
    if(is.na(as.Date(enrollby, format=&amp;quot;%Y-%m-%d&amp;quot;))){
      enrollby &amp;lt;- as.Date(paste(year(min(df[[enroll_date]], na.rm=TRUE)),
                                &#39;-09-15&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
      warning(paste(&amp;quot;enrollby must be a string with format %Y-%m-%d,&amp;quot;,
                    &amp;quot;defaulting to&amp;quot;, 
                    enrollby, sep=&#39; &#39;))
    }else{
      enrollby &amp;lt;- as.Date(enrollby, format=&amp;quot;%Y-%m-%d&amp;quot;)
    }
  }
  if(missing(exitby)){
    exitby &amp;lt;- as.Date(paste(year(max(df[[exit_date]], na.rm=TRUE)),
                            &#39;-06-01&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
  }else{
    if(is.na(as.Date(exitby, format=&amp;quot;%Y-%m-%d&amp;quot;))){
      exitby &amp;lt;- as.Date(paste(year(max(df[[exit_date]], na.rm=TRUE)),
                                &#39;-06-01&#39;, sep=&#39;&#39;), format=&#39;%Y-%m-%d&#39;)
      warning(paste(&amp;quot;exitby must be a string with format %Y-%m-%d,&amp;quot;,
                    &amp;quot;defaulting to&amp;quot;, 
                    exitby, sep=&#39; &#39;))
    }else{
      exitby &amp;lt;- as.Date(exitby, format=&amp;quot;%Y-%m-%d&amp;quot;)
    }
  }
  if(!is.numeric(gap)){
    gap &amp;lt;- 14
    warning(&amp;quot;gap was not a number, defaulting to 14 days&amp;quot;)
  }
  output &amp;lt;- data.frame(id = as.character(unique(df[[sid]])),
                       moves = vector(mode = &#39;numeric&#39;, 
                                      length = length(unique(df[[sid]]))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where things start to get interesting is in the calculation of the number of student moves.&lt;/p&gt;

&lt;h3 id=&#34;handling-missing-data&#34;&gt;Handling Missing Data&lt;/h3&gt;

&lt;p&gt;One of the clever bits of code I forgot about when I initially tried to refactor Version 1 appears under &amp;ldquo;Business Rule 2: The Early Summer&amp;rdquo;. When the &lt;code&gt;exit_date&lt;/code&gt; is missing, this code simply breaks out of the loop:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  if(is.na(df[[&#39;exit_date&#39;]][i])){
    next
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the new code will not be utilizing &lt;code&gt;for&lt;/code&gt; loops or really any more of the basic control flow, I had to device a different way to treat missing data. The steps to apply the business rules that I present below will fail spectacularly with missing data.&lt;/p&gt;

&lt;p&gt;So the first thing that I do is select the students who have missing data, assign the &lt;code&gt;moves&lt;/code&gt; in the &lt;code&gt;output&lt;/code&gt; to &lt;code&gt;NA&lt;/code&gt;, and then subset the data to exclude these students.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;incomplete &amp;lt;- df[!complete.cases(df[, c(enroll_date, exit_date)]), ]
if(dim(incomplete)[1]&amp;gt;0){
  output[which(output[[&#39;id&#39;]] %in% incomplete[[sid]]),][[&#39;moves&#39;]] &amp;lt;- NA
}
output &amp;lt;- data.table(output, key=&#39;id&#39;)
df &amp;lt;- df[complete.cases(df[, c(enroll_date, exit_date)]), ]
dt &amp;lt;- data.table(df, key=sid)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;woe-with-data-table&#34;&gt;Woe with &lt;code&gt;data.table&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Now with the data complete and in a &lt;code&gt;data.table&lt;/code&gt;, I have to do a little bit of work to assist with my frustrations with &lt;code&gt;data.table&lt;/code&gt;. Because &lt;code&gt;data.table&lt;/code&gt; does a lot of work with the &lt;code&gt;[&lt;/code&gt; operator, I find it very challenging to use a string argument to reference a column in the data. So I just gave up and internally rename these attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dt$sasid &amp;lt;- as.factor(as.character(dt$sasid))
setnames(dt, names(dt)[which(names(dt) %in% enroll_date)], &amp;quot;enroll_date&amp;quot;)
setnames(dt, names(dt)[which(names(dt) %in% exit_date)], &amp;quot;exit_date&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;magic-with-data-table-business-rules-1-and-2-in-two-lines-each&#34;&gt;Magic with &lt;code&gt;data.table&lt;/code&gt;: Business Rules 1 and 2 in two lines each&lt;/h3&gt;

&lt;p&gt;Despite by challenges with the way that &lt;code&gt;data.table&lt;/code&gt; re-imagines &lt;code&gt;[&lt;/code&gt;, it does allow for clear, simple syntax for complex processes. Gone are the &lt;code&gt;for&lt;/code&gt; loops and conditional blocks. How does &lt;code&gt;data.table&lt;/code&gt; allow me to quickly identified whether or not a students first or last enrollment are before or after my cutoffs?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;first &amp;lt;- dt[, list(enroll_date=min(enroll_date)), by=sid]
output[id %in% first[enroll_date&amp;gt;enrollby][[sid]], moves:=moves+1L]
last &amp;lt;- dt[, list(exit_date=max(exit_date)), by=sid]  
output[id %in% last[exit_date&amp;lt;exitby][[sid]], moves:=moves+1L]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Line 1 creates a &lt;code&gt;data.table&lt;/code&gt; with the student identifier and a new &lt;code&gt;enroll_date&lt;/code&gt; column that is equal to the minimum &lt;code&gt;enroll_date&lt;/code&gt; for that student.&lt;/p&gt;

&lt;p&gt;The second line is very challenging to parse if you&amp;rsquo;ve never used &lt;code&gt;data.table&lt;/code&gt;. The first argument for &lt;code&gt;[&lt;/code&gt; in &lt;code&gt;data.table&lt;/code&gt; is a subset/select function. In this case,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;id %in% first[enroll_date&amp;gt;enrollby][[sid]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;means,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Select the rows in &lt;code&gt;first&lt;/code&gt; where the &lt;code&gt;enroll_date&lt;/code&gt; attribute (which was previously assigned as the minimum &lt;code&gt;enroll_date&lt;/code&gt;) is less than the global function argument &lt;code&gt;enrollby&lt;/code&gt; and check if the &lt;code&gt;id&lt;/code&gt; of &lt;code&gt;output&lt;/code&gt; is in the &lt;code&gt;sid&lt;/code&gt; vector.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So &lt;code&gt;output&lt;/code&gt; is being subset to only include those records that meet that condition, in other words, the students who should have a move because they entered the school year late.&lt;/p&gt;

&lt;p&gt;The second argument of &lt;code&gt;[&lt;/code&gt; for &lt;code&gt;data.tables&lt;/code&gt; is explained in this footnote &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:increments&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:increments&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; if you&amp;rsquo;re not familiar with it.&lt;/p&gt;

&lt;h3 id=&#34;recursion-which-is-also-known-as-recursion&#34;&gt;Recursion. Which is also known as recursion.&lt;/h3&gt;

&lt;p&gt;The logic for Business Rules 3-5 are substantially more complex. At first it was not plainly obvious how to avoid a slow &lt;code&gt;for&lt;/code&gt; loop for this process. Each of the rules on switching schools requires an awareness of context&amp;ndash; how does one record of a student compare to the very next record for that student?&lt;/p&gt;

&lt;p&gt;The breakthrough was thinking back to my single semester of computer science and the concept of recursion. I created a new function inside of this function that can count how many moves are associated with a set of enrollment records, ignoring the considerations in Business Rules 1 and 2. Here&amp;rsquo;s my solution. I decided to include inline comments because I think it&amp;rsquo;s easier to understand that way.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;school_switch &amp;lt;- function(dt, x=0){
  # This function accepts a data.table dt and initializes the output to 0.
    if(dim(dt)[1]&amp;lt;2){
    # When there is only one enrollment record, there are no school changes to
    # apply rules 3-5. Therefore, the function returns the value of x. If the
    # initial data.table contains a student with just one enrollment record, 
    # this function will return 0 since we initialize x as 0.
      return(x)
    }else{
      # More than one record, find the minimum exit_date which is the &amp;quot;first&amp;quot;
      # record
      exit &amp;lt;- min(dt[, exit_date])
      # Find out which school the &amp;quot;first&amp;quot; record was at.
      exit_school &amp;lt;- dt[exit_date==exit][[schid]]
      # Select which rows come after the &amp;quot;first&amp;quot; record and only keep them
      # in the data.table
      rows &amp;lt;- dt[, enroll_date] &amp;gt; exit
      dt &amp;lt;- dt[rows,]
      # Find the minimum enrollment date in the subsetted table. This is the
      # enrollment that follows the identified exit record
      enroll &amp;lt;- min(dt[, enroll_date])
      # Find the school associated with that enrollment date
      enroll_school &amp;lt;- dt[enroll_date==enroll][[schid]]
      # When the difference between the enrollment and exit dates are less than
      # the gap and the schools are the same, there are no moves. We assign y,
      # our count of moves to x, whatever the number of moves were in this call
      # of school_switch
      if(difftime(min(dt[, enroll_date], na.rm=TRUE), exit) &amp;lt; gap &amp;amp;
         exit_school==enroll_school){
        y = x
      # When the difference in days is less than the gap (and the schools are
      # different), then our number of moves are incremented by 1.
      }else if(difftime(min(dt[, enroll_date], na.rm=TRUE), exit) &amp;lt; gap){
        y = x + 1L
      }else{
      # Whenever the dates are separated by more than the gap, regardless of which
      # school a student is enrolled in at either point, we increment by two.
        y = x + 2L
      }
      # Explained below outside of the code block.
      school_switch(dt, y)
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The recursive aspect of this method is calling &lt;code&gt;school_switch&lt;/code&gt; within &lt;code&gt;school_switch&lt;/code&gt; once the function reaches its end. Because I subset out the row with the minimum &lt;code&gt;exit_date&lt;/code&gt;, the &lt;code&gt;data.table&lt;/code&gt; has one row processed with each iteration of &lt;code&gt;school_switch&lt;/code&gt;. By passing the number of moves, &lt;code&gt;y&lt;/code&gt; back into &lt;code&gt;school_switch&lt;/code&gt;, I am &amp;ldquo;saving&amp;rdquo; my work from each iteration. Only when a single row remains for a particular student does the function &lt;code&gt;return&lt;/code&gt; a value.&lt;/p&gt;

&lt;p&gt;This function is called using &lt;code&gt;data.table&lt;/code&gt;&amp;rsquo;s special &lt;code&gt;.SD&lt;/code&gt; object, which accesses the subset of the full &lt;code&gt;data.table&lt;/code&gt; when using the &lt;code&gt;by&lt;/code&gt; argument.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dt[, moves:= school_switch(.SD), by=sid]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This calls &lt;code&gt;school_switch&lt;/code&gt; after splitting the &lt;code&gt;data.table&lt;/code&gt; by each &lt;code&gt;sid&lt;/code&gt; and then stitches the work back together, split-apply-combine style, resulting in a &lt;code&gt;data.table&lt;/code&gt; with a set of &lt;code&gt;moves&lt;/code&gt; per student identifier. With a little bit of clean up, I can simply add these moves to those recorded earlier in &lt;code&gt;output&lt;/code&gt; based on Business Rules 1 and 2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  dt &amp;lt;- dt[,list(switches=unique(moves)), by=sid]
  output[dt, moves:=moves+switches]
  return(output)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;quick-and-dirty-system-time&#34;&gt;Quick and Dirty &lt;code&gt;system.time&lt;/code&gt;&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:optimization&#34;&gt;On a mid-2012 Macbook Air, the current mobility calculation is very effective with tens of thousands of student records and practical for use in the low-hundreds of thousands of records range.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:optimization&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:increments&#34;&gt;I thought I was going to use &lt;code&gt;data.table&lt;/code&gt; for some of its speedier features as I wrote this initial function. I didn&amp;rsquo;t in this go (though I do in Version 2). However, I do find the &lt;code&gt;data.table&lt;/code&gt; syntax for assigning values to be really convenient, particularly the &lt;code&gt;:=&lt;/code&gt; operator which is common in several other languages. In &lt;code&gt;data.table&lt;/code&gt;, the syntax &lt;code&gt;dt[,name:=value]&lt;/code&gt; assigns &lt;code&gt;value&lt;/code&gt; to an exist (or new) column called &lt;code&gt;name&lt;/code&gt;. Because of the need &lt;code&gt;select&lt;/code&gt; operator in &lt;code&gt;data.table&lt;/code&gt;, I can just use &lt;code&gt;dt[id,moves:=moves+1L]&lt;/code&gt; to select only the rows where the table &lt;code&gt;key&lt;/code&gt;, in this case &lt;code&gt;sid&lt;/code&gt;, matches &lt;code&gt;id&lt;/code&gt;, and then increment &lt;code&gt;moves&lt;/code&gt;. Nice.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:increments&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A New Calculation for Student Mobility</title>
      <link>http://www.json.blog/2013/09/a-new-calculation-for-student-mobility/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/09/a-new-calculation-for-student-mobility/</guid>
      <description>

&lt;p&gt;How do we calculate student mobility? I am currently soliciting responses from other data professionals across the country. But when I needed to produce mobility numbers for some of my work a couple of months ago, I decided to develop a set of business rules without any exposure to how the federal government, states, or other existing systems define mobility. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:ignorance&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:ignorance&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I am fairly proud of my work on mobility. This post will review how I defined student mobility. I am hopeful that it matches or bests current techniques for calculating the number of schools a student has attended. In my next post, I will share the first two major versions of my implementation of these mobility business rules in &lt;code&gt;R&lt;/code&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:learningexperience&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:learningexperience&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Together, these posts will represent the work I referred to in my previous post on the &lt;a href=&#34;|filename|documentation-of-business-rules-and-analysis.md&#34;&gt;importance of documenting business rules and sharing code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-rules&#34;&gt;The Rules&lt;/h2&gt;

&lt;p&gt;Working with district data presents a woefully incomplete picture of the education mobile students receive. Particularly in a state like Rhode Island, where our districts are only a few miles wide, there is substantial interdistrict mobility. When a student moves across district lines, their enrollment is not recorded in local district data. However, even with state level data, highly mobile students cross state lines and present incomplete data. A key consideration for calculating how many schools a student has attended in a particular year is capturing &amp;ldquo;missing&amp;rdquo; data sensibly.&lt;/p&gt;

&lt;p&gt;The typical structure of enrollment records looks something like this:&lt;/p&gt;

&lt;p&gt;| Unique Student ID | School Code | Enrollment Date | Exit Date ||
|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|
| 1000000           | 10101       | 2012-09-01      | 2012-11-15 |
| 1000000           | 10103       | 2012-11-16      | 2013-06-15 |&lt;/p&gt;

&lt;p&gt;A compound key for this data consists of the Unique Student ID, School Code, and Enrollment Date, meaning that each row must be a unique combination of these three factors. The data above shows a simple case of a student enrolling at the start of the school year, switching schools once with no gap in enrollment, and continuing at the new school until the end of the school year. For the purposes of mobility, I would define the above as having moved one time.&lt;/p&gt;

&lt;p&gt;But it is easy to see how some very complex scenarios could quickly arise. What if student &lt;code&gt;1000000&lt;/code&gt;&amp;rsquo;s record looked like this?&lt;/p&gt;

&lt;p&gt;| Unique Student ID | School Code | Enrollment Date | Exit Date ||
|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|
| 1000000           | 10101       | 2012-10-15      | 2012-11-15 |
| 1000000           | 10103       | 2013-01-03      | 2013-03-13 |
| 1000000           | 10103       | 2013-03-20      | 2013-05-13 |&lt;/p&gt;

&lt;p&gt;There are several features that make it challenging to assign a number of &amp;ldquo;moves&amp;rdquo; to this student. First, the student does not enroll in school until October 15, 2012. This is nearly six weeks into the typical school year in the Northeastern United States. Should we assume that this student has enrolled in no school at all prior to October 15th or should we assume that the student was enrolled in a school that was outside of this district and therefore missing in the data? Next, we notice the enrollment gap between November 15, 2012 and January 3, 2013. Is it right to assume that the student has moved only once in this period of time with a gap of enrollment of over a month and a half? Then we notice that the student exited school &lt;code&gt;10103&lt;/code&gt; on March 13, 2013 but was re-enrolled in the same school a week later on March 20, 2013. Has the student truly &amp;ldquo;moved&amp;rdquo; in this period? Lastly, the student exits the district on May 13, 2013 for the final time. This is nearly a month before the end of school. Has this student moved to a different school?&lt;/p&gt;

&lt;p&gt;There is an element missing that most enrollment data has which can enrich our understanding of this student&amp;rsquo;s record. All district collect an exit type, which explains if a student is leaving to enroll in another school within the district, another school in a different district in the same state, another school in a different state, a private school, etc. It also defines whether a student is dropping out, graduating, or has entered the juvenile justice system, for example. However, it has been my experience that this data is reported inconsistently and unreliably. Frequently a student will be reported as changing schools within the district without a subsequent enrollment record, or reported as leaving the district but enroll within the same district a few days later. Therefore, I think that we should try and infer the number of schools that a student has attended using soley the enrollment date, exit date, and school code for each student record. This data is far more reliable for a host of reasons, and, ultimately, provides us with all the information we need to make intelligent decisions.&lt;/p&gt;

&lt;p&gt;My proposed set of business rules examines &lt;code&gt;school code&lt;/code&gt;, &lt;code&gt;enrollment date&lt;/code&gt;, and &lt;code&gt;exit date&lt;/code&gt; against three parameters: &lt;code&gt;enrollment by&lt;/code&gt;, &lt;code&gt;exit by&lt;/code&gt;, and &lt;code&gt;gap&lt;/code&gt;.
Each students minimum enrollment date is compared to &lt;code&gt;enrollment by&lt;/code&gt;. If that student entered the data set for the first time before the &lt;code&gt;enrollment by&lt;/code&gt;, the assumption is that this record represents the first time the student enrolls in any school for that year, and therefore the student has 0 &lt;code&gt;moves&lt;/code&gt;. If the student enrolls for the first time after &lt;code&gt;enrollment by&lt;/code&gt;, then the record is considered the second school a student has attended and their &lt;code&gt;moves&lt;/code&gt; attribute is incremented by &lt;code&gt;1&lt;/code&gt;. Similarly, if a student&amp;rsquo;s maximium &lt;code&gt;exit date&lt;/code&gt; is after &lt;code&gt;exit by&lt;/code&gt;, then this considered to be the student&amp;rsquo;s last school enrolled in for the year and they are credited with &lt;code&gt;0&lt;/code&gt; &lt;code&gt;moves&lt;/code&gt;, but if &lt;code&gt;exit date&lt;/code&gt; is prior to &lt;code&gt;exit by&lt;/code&gt;, then that student&amp;rsquo;s &lt;code&gt;moves&lt;/code&gt; is incremented by &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That takes care of the &amp;ldquo;ends&amp;rdquo;, but what happens as students switch schools in the &amp;ldquo;middle&amp;rdquo;? I proposed that each &lt;code&gt;exit date&lt;/code&gt; is compared to the subsequent &lt;code&gt;enrollment date&lt;/code&gt;. If &lt;code&gt;enrollment date&lt;/code&gt; occurs within &lt;code&gt;gap&lt;/code&gt; days of the previous &lt;code&gt;exit date&lt;/code&gt;, and the &lt;code&gt;school code&lt;/code&gt; of enrollment is not the same as the &lt;code&gt;school code&lt;/code&gt; of exit, then a student&amp;rsquo;s &lt;code&gt;moves&lt;/code&gt; are incremented by &lt;code&gt;1&lt;/code&gt;. If the &lt;code&gt;school codes&lt;/code&gt; are identical and the difference between dates is less than &lt;code&gt;gap&lt;/code&gt;, then the student is said to have not moved at all. If the difference between the &lt;code&gt;enrollment date&lt;/code&gt; and the previous &lt;code&gt;exit date&lt;/code&gt; is greater than &lt;code&gt;gap&lt;/code&gt;, then the student&amp;rsquo;s &lt;code&gt;moves&lt;/code&gt; is incremented by &lt;code&gt;2&lt;/code&gt;, the assumption being that the student likely attended a different school between the two observations in the data.&lt;/p&gt;

&lt;p&gt;Whereas calculating student mobility may have seemed a simple matter of counting the number of records in the enrollment file, clearly there is a level of complexity this would fail to capture.&lt;/p&gt;

&lt;p&gt;Check back in a few days to see my next post where I will share my initial implementation of these business rules and how I achieved an 10x speed up with a massive code refactor.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:ignorance&#34;&gt;My ignorance was intentional. It is good to stretch those brain muscles that think through sticky problems like developing business rules for a key statistic. I can&amp;rsquo;t be sure that I have developed the most considered and complete set of rules for mobility, which is why I&amp;rsquo;m now soliciting other&amp;rsquo;s views, but I am hopeful my solution is at least as good.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:ignorance&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:learningexperience&#34;&gt;I think showing my first two implementation of these business rules is an excellent opportunity to review several key design considerations when programming in &lt;code&gt;R&lt;/code&gt;. From version 1 to version 2 I achieved a 10x speedup due to a complete refactor that avoided &lt;code&gt;for&lt;/code&gt; loops, used &lt;code&gt;data.table&lt;/code&gt;, and included some clever use of recursion.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:learningexperience&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Documentation of Business Rules and Analysis</title>
      <link>http://www.json.blog/2013/09/documentation-of-business-rules-and-analysis/</link>
      <pubDate>Fri, 13 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/09/documentation-of-business-rules-and-analysis/</guid>
      <description>&lt;p&gt;One of the most challenging aspects of being a data analyst is translating programmatic terms like &amp;ldquo;student mobility&amp;rdquo; into precise business rules. Almost any simple statistic involves a series of decisions that are often opaque to the ultimate users of that statistic.&lt;/p&gt;

&lt;p&gt;Documentation of business rules is a critical aspect of a data analysts job that, in my experience, is often regrettably overlooked. If you have ever tried to reproduce someone else&amp;rsquo;s analysis, asked different people for the same statistic, or tried to compare data from multiple years, you have probably encountered difficulties getting a consistent answer on standard statistics, e.g. how many students were proficient in math, how many students graduated in four years, what proportion of students were chronically absent? All too often documentation of business rules is poor or non-existent. The result is that two analysts with the same data will produce inconsistent statistics. This is not because of something inherent in the quality of the data or an indictment of the analyst&amp;rsquo;s skills. In most cases, the undocumented business rules are essentially trivial, in that the results of any decision has a small impact on the final result and any of the decisions made by the analysts are equally defensible.&lt;/p&gt;

&lt;p&gt;This major problem of lax or non-existent documentation is one of the main reasons I feel that analysts, and in particular analysts working in the public sector, should extensively use tools for code sharing and version control like &lt;a href=&#34;http://www.github.com/&#34;&gt;Github&lt;/a&gt;, use free tools whenever possible, and generally adhere to best practices in reproducible research.&lt;/p&gt;

&lt;p&gt;I am trying to put as much of my code on Github as I can these days. Much of what I write is still very disorganized and, frankly, embarrassing. A lot of what is in my Github repositories is old, abandoned code written as I was learning my craft. A lot of it is written to work with very specific, private data. Most of it is poorly documented because I am the only one who has ever had to use it, I don&amp;rsquo;t interact with anyone through practices like code reviews, and frankly I am lazy when pressed with a deadline. But that&amp;rsquo;s not really the point, is it? The worst documented code is code that is hidden away on a personal hard drive, written for an expensive proprietary environment most people and organizations cannot use, or worse, is not code at all but rather a series of destructive data edits and manipulations. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fuckexcel&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fuckexcel&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;One way that I have been trying to improve the quality and utility of the code I write is by contributing to an open source R package, &lt;code&gt;eeptools&lt;/code&gt;. This is a package written and maintained by Jared Knowles, an employee of the Wisconsin Department of Public Instruction, whom I met at a &lt;a href=&#34;http://www.gse.harvard.edu/sdp/&#34;&gt;Strategic Data Project&lt;/a&gt; convening. &lt;code&gt;eeptools&lt;/code&gt; is consolidating several functions in R for common tasks education data analysts are faced with. Because this package is available on &lt;a href=&#34;http://cran.us.r-project.org/&#34;&gt;CRAN&lt;/a&gt;, the primary repository for R packages, any education analyst can have access to its functions in one line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#39;eeptools&#39;); require(eeptools)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Submitting code to a CRAN package reinforces several habits. First, I get to practice writing R documentation,  explaining how to use a function, and therefore, articulating the assumptions and business rules I am applying. Second, I have to write my code with a wider tolerance for input data. One of the easy pitfalls of a beginning analyst is writing code that is too specific to the dataset in front of you. Most of the errors I have found in analyses during quality control stem from assumptions embedded in code that were perfectly reasonable with a single data set that lead to serious errors when using different data. One way to avoid this issue is through &lt;a href=&#34;http://en.wikipedia.org/wiki/Test-driven_development&#34;&gt;test-driven development&lt;/a&gt;, writing a good testing suite that tests a wide range of unexpected inputs. I am not quite there yet, personally, but thinking about how my code would have to work with arbitrary inputs and ensuring it fails gracefully &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fallingwithstyle&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fallingwithstyle&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; is an excellent side benefit of preparing a pull request &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:gitterms&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:gitterms&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; . Third, it is an opportunity to write code for someone other than myself. Because I am often the sole analyst with my skillset working on a project, it is easy to not consider things like style, optimizations, clarity, etc. This can lead to large build-ups of &lt;a href=&#34;http://en.wikipedia.org/wiki/Technical_debt&#34;&gt;technical debt&lt;/a&gt;, complacency toward learning new techniques, and general sloppiness. Submitting a pull request feels like publishing. The world has to read this, so it better be something I am proud of that can stand up to the scrutiny of third-party users.&lt;/p&gt;

&lt;p&gt;My first pull request, which was accepted into the package, calculates age in years, months, or days at an arbitrary date based on date of birth. While even a beginning R programmer can develop a similar function, it is the perfect example of an easily compartmentalized component, with a broad set of applications, that can be accessed frequently .&lt;/p&gt;

&lt;p&gt;Today I submitted by second pull request that I hope will be accepted. This time I covered a much more complex task&amp;ndash; calculating student mobility. To be honest, I am completely unaware of existing business rules and algorithms used to produce the mobility numbers that are federally reported. I wrote this function from scratch thinking through how I would calculate the number of schools attended by a student in a given year. I am really proud of both the business rules I have developed and the code I wrote to apply those rules. My custom function can accept fairly arbitrary inputs, fails gracefully when it finds data it does not expect, and is pretty fast. The original version of my code took close to 10 minutes to run on ~30,000 rows of data. I have reduced that with a complete rewrite prior to submission to 16 seconds.&lt;/p&gt;

&lt;p&gt;While I am not sure if this request will be accepted, I will be thrilled if it is. Mobility is a tremendously important statistic in education research and a standard, reproducible way to calculate it would be a great help to researchers. How great would it be if &lt;code&gt;eeptools&lt;/code&gt; becomes one of the first packages education data analysts load and my mobility calculations are used broadly by researchers and analysts? But even if it&amp;rsquo;s not accepted because it falls out of scope, the process of developing the business rules, writing an initial implementation of those rules, and then refining that code to be far simpler, faster, and less error prone was incredibly rewarding.&lt;/p&gt;

&lt;p&gt;My next post will probably be a review of that process and some parts of my &lt;code&gt;moves_calc&lt;/code&gt; function that I&amp;rsquo;m particularly proud of.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:fuckexcel&#34;&gt;Using a spreadsheet program, such as Excel, encourages directly manipulating and editing the source data. Each change permanently changes the data. Even if you keep an original version of the data, there is no recording of exactly what was done to change the data to produce your results. Reproducibility is all but impossible of any significant analysis done using spreadsheet software.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fuckexcel&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fallingwithstyle&#34;&gt;Instead of halting the function with hard to understand error when things go wrong, I do my best to &amp;ldquo;correct&amp;rdquo; easily anticipated errors or report back to users in a plain way what needs to be fixed. See also &lt;a href=&#34;http://en.wikipedia.org/wiki/Fault-tolerant_system&#34;&gt;fault-tolerant system&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fallingwithstyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:gitterms&#34;&gt;A &lt;a href=&#34;https://help.github.com/articles/using-pull-requests&#34;&gt;pull request&lt;/a&gt; is when you submit your additions, deletions, or any other modifications to be incorporated in someone else&amp;rsquo;s repository.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:gitterms&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Changes for Rhode Island State Aid to Education</title>
      <link>http://www.json.blog/2013/08/some-changes-for-rhode-island-state-aid-to-education/</link>
      <pubDate>Thu, 22 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/08/some-changes-for-rhode-island-state-aid-to-education/</guid>
      <description>

&lt;p&gt;In December 2009, the education department head, Professor Kenneth K. Wong, another graduate student and myself were part of a three-person team consulting the Rhode Island Department of Education (RIDE) on how to establish a new state funding formula. We worked with finance and legal staff at the department to develop the legislation for the 2010 session that would establish a state funding formula for the first time in 15 years.
 
The Board of Regents had already passed a resolution with its policy priorities that they wanted enshrined in the formula. Additionally, there had been many attempts over the past 5-10 years to pass a new formula that failed for various reasons, chief among them that all previously proposed formulas were accompanied with a call to increase state funding for education 30-50%, with some even envisioning nearly doubling the state education funding. Our task was to research funding formulas, both in practice in other states and in the literature research on school finance, and achieve the goals of the Board of Regents without proposing a mammoth increase in state aid that would sink the entire endeavor &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:increase&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:increase&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The general sense was that while more state aid had the potential to improve the progressiveness of education expenditures, the reality is the overall spending level in Rhode Island is high, and introducing new money was less important than redistributing state aid. I share this belief, particularly because I think adding money to the right places is simple once there is already a way to equitably distribute those funds. Tying up the increase in funding alongside a distribution method is a recipe for political horse-trading that can result in all kinds of distortions that prevent aid from flowing where needed. &lt;/p&gt;

&lt;p&gt;My role in this process was primarily to create Excel simulators that would allow us to immediately track the impacts of changing different parts of the formula. I also helped RIDE staff interpret the meaning of changes to the math behind the funding formula and understand what levers existed to change the formula and how these changes impacted both the resulting distribution and policy. &lt;/p&gt;

&lt;p&gt;We had three months. &lt;/p&gt;

&lt;p&gt;There are a lot of people who are unhappy about the results of the formula that ultimately passed in June 2010. Because we are redistributing essentially the same amount of state aid, there are some districts that are losing money while others are gaining funds &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:holdharmless&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:holdharmless&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Some dislike the fact that we used only a &amp;ldquo;weight&amp;rdquo; for free and reduced price lunch status. Alternative formulas (and formulas in other states) typically include numerous weights, from limited English proficiency and special education status, to gifted and talented and career and technical education &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:ajello&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:ajello&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. And yet others were displeased that many costs, including transportation and facilities maintenance, were excluded from the state-base of education aid. Then there are those who think the transition is all off&amp;ndash; five years is too long to wait to get the increases the formula proposes, and ten years is far too fast to lose the money the formula proposes. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:transitions&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:transitions&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;

&lt;h3 id=&#34;a-good-state-aid-system&#34;&gt;A Good State Aid System &lt;/h3&gt;

&lt;p&gt;In the end, I am proud of the formula produced for several reasons. 
First, it passed successfully and has been fully funded (and sometimes more than fully funded) each year of implementation throughout a period of massive structural budget deficits. This is no small accomplishment. The advocacy community rightfully pushes us to build ideal systems, but in our role of policy entrepreneurs we are faced with the reality that a policy that does not become law and is not supported as law may as well not exist. Producing a formula that has some of the other positive qualities discussed below, passing that formula, and implementing the formula with little fanfare is not a small accomplishment. &lt;/p&gt;

&lt;p&gt;Second, the formula is highly progressive, sending as much as 20 times more aid to some of our poorest communities in Rhode Island compared to the wealthiest. I am not positive how this compares to other states&amp;ndash; that&amp;rsquo;s a topic I certainly want to work on for a future post&amp;ndash; but with just 39 cities and towns, it seems to show a high preference for &lt;em&gt;vertical equity&lt;/em&gt;, treating different cities and towns differently. There are communities on both ends of the distribution who want substantially more state funding, and our state aid formula is not sufficient to effectively crowd out local capacity for education spending and ensure that our poorest communities are spending more than our wealthier ones &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:comparative&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:comparative&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, but it&amp;rsquo;s a very strong start. &lt;/p&gt;

&lt;p&gt;Third, the formula is relatively simple. While I do not necessarily agree that it is a virtue to have fewer weights and a simple formula in perpetuity, the experience with other states and other formula-based programs show that weights and complexities are very easy to add and very hard to take away. Once a particular policy preference is enshrined in the distribution method, it had better be right because a community of advocacy will maintain that weight long into the future. Personally, I felt it critical to start with the very simple &amp;ldquo;core&amp;rdquo; formula that could be adjusted over time. I have some ideas on how I might modify/add to this core that I will be sharing in this post, but I firmly believe that starting with a simple core was the right move. It is also worth noting that because of the need to ensure the transition is smoothed out so that gains in some districts equal the total losses in the others meant that even a more progressive weighting scheme would not impact school funding until the far back end of the transition period (which we proposed as 7 years but was pushed to 5 years during the legislative process), since communities were already gaining funds as fast as we could move them. For this reason, not only was a simple core preferable from my technocratic perspective, but it also was not likely to have any immediate downside. &lt;/p&gt;

&lt;p&gt;Fourth, we removed the long-term regionalization bonuses. Rhode Island had sought to reduce the ridiculous number of school districts by providing a bonus for regionalizing in the early 90s. Unfortunately, because of the timing of the abandonment of the previous state aid formula, the districts that did choose to regionalize had their base funding locked in at a level 6-8% higher than it should have been, because they were receiving a bonus that was meant to fade away over the course of several years. I could justify a small increase in state funding to pay some of the transition expenses of regionalizing districts, but long term funding increases? Part of the goal of regionalization is the reduction of overhead that allows for decreased costs (or increased services at the same costs). There is no ongoing need to supply a massive state bonus for regionalizing. &lt;/p&gt;

&lt;p&gt;Now just because I am proud of this work does not mean that I think we have &amp;ldquo;solved&amp;rdquo; education funding in Rhode Island. Personally, I believe there are other defensible ways to distribute funding in Rhode Island, each of which represents slightly different policy preferences. There is no hard and fast &amp;ldquo;right&amp;rdquo; or &amp;ldquo;wrong&amp;rdquo; way to do this, within certain guidelines. As I see it, so long as the formula is progressive and moving toward a greater chance of seeing a day where Providence has the highest paid staff in the state &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:comparative&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:comparative&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, we are on the right path. I don&amp;rsquo;t believe that Rhode Island will have a truly &amp;ldquo;great&amp;rdquo; education finance climate without a substantial growth in the economy or a huge new tax that dramatically lowers the ability of municipalities to generate school funding while bolstering state aid. However, I think we have a great foundation and a &amp;ldquo;good&amp;rdquo; system. &lt;/p&gt;

&lt;p&gt;For the remainder of this post, I would like to propose a few ideas that could help move Rhode Island from &amp;ldquo;good&amp;rdquo; to &amp;ldquo;very good&amp;rdquo; that I think are feasible within the next five years &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:goodtogreat&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:goodtogreat&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;

&lt;h3 id=&#34;a-very-good-state-aid-program&#34;&gt;A Very Good State Aid Program &lt;/h3&gt;

&lt;p&gt;After a little over three years since its establishment, I think we are ready to tackle several additional aspects of state education funding in Rhode Island. One thing you may notice is that few of these ideas impact the original formula. Part of why that is comes from my aforementioned preference for a simple formula, and part is because these include some non-formula issues that were not pursued in 2010 in an effort to keep the focus on the main policy matters. 
First, and perhaps the most consequential change that can be made to state funding, is the teacher pension fund payments. Currently, the state and local districts split the cost of teacher pension contributions &lt;sup&gt;60&lt;/sup&gt;&amp;frasl;&lt;sub&gt;40&lt;/sub&gt;. This is a flat split, regardless of the wealth of the community. I think it&amp;rsquo;s absurd to ignore community wealth for such a large portion of state education expenditures. Using the Adjusted Equalized Weighted Assessment Values (AEWAV) to determine the reimbursement rates would be a big improvement on the progressiveness of school funding. &lt;/p&gt;

&lt;p&gt;Second, I would make a slight change to the way that we fund charter schools. When we were developing the formula, there was broad agreement among policymakers that the &amp;ldquo;money should follow the child&amp;rdquo;. In one sense, this is the system we proposed since school district funding is based on enrollments. However, I think an irrational desire to not &amp;ldquo;double count&amp;rdquo; students, alongside the need to keep funding as flat as possible, pushed the formula a bit too far when it comes to charters. The old way of funding charter schools allowed districts to hold back 5% of the total per pupil expenditure from their charter school tuitions. This meant charter schools received 5% less funding than traditional public schools, but it also recognized that there are some fixed costs in districts that are not immediately recoverable when students leave on the margins. I think the state should return to this practice, however only if the state is willing to pay the withheld 5% to charters. I do think its fair to take into account some fixed costs, but I don&amp;rsquo;t believe it&amp;rsquo;s fair that charter schools received less funding as a result. &lt;/p&gt;

&lt;p&gt;Third, we excluded all building maintenance costs from the base amount of state aid. This was largely because the formula was supposed to represent only the marginal instructional costs associated with each student. I don&amp;rsquo;t necessarily think that these costs have to be added into the base amount. However, I would like to see the state contribute to the maintenance of buildings more directly. I think the state should provide a flat dollar amount, say &amp;#36;100,000, per building in each district, provided that key criteria are met. The buildings should be at 90% occupancy/utilization, should have a minimum size set based on the research on efficiency (roughly 300 students at the elementary level and 600 students for high schools), and there should be some minimum standard for building systems quality and upkeep. These requirements are mostly about making sure this flat fund, which is really about the fixed costs of maintaining buildings, doesn&amp;rsquo;t create incentives to build more. It may seem inconsequential, but I think it&amp;rsquo;s important to state the preference for well-sized, occupied &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:occupied&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:occupied&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;, and maintained buildings is worthwhile. &lt;/p&gt;

&lt;p&gt;I think it&amp;rsquo;s wrong that the minimum reimbursement rate for school construction aid was raised to 40% during the funding formula debates in the General Assembly. This amounts to a massive subsidy for suburban schools and the previous 30% minimum is part of why we have such stark facilities inequities in the state. We should remove the minimums on construction reimbursement and simply use AEWAV to determine the reimbursement rate. Also, we need to establish a revolving facilities loan fund, much like the one used for sewers (and now &lt;a href=&#34;http://blogs.wpri.com/2013/03/21/ricwfa-explainer/&#34;&gt;roads and bridges&lt;/a&gt;). Access to lower interest bonds should not be dependent on city finances. &lt;/p&gt;

&lt;p&gt;Fourth, one thing we did not include in the original funding formula that has come under considerably criticism is a special weight for students who are labeled English language learners. There are a few reasons we made this decision. The districts that have ELLs are the same districts that have high levels of poverty. In fact, the five communities that had more than 5% of their students classified as ELLs were, in order, also the top five districts with regards to free and reduced price lunch eligibility. Combined with a transition plan that was already increasing funding to these districts as rapidly as could be afforded, there were virtually no short-term consequences of not including an ELL weight. It&amp;rsquo;s worth noting that formula dollars are not categorical funds&amp;ndash; there are no restrictions on how districts should spend this money, and there are no guarantees that an ELL weight would have any impact on ELL spending. &lt;/p&gt;

&lt;p&gt;We were also concerned with incentivizing over-identification and failing to exit students who should no longer be classified as ELLs. I am also personally concerned with mistaking the additional supports we want to target as needed for English language acquisition; it would not only inspire the wrong policies and supports for these students, but it fails to recognize a host of needs that persist for these students well beyond English acquisition. &lt;/p&gt;

&lt;p&gt;During the funding formula hearings at House and Senate Finance Committees we discussed the need for further study on this issue. I think that the next weight in the formula should be based on the Census and American Communities Survey. By using these data sets, classification of students who are eligible for the weight would not be dependent on the school district itself. Rather than focus on child language acquisition, I think we should broaden this weight to be applied based on the percentage of households that speak a language other than English in the home, where English is spoken at a level below &amp;ldquo;very well&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:technical&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:technical&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. This would ensure that students who live in language minority households receive additional supports throughout their education, regardless of their language acquisition status. I would make this weight lower than some in the literature because it would apply to a broader set of students, probably somewhere around 40% like the poverty weight. For reference, the latest five-year estimate from the ACS data shows that 24.3% of households fit this definition in the city of Providence. With a 40% weight, at 22,500 students, with a foundation amount of around \$9,000 per student, this weight would increase funding to Providence by a little over \$16,000,000. Similar to other formula aid, these funds would be unrestricted. &lt;/p&gt;

&lt;p&gt;Now, while I think that \$16,000,000 is no small potatoes, and I am happy to express our policy preference to drive funding into communities where families are not using English in the home, some perspective is warranted. Providence will receive almost \$240,000,000 in state aid when the formula is fully transitioned, compared to about \$190,000,000 before. Adding this weight would only represent a 6% increase in state aid from the full formula amount. It&amp;rsquo;s an important increase, but I hope you&amp;rsquo;ll forgive me if I felt it was not grossly unfair to exclude it in the first iteration of the funding formula, especially considering we still have not fully transitioned to those higher dollar amounts sent to districts that would benefit from these funds. &lt;/p&gt;

&lt;h3 id=&#34;it-takes-money&#34;&gt;It Takes Money &lt;/h3&gt;

&lt;p&gt;Each of these recommendations, in my view, would improve the way that Rhode Island distributes education aid. Some of the changes are technical, others address areas that are currently not considered, and some are purely about increasing the progressiveness of aid. All of these changes will require an even greater state contribution to education aid, but these increases would be an order of magnitude lower than what it would take to increase the state aid to covering 50-60% of all education expenditures. While I would support some pretty radical changes to drive more money into the state aid system, I think that each of these improvements are worth doing on the path to increased aid. &lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:increase&#34;&gt;I should note, that few people I spoke to were not in favor of raising the amount of state aid. We all want more money to come from the state because those dollars are far more progressive. However, Rhode Island was deep in its recession at this point in time and the dollar amounts to make a real dent in the state to local share in education are just staggering. Rhode Island currently funds just short of 40% of total school expenditures at the state level. To increase that to 60%, which is closer to the national average, they would have to contribute &amp;#36;500M more&amp;ndash; a roughly 60% increase from the current level. Just for some context, the main tax fight of Rhode Island progressives has been to repeal tax cuts for higher income individuals that were instituted starting in 2006 in an attempt to move toward a flat income tax rate in Rhode Island. The impact of this repeal would be an increase in revenues that would cover roughly 10% of the increase in school funding required to move from 40% to 60% state aid. Of course, those dollars are supposed to pay for some portion of restoring pension benefits, so it&amp;rsquo;s already spoken for.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:increase&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:holdharmless&#34;&gt;Hold harmless provisions, when introduced in other states, serve to dramatically distort the redistributive properties of state aid and almost always require a huge influx of funds. In fact, a hold harmless provision in Rhode Island would have required a doubling of state aid, which ultimately would have guaranteed that wealthy communities continue to receive too much state aid while less wealthy communities are stuck fighting year after year for tremendous revenue increases through taxation just to get their fair share. Essentially, hold harmless would ensure that you never reach formula-level spending and guarantee that state aid would not be very progressive.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:holdharmless&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:ajello&#34;&gt;One very popular progressive member of the Rhode Island General Assembly had been working for years to pass a new funding formula and had five or six such weights in her version. Interestingly, with the glaring exception of sending &amp;#36;0 to Newport in state aid, the difference in the overall distribution of funds by district in Rhode Island using this formula and our formula was tiny, almost always &amp;lt;5%.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:ajello&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:transitions&#34;&gt;Smoothing the &amp;ldquo;gains&amp;rdquo; and &amp;ldquo;losses&amp;rdquo; overtime was important to keep the formula as close to revenue neutral as possible. Of course, there are increases due to inflation and other factors each year as a part of the base, but our goal was to truly redistribute the funds such that not only is the end number not a big increase in total state aid but that getting through the transition period did not have huge costs. If it did, there is no way we could feel confident we would ever reach the point where the formula actually dictated state aid, much like the hold harmless provision prevents a full transition. Modeling various transition plans was a nightmare for me.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:transitions&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:comparative&#34;&gt;Many people forget that education spending is about competition within a single market. Overall spending matters less within this market than how you spend compared to others. The trick is that an urban school primarily working with traditionally under served families needs to be able to pay not just for more material supplies, but mostly for higher quality teachers and staff (and perhaps quantity). Because of compensating wage differentials, even hiring teachers and staff that are the same quality as wealthy communities costs more.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:comparative&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:goodtogreat&#34;&gt;Perhaps I will write a future post on some ideas of how to push Rhode Island to &amp;ldquo;great&amp;rdquo;, even though I view all of those solutions as politically impossible.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:goodtogreat&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:occupied&#34;&gt;I would include any leased space as occupied. We should encourage full utilization of the buildings, whether that includes charter schools, central office use, city government, or private companies.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:occupied&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:technical&#34;&gt;This definition is clunky, but its how the ACS and Census track these things. This definition is clunky, but its how the ACS and Census track these things. We could verify the data using the data reported by districts about language spoken in the home. I would recommend using this data point to assist with whether or not to include these weights for charter schools. For example, approximately half of those families that do not speak English in the home also speak English very poorly. Therefore, I might apply half of the weight to each individual child whose family reports speaking a language other than English at home. Of course, the actual proportion of the weight should be specific to the ratio of speakers of language other than English to non-very well speakers of English by community.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:technical&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What can be done for Rhode Island Pensioners?</title>
      <link>http://www.json.blog/2013/08/what-can-be-done-for-rhode-island-pensioners/</link>
      <pubDate>Thu, 15 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/08/what-can-be-done-for-rhode-island-pensioners/</guid>
      <description>&lt;p&gt;&lt;em&gt;This post originally appeared on my old blog on January 2, 2013 but did not make the transition to this site due to error. I decided to repost it with a new date after recovering it from a cached version on the web.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rhode Island &lt;a href=&#34;http://blogs.wpri.com/2011/11/17/analysis-why-rhode-island-passed-pension-reform-in-2011/&#34;&gt;passed sweeping pension reform last fall&lt;/a&gt;, &lt;a href=&#34;http://blogs.wpri.com/2011/11/21/union-email-blasts-dems-on-pension-law-previews-legal-fight/&#34;&gt;angering&lt;/a&gt; the &lt;a href=&#34;http://blogs.wpri.com/2012/02/07/unions-to-ri-negotiate-a-pension-deal-before-you-lose-in-court/&#34;&gt;major labor unions&lt;/a&gt; and &lt;a href=&#34;http://www.rifuture.org/tag/pension&#34;&gt;progressives&lt;/a&gt; throughout the state. These reforms have &lt;a href=&#34;http://blogs.wpri.com/2011/10/24/moodys-raimondo-chafee-pension-bill-good-for-rhode-island/&#34;&gt;significantly decreased both the short and long-run costs to the state&lt;/a&gt;, while &lt;a href=&#34;http://blogs.wpri.com/2011/11/17/ri-lawmakers-ok-historic-pension-overhaul-by-wide-margins/&#34;&gt;decreasing the benefits of both current and future retirees&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the &lt;a href=&#34;http://blogs.wpri.com/2011/09/16/raimondo-chafee-set-to-freeze-colas-put-all-in-hybrid-plan/&#34;&gt;most controversial measures&lt;/a&gt; in the pension reform package was suspending annual raises &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:raises&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:raises&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; for current retirees. I have noticed two main critiques of this element. The first criticism was that ending this practice constitutes a decrease in benefits to existing retirees who did not consent to these changes, constituting a breach of contract and assault on property rights. This critique is outside of the scope of this post. What I would like to address is the second criticism, that annual raises are critical to retirement security due to inflation, especially for the most vulnerable pensioners who earn near-poverty level wages from their pensions.&lt;/p&gt;

&lt;p&gt;While I am broadly supportive of the changes made to the pension system in Rhode Island, I also believe that it is important to recognize the differential impact suspending annual raises has on a retired statehouse janitor who currently earns \$22,000 a year from their pension and a former state department director earning \$70,000 a year from their pension. Protecting the income of those most vulnerable to inflation is a worthy goal &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:worthy&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:worthy&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;I have a simple recommendation that I think can have a substantial, meaningful impact on the most vulnerable retirees at substantially less cost than annual raises. This recommendation will be attractive to liberals and conservatives, as well as the “business elite” that have long called for increasing Rhode Island&amp;rsquo;s competitiveness with neighboring states. It is time that Rhode Island leaves the company of just three other states– Minnesota, Nebraska, and Vermont– that have no tax exemptions for retirement income &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:exemptions&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:exemptions&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Rhode Island should exempt all income from pensions and social security up to 200% of the federal poverty level from state income taxes. This would go a long way to ensuring retirement security for those who are the most in need. It would also bring greater parity between our tax code and popular retirement destination states, potentially decreasing the impulse to move to New Hampshire, North Carolina, and Florida.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a progressive win. It&amp;rsquo;s a decrease in taxes that conservatives should like. It shouldn&amp;rsquo;t have a serious impact on revenues, especially if it goes a long way toward quelling the union and progressive rancor about the recent reforms. And it&amp;rsquo;s far from unprecedented– in fact, some form of retirement income tax exemption exists in virtually every other state.&lt;/p&gt;

&lt;p&gt;We should not be proud of taking away our most vulnerable pensioners&amp;rsquo; annual raises, even if it was necessary. Instead of ignoring the clear impact of this provision, my hope for 2013 is that we address it, while keeping an overall pretty good change to Rhode Island&amp;rsquo;s state retirement system.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:raises&#34;&gt;&lt;a href=&#34;http://blog.jasonpbecker.com/blog/2012/01/25/providence-pensions-lets-call-a-spade-a-spade-or-the-cola-a-raise/&#34;&gt;Not a cost-of-living adjustment&lt;/a&gt;, or COLA, as some call them.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:raises&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:worthy&#34;&gt;Interesting, increases in food prices has largely slowed and the main driver of inflation are healthcare costs. I wonder to what extent Medicare/Medicaid and Obamacare shield retirees from rising healthcare costs
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:worthy&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:exemptions&#34;&gt;&lt;a href=&#34;http://www.ncsl.org/documents/fiscal/TaxonPensions2011.pdf&#34;&gt;http://www.ncsl.org/documents/fiscal/TaxonPensions2011.pdf&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:exemptions&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts on Teach for America</title>
      <link>http://www.json.blog/2013/07/thoughts-on-tfa/</link>
      <pubDate>Mon, 29 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/07/thoughts-on-tfa/</guid>
      <description>&lt;p&gt;One of the most interesting discussions I had in class during graduate school was about how to interpret the body of evidence that existed about Teach for America. At the time, Kane, Rockoff and Staiger (KRS) had just published &amp;ldquo;&lt;a href=&#34;http://www0.gsb.columbia.edu/faculty/jrockoff/certification-final.pdf&#34;&gt;What does certification tell us about teacher effectiveness? Evidence from New York City&lt;/a&gt;&amp;rdquo; in Economics of Education Review . KRS produced value-added estimates for teachers and analyzed whether their initial certification described any variance in teacher effectiveness at raising student achievement scores. The results were, at least to me, astonishing. All else being equal, there was little difference if teachers were uncertified, traditionally certified, a NYC teaching fellow, or a TFA core member.&lt;/p&gt;

&lt;p&gt;Most people viewed these results as a positive finding for TFA. With minimal training, TFA teachers were able to compete with teachers hired by other means. Is this not a vindication that the selection process minimally ensures an equal quality workforce?&lt;/p&gt;

&lt;p&gt;I will not be discussing the finer points of&lt;/p&gt;

&lt;p&gt;[points out: &lt;a href=&#34;http://scholasticadministrator.typepad.com/thisweekineducation/2013/07/bruno-whats-the-point-of-teach-for-america.html&#34;&gt;http://scholasticadministrator.typepad.com/thisweekineducation/2013/07/bruno-whats-the-point-of-teach-for-america.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smarter Balance Released Items Scare Me</title>
      <link>http://www.json.blog/2013/07/smarter-balance-released-items-scare-me/</link>
      <pubDate>Tue, 23 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/07/smarter-balance-released-items-scare-me/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://ccssimath.blogspot.com/2013/06/our-sbac-practice-tests-run-through.html&#34;&gt;CCSSI Mathematics&lt;/a&gt; posted a scathing look at the items released by the &lt;a href=&#34;http://www.smarterbalanced.org/&#34;&gt;Smarter Balanced Assessment Consortium&lt;/a&gt; (SBAC). While the rest of the internet seems to be obsessed over &lt;a href=&#34;http://blogs.edweek.org/edweek/curriculum/2013/07/georgia_drops_out_of_parcc_tes.html&#34;&gt;Georgia leaving the Partnership for Assessment of College and Careers&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:absurdity&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:absurdity&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the real concern should be over the quality of these test items.&lt;/p&gt;

&lt;p&gt;Although CCSSI also deligently point out questions that are not well aligned to the standards, this is the least of my worries. Adjusting the difficulty of items and better alignment is something that testing companies know how to do and deal with all the time. Computerized testing is the new ground and a big part of why states are, rightfully, excited about the consortium.&lt;/p&gt;

&lt;p&gt;The problem with the SBAC items is they represent the worst of computerized assessment. Rather than demonstrating more authentic and complex tasks, they present convoluted scenarios and even more convoluted input methods. Rather than present multimedia in a way that is authentic to the tasks, we see heavy language describing how to input what amounts to multiple choice or fill-in the blank answers. What I see here is not worth the investment in time and equipment that states are being asked to make, and it is hardly a &amp;ldquo;next generation&amp;rdquo; set of items that will allow us to attain more accurate measures of achievement.&lt;/p&gt;

&lt;p&gt;SBAC looks poised to set up students to fail because of the mechanations of test taking. This is not only tragic at face value, but assures an increase in test-prep as the items are less authentic.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:absurdity&#34;&gt;There was a lot of concern trolling over Georgia leaving PARCC by &lt;a href=&#34;http://www.edexcellence.net/commentary/education-gadfly-daily/flypaper/2013/thats-how-the-consortia-crumble.html&#34;&gt;Andy Smarick on Twitter and Flypaper&lt;/a&gt;. I don&amp;rsquo;t really see this as devastating, nor do I think some kind of supplication to the Tea Party could have changed this. Short of federal mandating of common tests and standards, Georgia was never going to stay aligned with a consortium that includes Massachusetts. Georgia has an incredibly inexpensive testing program, because they have built really poor assessments that are almost entirely multiple choice. They also have some of the &lt;a href=&#34;http://educationnext.org/despite-common-core-states-still-lack-common-standards/&#34;&gt;lowest proficiency standards in the country&lt;/a&gt;. There was no way this state would move up to a testing regime that costs more than twice as much (but is around the country median) that is substantially more complex and will have a much higher standard for proficiency. Georgia is one of those states that clearly demonstrates some of the &amp;ldquo;soft bigotry of low expectations&amp;rdquo; by hiding behind inflated proficiency due to low standards.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:absurdity&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Summer Reading</title>
      <link>http://www.json.blog/2013/07/summer-reading/</link>
      <pubDate>Tue, 23 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/07/summer-reading/</guid>
      <description>

&lt;p&gt;This summer has been very productive for my fiction reading backlog. Here are just some of the things I have read since Memorial Day. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:affiliatescum&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:affiliatescum&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;novels&#34;&gt;Novels&lt;/h2&gt;

&lt;h3 id=&#34;the-name-of-the-wind-by-patrick-rothfuss&#34;&gt;The Name of the Wind by Patrick Rothfuss&lt;/h3&gt;

&lt;p&gt;I picked up &lt;a href=&#34;http://www.amazon.com/dp/0756405890/?tag=jasonpbeckerc-20&#34;&gt;The Name of the Wind&lt;/a&gt; on a whim while cruising through the bookstore. I was glad I did. This book tells a classic story&amp;ndash; a precocious young wizard learns to use his powers, building toward being the most important person in the world. The book is framed around an innkeeper and his apprentice who are more than they seem. When a man claiming to be the most famous storyteller in the land enters the inn, we learn that our innkeeper has past filled with spectacular exploits that our bard wants to record. Lucky for our reader, Kvothe, in addition to be a warrior-wizard of extraordinary talent, is a narcissist who decides to tell his whole story just this once to this most famous of all chroniclers. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:narcissist&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:narcissist&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Although I have spoken to several folks who found Kvothe to be utterly unlikeable because of both his sly form of arrogance and Rothfuss&amp;rsquo;s decision to seemingly make Kvothe worthy of such high self-worth, I loved this book.&lt;/p&gt;

&lt;p&gt;In this first book of the &lt;em&gt;The Kingkiller Chronicle&lt;/em&gt; (as these things tend to be named), we learn all about Kvothe&amp;rsquo;s formative years. We spend substantial time exploring dark times in Kvothe&amp;rsquo;s life when he endures tragedy, trauma, and horrible poverty before finally beginning to learn how to truly use his talents. It is a fair critique that Kvothe seems almost &amp;ldquo;too good&amp;rdquo;, but much of the story is about how skill, luck, and folly all contribute to his success and fame, much of which is based on exaggerated tellings of true events.&lt;/p&gt;

&lt;p&gt;If you are a fan of this sort of fantasy, with magic, destiny, love, power, and coming of age, I recommend picking up this book. Rothfuss has a gift. The sequel, &lt;a href=&#34;http://www.amazon.com/dp/0756407915/?tag=jasonpbeckerc-20&#34;&gt;The Wise Man&amp;rsquo;s Fear&lt;/a&gt; is already available, and I will certainly be reading it before the end of the summer.&lt;/p&gt;

&lt;h3 id=&#34;endymion-and-the-rise-of-endymion-by-dan-simmons&#34;&gt;Endymion and The Rise of Endymion by Dan Simmons&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.amazon.com/dp/0553572946/?tag=jasonpbeckerc-20&#34;&gt;Endymion&lt;/a&gt; and &lt;a href=&#34;http://www.amazon.com/dp/0553572989/?tag=jasonpbeckerc-20&#34;&gt;The Rise of Endymion&lt;/a&gt; are the must anticipated (15 years ago) follow up to Dan Simmons&amp;rsquo;s brilliant &lt;a href=&#34;http://www.amazon.com/dp/0553283685/?tag=jasonpbeckerc-20&#34;&gt;Hyperion&lt;/a&gt; and &lt;a href=&#34;http://www.amazon.com/dp/0553288202/?tag=jasonpbeckerc-20&#34;&gt;Fall of Hyperion&lt;/a&gt;. I strongly recommend the originals, which is one of the greatest tales in all of science fiction. I also recommend creating some distance between reading each set of books. Six years separate the publishing of these duologies. Each story is so rich, I think it is hard to appreciate if you read all four books in one go. Yet, the narrative is so compelling it might be hard to resist. I waited about one year between reading the original &lt;em&gt;Cantos&lt;/em&gt; and this follow up and I was glad I did.&lt;/p&gt;

&lt;p&gt;Set 272 years after the events of the original books, &lt;em&gt;Endymion&lt;/em&gt; and &lt;em&gt;The Rise of Endymion&lt;/em&gt; serve as crucial stories that satisfyingly close loops I did not even realize were open at the end of the originals. What was once a glimpse at future worlds and great cosmic powers now unfurl as major players, their primary motivations unveiled.&lt;/p&gt;

&lt;p&gt;These books are so entwined with the original that I will not say anything about its plot so that there are no spoilers. What I can offer is the following. Whereas books 1 and 2 play with story structure to captivating effect, these books do not. Instead, we are treated to a uniquely omniscient narrator, who is both truly omniscient and integral to the events of the story. How he gains this omniscience is a major plot point that&amp;rsquo;s pulled off effortlessly. The first two books are framed as an epic poem, known as the &lt;em&gt;Hyperion Cantos&lt;/em&gt;, written by one of the major characters in those events. Another thing we learn is the original &lt;em&gt;Cantos&lt;/em&gt; is not entirely reliable. Their author, who as not omniscient, had to fill in some blanks to complete the story, and also failed to understand some of the &amp;ldquo;heady&amp;rdquo; aspects of what happened and was sloppy in their explanations. Thus, we are treated both to key future events and simultaneously charged with a new reading of the original novels as written by a less than reliable narrator. What is true and what is not will all be told in this excellent follow up.&lt;/p&gt;

&lt;p&gt;A word to the wise&amp;ndash; Simmons may feel a bit &amp;ldquo;mushy&amp;rdquo; in his message for some &amp;ldquo;hard&amp;rdquo; science fiction readers. I think there is both profound depth and beautiful presentation of ideas, both complex enough to &amp;ldquo;earn&amp;rdquo; this treatment and some simpler than the story seems to warrant.&lt;/p&gt;

&lt;h3 id=&#34;the-rook&#34;&gt;The Rook&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.amazon.com/The-Rook-Novel-Daniel-OMalley/dp/0316098809//?tag=jasonpbeckerc-20&#34;&gt;The Rook&lt;/a&gt; is a fantastically fun debut novel&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:debuts&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:debuts&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; written by an Australian bureaucrat. I learned about this book from one of my favorite podcasts, &lt;a href=&#34;http://www.5by5.tv/incomparable&#34;&gt;The Incomparable&lt;/a&gt;. &lt;a href=&#34;http://5by5.tv/incomparable/128&#34;&gt;Episode 128: Bureaucracy was Her Superpower&lt;/a&gt; is an excellent discussion that you should listen to after reading this book. I feel the hosts of that show captured perfectly what made this book great&amp;ndash; it was completely honest and fair to its reader.&lt;/p&gt;

&lt;p&gt;It is not giving anything away to say that The Rook centers around a Myfanwy (mispronounced even by the main character as &lt;em&gt;Miffany&lt;/em&gt;, like &lt;em&gt;Tiffany&lt;/em&gt; with an M) who suddenly becomes aware of her troublesome surroundings but with complete amnesia. It would be easy to dismiss the memory loss as a trite plot driver, used as a cheap way to trick our characters and readers. But O&amp;rsquo;Malley is brilliant in his use of Myfanwy&amp;rsquo;s memory loss. This book does not lie to its reader or its characters. Memory loss does not conceal some simple literary irony. Instead, it serves to create a fascinating experience for a reader who learns to understand and love a character as she creates, understands, and learns to love herself.&lt;/p&gt;

&lt;p&gt;Myfanwy is not just an ordinary young woman with memory loss. She&amp;rsquo;s a high ranking official in what can best be described as the British X-Men who run MI-5. And she knew her memory loss was imminent. As such, she prepared letters for her future, new self to learn all about her life and her attempts to uncover the plot that would lead to her own memory loss. Again, the letters could be seen as cheap opportunities for exposition and to create false tension, but O&amp;rsquo;Malley never holds too tight to their use as a structure. We read more letters at the beginning of the story, and fewer later on as the reader is availed of facts and back story as they become relevant, without a poorly orchestrated attempt to withhold information from the main character. Instead of assuming Myfanwy is reading along with us, we easily slip into an understanding that shortly after our story begins, Myfanwy actually takes the time to read all the letters and we, thankfully, are not dragged along for the ride blow by blow.&lt;/p&gt;

&lt;p&gt;The Rook manages to tread space in both story and structure that should feel wholly unoriginal and formulaic but never becoming either. The powers of the various individuals are fascinating, original, and consequential. The structure of the book is additive, but the plot itself is not dependent on its machinations.&lt;/p&gt;

&lt;p&gt;Most of all, The Rook is completely fun and totally satisfying. That&amp;rsquo;s not something we say often in a post-Sopranos, post-Batman Begins world.&lt;/p&gt;

&lt;h3 id=&#34;the-ocean-at-the-end-of-the-lane&#34;&gt;The Ocean at the End of the Lane&lt;/h3&gt;

&lt;p&gt;Speaking of delightful, Neil Gaiman is at his best with &lt;a href=&#34;http://www.amazon.com/dp/0062255657/?tag=jasonpbeckerc-20&#34;&gt;The Ocean at the End of the Lane&lt;/a&gt;. Gaiman is the master of childhood, which is where I think he draws his power as a fantasy writer. He is able to so capture the imagination of a child in beautiful prose it is as thought I am transformed into an 8-year old boy reading by flashlight in bed late at night, anxious and frightened.&lt;/p&gt;

&lt;p&gt;The Ocean at the End of the Lane is a beautiful, dark fairy tale. Our narrator has recently experienced a loss in the family that has affected him profoundly, such that he is driven to detour back to the home he grew up in. Most of us can appreciate how deep sadness can drive us toward spending some time alone in a nostalgic place, both mentally and physically, as we work through our feelings.&lt;/p&gt;

&lt;p&gt;There, we are greeted with the resurfacing of memories from childhood when events most unnatural conspired to do harm against him and his family.&lt;/p&gt;

&lt;p&gt;I really don&amp;rsquo;t want to say much from this book except that it is heartbreakingly beautiful in a way that only someone like Gaiman can manage. This is a book that should be read in just one or two sittings. It is profoundly satisfying for anyone who loves to read books that transform who and where they are. Gaiman achieves this completely.&lt;/p&gt;

&lt;h2 id=&#34;comics&#34;&gt;Comics&lt;/h2&gt;

&lt;h3 id=&#34;locke-and-key&#34;&gt;Locke and Key&lt;/h3&gt;

&lt;p&gt;Joe Hill is a master of his craft. Over Memorial Day weekend there was a great Comixology sale that dramatically reduced the price of getting in on &lt;a href=&#34;http://www.amazon.com/dp/1600102379/?tag=jasonpbeckerc-20&#34;&gt;Locke and Key&lt;/a&gt; and I jumped right on board.&lt;/p&gt;

&lt;p&gt;I have rarely cared so much for a set of characters, regardless of the medium.&lt;/p&gt;

&lt;p&gt;Our main characters, the Locke family (three young children and their mother), are faced with tragedy in the very first panels of &lt;em&gt;Welcome to Lovecraft&lt;/em&gt;, the opening volume of this six-part series. I think what makes Locke and Key unique is rather than use tragedy simply as the opportunity to produce heroism, our protagonists are faced with real, long lasting, deep, and horrifying consequences.&lt;/p&gt;

&lt;p&gt;All the while, we are thrust into the fascinating world of Key House, the Locke family home where our main characters&amp;rsquo; father grew up. Key House is home to magical keys each of which can open one locked door. Step through that door, and there are fantastical consequences like dying, becoming a spirit free to float around the house until your spirit returns through the door. One door might bring great strength, another flight.&lt;/p&gt;

&lt;p&gt;It is not surprising that the tragedy that drives the Locke family back to Key House is deeply connected to the mysterious home&amp;rsquo;s history, and the very source of its magic. What is brilliant is how Joe Hill quietly reveals the greater plot through the every day misadventures of children who are dealing with a massive life change. These characters are rich, their world is fully realized, and the story is quite compelling. A must read.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t believe me? The Incomparable strikes again with a &lt;a href=&#34;http://5by5.tv/incomparable/126&#34;&gt;great episode&lt;/a&gt; on the first volume of Locke and Key.&lt;/p&gt;

&lt;h3 id=&#34;american-vampire&#34;&gt;American Vampire&lt;/h3&gt;

&lt;p&gt;I was turned on to &lt;a href=&#34;http://www.amazon.com/dp/1401229743/?tag=jasonpbeckerc-20&#34;&gt;American Vampire&lt;/a&gt; by &lt;a href=&#34;http://www.dansent.me&#34;&gt;Dan Benjamin&lt;/a&gt;. Wow. Phenomenal. These are real vampires.&lt;/p&gt;

&lt;h3 id=&#34;east-of-west&#34;&gt;East of West&lt;/h3&gt;

&lt;h3 id=&#34;saga&#34;&gt;Saga&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:affiliatescum&#34;&gt;Affiliate links throughout, if that kind of thing bugs you. If that kind of thing does bug you, could you shoot me an email and explain why? I admit to not getting all the &lt;em&gt;rage&lt;/em&gt; around affiliate linking.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:affiliatescum&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:narcissist&#34;&gt;Learning more about our main character, I am somewhat dubious that this is the only time he has told of his exploits, although this older Kvothe may have become a lot less inclined to boasting.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:narcissist&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:debuts&#34;&gt;Actually, The Rook is the second debut on this list. The Name of the Wind was Rothfuss&amp;rsquo;s first.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:debuts&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Economic Policy Institute is Wrong</title>
      <link>http://www.json.blog/2013/06/economic-policy-institute-is-wrong/</link>
      <pubDate>Thu, 20 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/06/economic-policy-institute-is-wrong/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;http://www.epi.org/&#34;&gt;Economic Policy Institute&lt;/a&gt; has release a &lt;a href=&#34;http://www.epi.org/files/2013/ib366-rhode-islands-hybrid-pension-plan.pdf&#34;&gt;short issue brief&lt;/a&gt; on the Rhode Island Retirement Security Act (RIRSA) by &lt;a href=&#34;https://twitter.com/rhiltnsmth&#34;&gt;Robert Hiltonsmith&lt;/a&gt; that manages to get all of the details right but the big picture entirely wrong.&lt;/p&gt;

&lt;p&gt;The EPI Issue Brief details the differences between the retirement system for state workers before and after the passage of RIRSA as accurately and clearly as I have ever seen. Mr. Hiltonsmith has done a notable job explaining the differences between the new system and the old system.&lt;/p&gt;

&lt;p&gt;The brief, unfortunately, fails by engaging in two common fallacies to support its broader conclusions. The first is the &lt;a href=&#34;http://en.wikipedia.org/wiki/Straw_man&#34;&gt;straw man fallacy&lt;/a&gt;. Mr. Hiltonsmith takes a limited set of the objectives of the entire RIRSA legislation and says defined contribution plans do not meet those objectives. That is true, but ignores the other objectives it does accomplish which were also part of the motivation behind RIRSA. The second is &lt;a href=&#34;http://en.wikipedia.org/wiki/Circular_reasoning&#34;&gt;circular reasoning&lt;/a&gt;. In this case, Mr. Hiltonsmith states that the reason for a low funding ratio is because the state did not put 100% of its paper liability into the pension fund. This is a tautology and not in dispute and should not be trumpeted as a conclusion of analysis.&lt;/p&gt;

&lt;p&gt;Here are his three main points that he believes makes RIRSA a bad policy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The defined contribution plan does not save the state money from its annual pension contributions.&lt;/li&gt;
&lt;li&gt;The defined contribution plan is likely to earn lower returns and therefore result in lower benefits for retirees.&lt;/li&gt;
&lt;li&gt;The defined contribution plan does not solve the low funding ratio of the pension plan which exists because law makers did not make required contributions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of course, the defined contribution portion of RIRSA was not in place to do any of these three things. The purpose of including a defined contribution plan in the new state pension system is to create stability in annual budget allocations and avoid locking the government into promises it has demonstrated it fails to keep. Defined benefit plans require the state to change pension contributions when there are market fluctuations and leads to anti-cyclical costs, where the state is forced to put substantially more resources into pensions when revenues are lowest and spending on social welfare is most important. The defined contribution plan keeps the payments required by the state consistent and highly predictable. This is far preferable from a budget perspective.&lt;/p&gt;

&lt;p&gt;It is unfortunate that there are lower returns to defined contribution plans which may lead to a decrease in overall benefits. It is my opinion that the unions in Rhode Island should be pushing for a substantially better match on the defined contribution portion of their plan that more closely resembles private sector match rates. This could more than alleviate the difference in benefits while maintaining the predictability, for budgeting purposes, of the defined contribution plan. I doubt this policy would have much hope of passing while Rhode Island slowly crawls out of a deep recession, but it is certainly a reasonable matter for future legislatures.&lt;/p&gt;

&lt;p&gt;There are only two ways to decrease the current pension fund shortfalls: increase payments to the fund or decrease benefits. There is no structural magic sauce to get around this. Structural changes in the pension system are aimed at reducing the likelihood that the state will reproduce its current situation, with liabilities well outstripping funds. It is true that the &amp;ldquo;savings&amp;rdquo; largely came from cutting benefits. I have not heard anyone claim otherwise. The only alternative was to put a big lump sum into the pension fund. That clearly was not a part of RIRSA.&lt;/p&gt;

&lt;p&gt;It is absurd to judge RIRSA on the ability of defined contribution plans to achieve policy objectives that are unrelated to the purpose of this structural change.&lt;/p&gt;

&lt;p&gt;Perhaps the most troubling conclusion of this brief was that,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The shortfall in Rhode Island&amp;rsquo;s pension plan for public employees is largely due not to overly generous benefits, but to the failure of state and local government employers to pay their required share of pensions&amp;rsquo; cost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I read that and expected to see evidence of skipped payments or a discussion of overly ambitious expectations for investment returns, etc. Instead, it seems that this conclusion is based simply on the fact that the benefits in Rhode Island were not deemed outrageously large, and therefore Rhode Island should just pay the liability hole. The &amp;ldquo;failure&amp;rdquo; here is predicated entirely on the idea that the pensions as offered should be met, period, whatever the cost to the government. This is the &amp;ldquo;required share&amp;rdquo;. Which, of course, is technically true without a change in the law, but feels disingenuous. It is essentially a wholesale agreement with the union interpretation of the state pension system as an immutable contract. The courts will likely resolve whether or not this is true. My objection is that Mr. Hiltonsmith makes a definitive statement on this rationale without describing it. In such a lucid description of how the retirement system has changed, it seems this could only be intentional omission intended to support a predetermined conclusion rather than illuminate the unconvinced.&lt;/p&gt;

&lt;p&gt;Mr. Hiltonsmith also claims that, &amp;ldquo;Over the long term, RIRSA may cost the state upwards of \$15 million a year in additional contributions while providing a smaller benefit for the average full-career worker.&amp;rdquo; I am not 100% certain, but based on his use of the normal cost &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:normal-cost&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:normal-cost&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; to do these calculations, it appears this conclusion is drawn only based on the marginal contributions to current employees. In other words, if we &lt;em&gt;completely ignore&lt;/em&gt; the existing liability, the new plan cost the state more money marginally while potentially decreasing benefits for employees. It is my opinion that Mr. Hiltonsmith is intentionally creating the perception that RIRSA costs more than the current plan while providing fewer benefits. Again, this is true for future liabilities, but ignores that RIRSA also dramatically decreased the unfunded liabilities through cutting existing retiree benefits. So the overall cost for the act is far less, while the marginal cost was increased with the objective of decreasing the instability in government appropriations.&lt;/p&gt;

&lt;p&gt;We can have a serious debate about whether there is value in the state goals of a defined contribution plan. In my view, the purpose of switching to this structure is about:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Portability of plans for more mobile workers, potentially serving to attract younger and more highly skilled employees.&lt;/li&gt;
&lt;li&gt;Stability in government expenditures on retiree benefits from year to year that are less susceptible to market forces. This includes avoiding the temptation to reduce payments when there are strong market returns as well as the crushing difficulty of increasing payments when the market (and almost certainly government receipts) are down.&lt;/li&gt;
&lt;li&gt;Insulating workers from a government that perpetually writes checks they can cash, as was the case with the current system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This paper does not address any of these objectives or others I might have forgotten. In essence, the brief looks at only one subset of the perceived costs of this structural change, but it is far from a comprehensive analysis of the potential universe of both costs and benefits. In fact, it fails to even address the most commonly cited benefits. That is why I view it as heavily biased and flawed, even if I might draw similar conclusions from a more thorough analysis.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:normal-cost&#34;&gt;Definition: Active participants earn new benefits each year. Actuaries call that the normal cost. The normal cost is always reflected in the cash and accounting cost of the plan. &lt;a href=&#34;http://www.actuary.org/pdf/pension/fundamentals_0704.pdf&#34;&gt;Source&lt;/a&gt; In other words, the normal cost only looks at the new benefits added to the liability, not the existing liability.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:normal-cost&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Calculating Age in R</title>
      <link>http://www.json.blog/2013/06/calculating-age-in-r/</link>
      <pubDate>Wed, 12 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/06/calculating-age-in-r/</guid>
      <description>&lt;p&gt;A few months back I wrote some code to calculate age from a date of birth and arbitrary end date. It is not a real tricky task, but it is certainly one that comes up often when doing research on individual-level data.&lt;/p&gt;

&lt;p&gt;I was a bit surprised to only find bits and pieces of code and advice on how to best go about this task. After reading through some old R-help and Stack Overflow responses on various ways to do date math in R, this is the function I wrote &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:wrote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:wrote&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;age_calc &amp;lt;- function(dob, enddate=Sys.Date(), units=&#39;months&#39;){
  if (!inherits(dob, &amp;quot;Date&amp;quot;) | !inherits(enddate, &amp;quot;Date&amp;quot;))
    stop(&amp;quot;Both dob and enddate must be Date class objects&amp;quot;)
  start &amp;lt;- as.POSIXlt(dob)
  end &amp;lt;- as.POSIXlt(enddate)
  
  years &amp;lt;- end$year - start$year
  if(units==&#39;years&#39;){
    result &amp;lt;- ifelse((end$mon &amp;lt; start$mon) | 
                      ((end$mon == start$mon) &amp;amp; (end$mday &amp;lt; start$mday)),
                      years - 1, years)    
  }else if(units==&#39;months&#39;){
    months &amp;lt;- (years-1) * 12
    result &amp;lt;- months + start$mon
  }else if(units==&#39;days&#39;){
    result &amp;lt;- difftime(end, start, units=&#39;days&#39;)
  }else{
    stop(&amp;quot;Unrecognized units. Please choose years, months, or days.&amp;quot;)
  }
  return(result)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few notes on proper usage and the choices I made in writing this function:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The parameters &lt;code&gt;dob&lt;/code&gt; and &lt;code&gt;enddate&lt;/code&gt; expect data that is already in one of the various classes that minimally inherits the base class &lt;code&gt;Date&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;This function takes advantage of the way that R treats vectors, so both &lt;code&gt;dob&lt;/code&gt; and &lt;code&gt;enddate&lt;/code&gt; can be a single or multi-element vector. For example &lt;code&gt;enddate&lt;/code&gt; is a single date, as is the default, then the function will return a vector that calculates the difference between &lt;code&gt;dob&lt;/code&gt; and that single date for each element in &lt;code&gt;dob&lt;/code&gt;. If &lt;code&gt;dob&lt;/code&gt; and &lt;code&gt;enddate&lt;/code&gt; are both vectors with n&amp;gt;1, then the returned vector will contain the &lt;a href=&#34;http://heather.cs.ucdavis.edu/~matloff/r.old.html#elementwise&#34;&gt;element-wise&lt;/a&gt; difference between &lt;code&gt;dob&lt;/code&gt; and &lt;code&gt;enddate&lt;/code&gt;. When the vectors are of different sizes, the shorter vector will be repeated over until it reaches the same length as the longer vector. This is known as &lt;a href=&#34;http://cran.r-project.org/doc/manuals/R-intro.html#The-recycling-rule&#34;&gt;recycling&lt;/a&gt;, and it is the default behavior in R.&lt;/li&gt;
&lt;li&gt;This function always returns an integer. Calculating age in years will never return, say, 26.2. Instead, it assumes that the correct behavior for age calculations is something like a &lt;code&gt;floor&lt;/code&gt; function. For examle, the function will only return 27 if &lt;code&gt;enddate&lt;/code&gt; is minimally your 27th birthday. Up until that day you are considered 26. The same is true for age in months.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is probably the first custom function in almost 3 years using R that I wrote to be truly generalizable. I was inspired by three factors. First, this is a truly frequent task that I will have to apply to many data sets in the future that I don&amp;rsquo;t want to have to revisit. Second, a professional acquaintance, &lt;a href=&#34;http://jaredknowles.com/&#34;&gt;Jared Knowles&lt;/a&gt;, is putting together a CRAN package with various convenience functions for folks who are new to R and using it to analyze education data &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:https-github-com-jknowles-eeptools&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:https-github-com-jknowles-eeptools&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This seemed like an appropriate addition to that package, so I wanted to write it to that standard. In fact, it was my first (and to date, only) submitted and accepted pull request on Github. Third, it is a tiny, simple function so it was easy to wrap my head around and write it well. I will let you be the judge of my success or failure &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:inspiration&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:inspiration&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:wrote&#34;&gt;I originally used &lt;code&gt;Sys.time()&lt;/code&gt; not realizing there was a &lt;code&gt;Sys.Date()&lt;/code&gt; function. Thanks to Jared Knowles for that edit in preparation for a CRAN check.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:wrote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:https-github-com-jknowles-eeptools&#34;&gt; &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:https-github-com-jknowles-eeptools&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:inspiration&#34;&gt;Thanks to &lt;a href=&#34;http://mcfromnz.wordpress.com/2013/06/12/updated-age-calculation-function/&#34;&gt;Matt&amp;rsquo;s Stats n Stuff&lt;/a&gt; for getting me to write this post. When I saw another age calculation function pop up on the r-bloggers feed I immediately thought of this function. Matt pointed out that it was quite hard to Google for age calculations in R, lamenting that Google doesn&amp;rsquo;t meaningfully crawl Github where I linked to find my code. So this post is mostly about providing some help to less experience R folks who are frantically Googling as both Matt and I did when faced with this need.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:inspiration&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stubborn Problem with MOOCs</title>
      <link>http://www.json.blog/2013/06/stubborn-problem-with-moocs/</link>
      <pubDate>Wed, 05 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/06/stubborn-problem-with-moocs/</guid>
      <description>

&lt;p&gt;Kevin Carey has a &lt;a href=&#34;http://higheredwatch.newamerica.net/blogposts/2013/moocs_robots_and_the_secret_of_life-85293&#34;&gt;great new piece&lt;/a&gt; on his experience taking a Massive Open Online Course (MOOC). If you have any interest in blended learning, online education, and higher education policy, I would consider this a must read.&lt;/p&gt;

&lt;p&gt;He carefully addresses many of the common concerns to online only course work. Can it be truly rigorous? Yes. Do you lose the &amp;ldquo;magic&amp;rdquo; of taking a big lecture course in person? Maybe, but there&amp;rsquo;s more magic in pressing pause and rewind on a video. Can you assess 40,000 students in a rigorous way? Yes.&lt;/p&gt;

&lt;p&gt;Carey concludes that the cost of attending an institute of higher education, and of paying so many PhD instructors for lecture courses, is astronomically high considering the small value-add that arguably exists between the best online version of a course and what most professors and lecturers offer.&lt;/p&gt;

&lt;p&gt;The implication for Carey is clear: online education done extremely well can be as effective as some university courses today, and most university courses tomorrow.&lt;/p&gt;

&lt;p&gt;I agree that lectures in person are not better than online lectures. I also agree that intellectually stimulating conversation about content/material can happen online in forums, over video chats, using popular social networks, etc. I even agree that it is possible to do rigorous assessment in many domains &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:humanities&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:humanities&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I am quite confident that a well-implemented MOOC could replace the typical college lecture course today on many campuses. The problem with MOOCs is not their ability to replicate the quality material aspects of a college course.&lt;/p&gt;

&lt;h2 id=&#34;what-is-the-problem-with-moocs&#34;&gt;What is the Problem with MOOCs?&lt;/h2&gt;

&lt;p&gt;Carey spent 15 hours a week watching lectures, working through problem sets, and communicating with fellow students to complete the course work with satisfactory outcomes. Think about the amount of perseverance it takes to work that hard independently from a computer in your home. There is an entire world of books and webpages dedicated to helping upperclass, knowledge economy employees work productively from their home offices because while some thrive, many struggle to be productive. Professionals find they have to be careful to close their doors, set clear boundaries with family members around work hours, put on a suit and a tie like they are going into work, and not use the office for non-work activities, to name a few techniques, to ensure they are productive working remotely &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:remote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:remote&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Non-traditional students, first-generation college students, and poor students are all likely to face challenges recreating effective work spaces. This is not a matter of bandwidth, quality computer access, or digital skills. All of these things are real challenges, but will disappear within the next decade. What&amp;rsquo;s not likely to change is the need for quiet, comfortable space to work seriously for hours on end, uninterrupted.&lt;/p&gt;

&lt;p&gt;But these students will also miss out on another key part of what makes college an effective place for learning&amp;ndash; you&amp;rsquo;re in a place that&amp;rsquo;s dedicated to learning surrounded by people dedicated to the same pursuit. When you see people every day who are walking into class &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:toclass&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:toclass&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; there is a sense that you are a part of a learning community. There is a pressure to succeed because you see the hard work of your peers. I truly believe that making going to class and studying a habit is greatly supported by being surrounded by that habit. Look no further than group study areas and coffee shops around universities to see tons of students, outside of their dorms and in pop up communal office. This is true even at Research I universities, even among students who do not share classes. Those students know how to use forums, social networking, instant messages and more.&lt;/p&gt;

&lt;p&gt;I am not saying that college is about unexpected collisions of people or ideas in some nebulous way. I mean quite literally that being a good student is partly possible because you&amp;rsquo;re surrounded by students.&lt;/p&gt;

&lt;p&gt;These supports are not irreplaceable. They do not require $50,000 a year. On this, I completely agree with Carey. But the reality is the students who will easily adapt and find substitute supports, regardless of cost, will not be the ones to use MOOCs at the start.&lt;/p&gt;

&lt;p&gt;Community colleges are the major target with MOOCs. They are already struggling to stay low cost institutions, their faculty are generally less credentialed and have substantially less power than tenured faculty at research institutions. They also are less likely to be able to make the case that their lecture is world class. However, their students are the ones that have the most to lose.&lt;/p&gt;

&lt;p&gt;Community college students are about to lose a critical support: the culture of being students with other students.&lt;/p&gt;

&lt;p&gt;Academic preparation is frequently discussed when trying to predict college success, but I don&amp;rsquo;t think we should dismiss the importance of social integration. Only an extreme classist view could believe that MOOCs remove the need for social integration because the &amp;ldquo;institution&amp;rdquo; of traditional universities and colleges no longer exist. We will simply accomplish shifting the burden of a new, challenging integration to those who are already struggling.&lt;/p&gt;

&lt;h2 id=&#34;a-world-of-free-decentralized-higher-education&#34;&gt;A World of Free, Decentralized Higher Education&lt;/h2&gt;

&lt;p&gt;I am also concerned with a future where MOOCs are broadly available, very inexpensive, and degrees are not considered important. This may seem like an ideal end stage, where skill and knowledge are what is rewarded and the &amp;ldquo;gatekeepers&amp;rdquo; to traditional power have fallen.&lt;/p&gt;

&lt;p&gt;Yet, this highly market-driven future is likely to continue to exacerbate the difference between the haves and have-nots, a decidedly poor outcome. Education markets struggle from information failures. &lt;strong&gt;Need more here&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;proceed-with-caution&#34;&gt;Proceed with Caution&lt;/h2&gt;

&lt;p&gt;I am honestly thrilled about MOOCs. I just feel more cautious about their public policy implications in the immediate term. Let&amp;rsquo;s start with rigorous experiments and lowering the costs at our most elite institutions before we decide to &amp;ldquo;solve&amp;rdquo; remedial course work and the higher education system writ large in one fell swoop.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:humanities&#34;&gt;Certainly in the STEM fields. It may be more challenging to address humanities and social sciences that are heavy on writing.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:humanities&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:remote&#34;&gt;Yes, many people think it&amp;rsquo;s crazy that folks find it challenging to work from home. I promise you, this is a real thing, mostly experience by people who have actually tried working from home. Hence, &lt;a href=&#34;http://en.wikipedia.org/wiki/Coworking&#34;&gt;coworking&lt;/a&gt; and other solutions.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:remote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:toclass&#34;&gt;Not just your class, and not just from dorm rooms but also from cars.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:toclass&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Thinking</title>
      <link>http://www.json.blog/2013/06/linear-thinking/</link>
      <pubDate>Sun, 02 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/06/linear-thinking/</guid>
      <description>&lt;p&gt;Apple will be revealing new details for both of its major operating systems at WWDC on June 10, 2013. The focus of much speculation has been how Apple will improve multi-tasking and inter-app communication in iOS7. As batteries have grown, CPUs have become increasingly powerful, and the application &lt;a href=&#34;|filename|frictionless.md&#34;&gt;ecosystem&lt;/a&gt; has matured &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:viticci&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:viticci&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the iOS sandboxing model has felt increasingly limiting and outdated.&lt;/p&gt;

&lt;p&gt;I think that there is a simple change that could dramatically increase the effectiveness of multitasking on iOS by re-examining how application switching works.&lt;/p&gt;

&lt;p&gt;Scrolling through a long list of applications, either through the basement shelf or via the four-finger gesture on an iPad, is both slow and lacking in contextual cues. In a simple case where I am working with two applications simultaneously, how do I switch between them? The list of open applications always places the current application in the first position. The previously used application sits in the second position. The first time I want to change to another application this is not so bad. I move to the &amp;ldquo;right&amp;rdquo; on the list to progress forward into the next application &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:next&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:next&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The trouble comes when I want to return to where I was just working. The most natural mental model for this switch is to move &amp;ldquo;left&amp;rdquo; in the list. I moved &amp;ldquo;right&amp;rdquo; to get here and millions of years of evolution has taught me the the &amp;ldquo;undo button&amp;rdquo; for moving right is to move left &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:inform7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:inform7&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. But of course, when I attempt to move &amp;ldquo;left&amp;rdquo;, I find no destination &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:youcant&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:youcant&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. I can pop an application from anywhere on the list, but I can only prepend new applications to the list &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:ignorant&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:ignorant&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Apple needs to move away from its linear thinking and enter the second dimension.&lt;/p&gt;

&lt;p&gt;What if I could drag apps in the switcher on top of each other to make a new stack, not unlike the option on the OS X launch bar? Throughout this stack, position is maintained regardless of which application is in use/was last used. I can always move &lt;strong&gt;up&lt;/strong&gt; from Chrome to ByWord, and &lt;strong&gt;down&lt;/strong&gt; to Good Reader, for example, if I was writing a report. Apple might call this a Stack, mirroring the term in OSX, but I would prefer this to be called a &lt;em&gt;Flow&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The goal of this feature is to organize a &lt;em&gt;Flow&lt;/em&gt; for a particular task that requires using multiple apps. One feature might be saving a &amp;ldquo;Flow&amp;rdquo;, this way each time I want to write a blog post, I tap the same home screen button and the same four apps in the same order launch in a &lt;em&gt;Flow&lt;/em&gt;, ready for easy switching using the familiar four-finger swipe gesture up and down. I no longer have to worry about the sequence I have recently accessed applications which is confusing and requires me to look at the app switcher draw or needlessly and repeatedly swipe through applications. I never have to worry about lingering too long on one application while swiping through and switching to that app, changing my position to the origin of the list and starting over again.&lt;/p&gt;

&lt;p&gt;For all the calls for complex inter-app communication or having multiple apps active on the screen at the same time, it seems a simple interface change to application switching could complete change the way we multitask on iOS.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:viticci&#34;&gt;And Federico Viticci has either shown us the light or gone completely mad.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:viticci&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:next&#34;&gt;For now, lets assume the right application is next in the stack. I&amp;rsquo;ll get to that issue with my second change.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:next&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:inform7&#34;&gt;&lt;code&gt;You are in a green room. &amp;gt; Go east. You are in a red room. &amp;gt; Go west. You are in a green room.&lt;/code&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:inform7&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:youcant&#34;&gt;&lt;code&gt;You can&#39;t go that way.&lt;/code&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:youcant&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:ignorant&#34;&gt;I don&amp;rsquo;t know enough about data structures yet to name what&amp;rsquo;s going on here. I am tempted to think that the challenge is they have presented a list to users, with a decidedly horizontal metaphor, when they actually have created something more akin to a stack, with a decidedly vertical metaphor. But a stack isn&amp;rsquo;t quite the right way to understand the app switcher. You can &amp;ldquo;pop&amp;rdquo; an app from any arbitrary position on the app switcher, but funny enough can only push a new app on to the top of the switcher.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:ignorant&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning URLs with TextExpander</title>
      <link>http://www.json.blog/2013/05/cleaning-urls-with-textexpander/</link>
      <pubDate>Thu, 30 May 2013 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2013/05/cleaning-urls-with-textexpander/</guid>
      <description>&lt;p&gt;One thing I really dislike about Google Reader is it replaces the links to posts in my RSS feed. My &lt;a href=&#34;http://pinboard.in/u:jasonpbecker&#34;&gt;Pinboard account&lt;/a&gt; is littered with links that start with &lt;code&gt;http://feedproxy.google.com&lt;/code&gt;. I am quite concerned that with the demise of &lt;a href=&#34;http://googlereader.blogspot.com/2013/03/powering-down-google-reader.html&#34;&gt;Google Reader&lt;/a&gt; on July 1, 2013, these redirects will no longer work.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not just Google that obscures the actually address of links on the internet. The popularity of using link shortening services, both to save characters on Twitter and to collect analytics, has proliferated the &lt;em&gt;Internet of Redirects&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Worse still, after I am done cutting through redirects, I often find that the ultimate link include all kinds of extraneous attributes, most especially a barrage of &lt;code&gt;utm_*&lt;/code&gt; campaign tracking.&lt;/p&gt;

&lt;p&gt;Now, I understand why all of this is happening and the importance of the services and analytics this link cruft provides. I am quite happy to click on shortened links, move through all the redirects, and let sites know just how I found them. But quite often, like when using a bookmarking service or writing a blog post, I just want the simple, plain text URL that gets me directly to the permanent home of the content.&lt;/p&gt;

&lt;p&gt;One part of my workflow to deal with link cruft is a TextExpander snippet I call &lt;code&gt;cleanURL&lt;/code&gt;. It triggers a simple Python script that grabs the URL in my clipboard, traces through the redirects to the final destination, then strips links of campaign tracking attributes, and ultimately pastes a new URL that is much &amp;ldquo;cleaner&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Below I have provided the script. I hope it is useful to some other folks, and I would love some recommendations for additional &amp;ldquo;cleaning&amp;rdquo; that could be performed.&lt;/p&gt;

&lt;p&gt;My next task is expanding this script to work with &lt;a href=&#34;http://pinboard.in&#34;&gt;Pinboard&lt;/a&gt; so that I can clean up all my links before the end of the month when Google Reader goes belly up.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::python
#!/usr/bin/python
import requests
import sys
from re import search
from subprocess import check_output

url = check_output(&#39;pbpaste&#39;)

# Go through the redirects to get the destination URL
r = requests.get(url)

# Look for utm attributes
match =  search(r&#39;[?&amp;amp;#]utm_&#39;, r.url)

# Because I&#39;m not smart and trigger this with
# already clean URLs
if match:
  cleanURL = r.url.split(match.group())[0]
else:
  cleanURL = r.url

print cleanURL
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The Slow Trek to Pelican</title>
      <link>http://www.json.blog/2012/12/the-slow-trek-to-pelican/</link>
      <pubDate>Thu, 20 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/12/the-slow-trek-to-pelican/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Update: Please see below for two solutions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I have grown increasingly unhappy with Wordpress lately. My blog is simple. My design tastes are simple. My needs are simple. I like control. I am a geek. And I really need an excuse to learn Python, which seems to be rapidly growing into one of the most important programming languages for a data analyst.&lt;/p&gt;

&lt;p&gt;I have decided to migrate this blog over to &lt;a href=&#34;http://docs.getpelican.com/en/3.1.1/&#34;&gt;Pelican&lt;/a&gt;, a static site generator written in Python. Static sites are the &amp;ldquo;classic&amp;rdquo; way to do a webpage&amp;ndash; just upload a bunch of HTML and CSS files, maybe some Javascript. But no databases and no constructing the page a user sees in the browser as they request it. This puts substantially less strain on a web server and makes it far easier to export and move a webpage since all you need to do is duplicate files. What makes static sites a real pain is that there is a lot of repetition. Folks adopted dynamic sites that use content management system so that they can write a page called &amp;ldquo;post.php&amp;rdquo; one time, and for each unique post just query a database for the unique content. The frame around the post, layout, components, etc are all just written once. Static site generators allow you to build a webpage using a similar, but far more stripped down, layout system. However, rather than generate each page on the web server, you generate each page by running a script locally that transforms plain text documents into well-formed HTML/CSS. Then you can just upload a directory and the whole site is ready to go.&lt;/p&gt;

&lt;p&gt;Pelican comes with a pretty good script that will take Wordpress XML that&amp;rsquo;s available via the built-in export tools and transform each post into a &lt;a href=&#34;http://docutils.sourceforge.net/rst.html&#34;&gt;reStructuredText&lt;/a&gt; files, a format similar to &lt;a href=&#34;http://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;. I prefer Markdown so I used &lt;a href=&#34;http://johnmacfarlane.net/pandoc/&#34;&gt;pandoc&lt;/a&gt; to convert all my *.rst posts into *.md files.&lt;/p&gt;

&lt;p&gt;So far, so good.&lt;/p&gt;

&lt;p&gt;But one of the really big problems I had with Wordpress was a growing dependency on plugins that added non-standard, text-based markup in my posts that would be rendered a particular way. For example, text surrounded by two parenthesis, &amp;lsquo;[^0]&amp;lsquo;, became a footnote. For code syntax highlighting, I use a &amp;ldquo;short code&amp;rdquo;, which puts &amp;ldquo;sourcecode language=&amp;lsquo;r&amp;rsquo;&amp;rdquo;, for example, between brackets []. All of these plugins have been great, but now when you try to export a post you get the non-standard markup in-line as part of your posts. It makes it very difficult to recreate a post the way it looks today.&lt;/p&gt;

&lt;p&gt;This presents a great opportunity to learn a little Python. So I have begun to scrounge together some basic Python knowledge to write some scripts to clean up my Markdown files and convert the syntax of the short codes that I have used to properly formatted Markdown so that when I run the pelican script it will accurately reproduce each post.&lt;/p&gt;

&lt;p&gt;Unfortunately, I&amp;rsquo;ve hit a snag with my very first attempt. Footnotes are a big deal to me and have standard Markdown interpretation. In Markdown, footnotes are inserted in the text where &amp;ldquo;[\^#]&amp;rdquo; appears in the text, where # = the footnote identifier/key. Then, at the end of the document, surrounded by new lines, the footnote text is found with &amp;ldquo;[\^#]: footnote text&amp;rdquo; where # is the same identifier. So I needed to write a script that found each instance of text surrounded by two parentheses, insert the [\^#] part in place of the footnote, and then add the footnote at the bottom of the post in the right format.&lt;/p&gt;

&lt;p&gt;I created a test text file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::
This is a test ((test footnote)).
And here is another test ((footnote2)). Why not add a third? ((Three
Three)).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal was to end up with a file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::
This is a test [^1]. And here is another
test [^2]. Why not add a third? [^3].

[^1]: test footnote

[^2]: footnote2

[^3]: Three Three
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, the output isn&amp;rsquo;t quite right. My best attempt resulted in
a file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::
This is a test [^1] And here is another te[^2])). Why not add a
t[^3]ree)).

[^1]: ((test footnote))

[^2]: ((footnote2))

[^3]: ((Three Three))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ugh.&lt;/p&gt;

&lt;p&gt;So I am turning to the tiny slice of my readership that might actually know Python or just code in general to help me out. Where did I screw up? The source to my Python script is below so feel free to comment here or on this &lt;a href=&#34;https://gist.github.com/4342554#file-wpfootnotestomarkdown-py&#34;&gt;Gist&lt;/a&gt;. I am particularly frustrate that the regex appears to be capturing the parenthesis, because that&amp;rsquo;s not how the same code behaves on &lt;a href=&#34;http://www.pythonregex.com&#34;&gt;PythonRegex.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If anyone can help me with the next step, which will be creating arguments that will understand an input like *.rst and set the output to creating a file that&amp;rsquo;s *.md, that would be appreciated as well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::python
import re

p = re.compile(&amp;quot;\(\(([^\(\(\)\)]+)\)\)&amp;quot;)
file_path = str(raw_input(&#39;File Name &amp;gt;&#39;))
text = open(file_path).read()

footnoteMatches = p.finditer(text)

coordinates = []
footnotes = []

# Print span of matches
for match in footnoteMatches:
    coordinates.append(match.span())
    footnotes.append(match.group())

for i in range(0,len(coordinates)):
    text = (text[0:coordinates[i][0]] + &#39;[^&#39; + str(i+1)+ &#39;]&#39; +
            text[coordinates[i][1]+1:])
    shift = coordinates[i][1] - coordinates[i][0]
    j = i + 1
    while j &amp;lt; len(coordinates):
        coordinates[j] = (coordinates[j][0] - shift, coordinates[j][1] - shift)
        j += 1

referenceLinkList = [text 1=&amp;quot;&#39;
&#39;&amp;quot; language=&amp;quot;,&amp;quot;][/text]
for i in range(0, len(footnotes)):
    insertList = &#39;&#39;.join([&#39;\n&#39;, &#39;[^&#39;, str(i+1), &#39;]: &#39;, footnotes[i], &#39;\n&#39;])
    referenceLinkList.append(insertList)

text = &#39;&#39;.join(referenceLinkList)

newFile = open(file_path, &#39;w&#39;)
newFile.truncate()
newFile.write(text)
newFile.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;update-with-solutions&#34;&gt;Update with solutions:&lt;/h2&gt;

&lt;p&gt;I am happy to report I now have two working solutions. The first one comes courtesy of &lt;a href=&#34;https://github.com/ilikepi&#34;&gt;James Blanding&lt;/a&gt; who was kind enough to &lt;a href=&#34;https://gist.github.com/4355865&#34;&gt;fork&lt;/a&gt; the gist I put up. While I was hoping to take a look tonight at his fork tonight, &lt;a href=&#34;http://news.ycombinator.com/item?id=4957935&#34;&gt;Github was experiencing some downtime&lt;/a&gt;.  So I ended up fixing the script myself a slightly different way (seen below). I think James&amp;rsquo;s approach is superior for a few reasons, not the least of which was avoiding the ugly if/elif/else found in my code by using a global counter. He also used .format() a lot better than I did, which I didn&amp;rsquo;t know existed until I found it tonight.&lt;/p&gt;

&lt;p&gt;I made two other changes before coming to my solution. First, I realized my regex was completely wrong. I didn&amp;rsquo;t want to capture anything within the two parenthesis when no parenthesis were contained, as the original regex did. Instead, I wanted to make sure to preserve any parenthetical comments contained within my footnotes. So the resulting regex looks a bit different. I also switched from using user input to taking in the filepath as an argument.&lt;/p&gt;

&lt;p&gt;My next step will be to learn a bit more about the os module which seems to contain what I need so that this Python script can behave like a good Unix script and know what to do with one file or a list of files as a parameter (and of course, most importantly, a list generated from a wild card like *.rst). I will also be incorporating the bits of James&amp;rsquo;s code that I feel confident I understand and that I like better.&lt;/p&gt;

&lt;p&gt;Without further ado, my solution (I updated the gist as well):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::python
from sys import argv
import re

name, file_path = argv

p = re.compile(r&amp;quot;[\s]\(\((.*?[)]{0,1})\)\)[\s]{0,1}&amp;quot;)
# The tricky part here is to match all text between &amp;quot;((&amp;quot;&amp;quot;))&amp;quot;, including as 
# many as one set of (), which may even terminate ))). The {0,1} captures as
# many as one ). The trailing space is there because I often surrounded the 
# &amp;quot;((&amp;quot;&amp;quot;))&amp;quot; with a space to make it clear in the WordPress editor.

# file_path = str(raw_input(&#39;File Name &amp;gt;&#39;))
text = open(file_path).read()

footnoteMatches = p.finditer(text)

coordinates = []
footnotes = []

# Print span of matches
for match in footnoteMatches:
    coordinates.append(match.span())
# Capture only group(1) so you get the content of the footnote, not the 
# whole pattern which includes the parenthesis delimiter.
    footnotes.append(match.group(1))

newText = []
for i in range(0, len(coordinates)):
    if i == 0:
        newText.append(&#39;&#39;.join(text[:coordinates[i][0]] +
                               &#39; [^{}]&#39;).format(i + 1))
    elif i &amp;lt; len(coordinates) - 1 :
        newText.append(&#39;&#39;.join(text[coordinates[i-1][1]:coordinates[i][0]] +
                          &#39; [^{}]&#39;).format(i + 1))
    else:
        newText.append(&#39;&#39;.join(text[coordinates[i-1][1]:coordinates[i][0]] +
                          &#39; [^{}]&#39;).format(i + 1))
        # Accounts for text after the last footnote which only runs once.
        newText.append(text[coordinates[i][1]:]+&#39;\n&#39;)

endNotes = []
for j in range(0, len(footnotes)):
    insertList = &#39;&#39;.join([&#39;\n&#39;,&#39;[^{}]: &#39;, footnotes[j], &#39;\n&#39;]).format(j + 1)
    endNotes.append(insertList)

newText = &#39;&#39;.join(newText) + &#39;\n&#39; + &#39;&#39;.join(endNotes)

newFile = open(file_path, &#39;w&#39;)
newFile.truncate()
newFile.write(newText)
newFile.close()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A little knowledge is a wonderful (dangerous) thing</title>
      <link>http://www.json.blog/2012/11/a-little-knowledge-is-a-wonderful-dangerous-thing/</link>
      <pubDate>Tue, 20 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/11/a-little-knowledge-is-a-wonderful-dangerous-thing/</guid>
      <description>&lt;p&gt;It is so tempting to try to apply cognitive science results in education. It seems like an obvious step on the long road of moving education from a field of theory and philosophies to one more grounded in empirical research. Yet, learning myths are persistent. Even scarier, &amp;ldquo;&lt;a href=&#34;http://cedarsdigest.wordpress.com/2012/11/18/myths-come-from-values-not-from-ignorance/&#34;&gt;those who know the most about neuroscience also believe the most myths.&lt;/a&gt;&amp;ldquo;&lt;/p&gt;

&lt;p&gt;Educators may have the best intentions when trying to infuse their practice with evidence, but they all too often are not equipped as critical consumers of research. Worse, the education profession has historically been wrapped in &amp;ldquo;thoughtworld&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, where schools of education have taught same ideas about effective teaching and learning for decades without a basis in empirical research. These same ideas are taught to principals, district administrators, and teachers, so nary a critical voice can stop the myths from being repeated and mutually reinforcing each other.&lt;/p&gt;

&lt;p&gt;Effectively conducting empirical research, translating research for policymakers, and implementing research-based program design is my job. I came to education purely from a research and policy perspective, and I am equipped to understand some of the empirical research done on effective schooling &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;I have to confront an awful history of &amp;ldquo;outsiders&amp;rdquo; like myself who have brought round after round of poorly supported, poorly evaluated reforms. I have to confront the history of districts and schools discarding some very effective programs because of leadership changes, lack of resources, and most of all a lack good, systematic evaluation of programs. And I have to be damn good at what I do, because even a small misstep could paint me just like every other &amp;ldquo;expert&amp;rdquo; that has rolled through with the newest great idea.&lt;/p&gt;

&lt;p&gt;I think this is why I tend to favor interventions that are very small. Simple, small, hard to &amp;ldquo;mess up&amp;rdquo; interventions,  based in research, implemented just a few at a time have tremendous potential. I love the oft-cited work on &lt;a href=&#34;http://www.nber.org/papers/w15361&#34;&gt;filling out the FAFSA along with tax filing at H&amp;amp;R Block&lt;/a&gt;. It is simple. There is no fear of &amp;ldquo;dosage&amp;rdquo; or implementation fidelity. There are both sound theoretical reasons and empirical results from other domains that suggest a high likelihood of success. It has the potential to make a huge impact on students without adding any load to teachers who are, say, implementing a brand new and complicated curriculum this year. This is how you earn trust through building
success.&lt;/p&gt;

&lt;p&gt;I am also a fan of some really big, dramatic changes, but how I get there will have to be the subject of a future post.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;E.D. Hirsch&amp;rsquo;s term
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;In the area of neuroscience and cognitive science, I am probably only marginally better off than most teachers. My Sc.B. is in chemistry. So a background in empirical physical sciences and my knowledge of social science may help me to access some of the research on how people learn, but I would probably be just as susceptible to overconfidence in my ability to process this research and repeat untruths as many very intelligent educators.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Frictionless</title>
      <link>http://www.json.blog/2012/11/frictionless/</link>
      <pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/11/frictionless/</guid>
      <description>&lt;p&gt;Apple has released the iPad Mini. Microsoft unveiled the Surface RT. Google has expanded its play with the Nexus 4 (phone) and 10 (tablet) to sandwich the previously released 7. In virtually every review of these
new devices the Apple advantage was ecosystem.&lt;/p&gt;

&lt;p&gt;Time and time again, following descriptions of well designed and built hardware &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:hardware&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:hardware&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, reviewers were forced to add some form of, &amp;ldquo;But the ecosystem cannot compete with Apple&amp;rsquo;s 275,000 tablet-optimized application.&amp;rdquo; I think this understates the power of Apple&amp;rsquo;s amazing developer advantage.&lt;/p&gt;

&lt;p&gt;I use three distinct computing platforms every day: my phone, my tablet, and my traditional PC (laptop and desktop). There are times where I use an application which is specific to one platform or the other. Dark Sky,
for example, is incredibly useful on my iPhone but would be pretty pointless on my Mac Mini or Macbook Air. This kind of platform-specific, quality application is what most would consider the App Store advantage. Not me.&lt;/p&gt;

&lt;p&gt;Apple&amp;rsquo;s true advantage is when applications are available &lt;em&gt;across all three platforms&lt;/em&gt;, offering simultaneously a device-optimized and consistent experience no matter what I am using.&lt;/p&gt;

&lt;p&gt;They offer a &lt;em&gt;frictionless&lt;/em&gt; experience.&lt;/p&gt;

&lt;p&gt;There is a good reason people were so excited for Tweetbot for OSX and love to use Reeder on iPhone, iPad, and OSX. The features, feel, gestures, and even notification sounds having consistency across
environments makes it easier to use computers. The so-called &amp;ldquo;halo effect&amp;rdquo; of the iPod was widely discussed in the early 2000s. iTunes on every Windows machine represented the tail end of a long play that pushed the promise of frictionless computing with Apple products. iOS delivers on this promise in spades.&lt;/p&gt;

&lt;p&gt;Google knows a big selling point of Android is offering the best mobile experience with their web products. As an early and voracious user of Gmail, Google Contacts, and Google Calendar, I do find this enticing. But Android apps are never going to be able to offer the frictionless experience offered by Apple across the mobile and desktop space. ChromeOS is Google&amp;rsquo;s best effort to push a frictionless platform, but it&amp;rsquo;s entirely limited to non-native applications so anything but Google products require major modifications and just won&amp;rsquo;t be the same.&lt;/p&gt;

&lt;p&gt;Microsoft sees the Apple advantage clearly, and they understand Google&amp;rsquo;s inability to fully compete. That&amp;rsquo;s why they are launching Windows 8, in many ways attempting to even further integrate the tablet and desktop than Apple. The Surface, and Windows 8 writ large, is a bet that Apple made a mistake grouping tablets with cell phones. The tablet, according to Microsoft, is about replacing laptops and should be grouped with the desktop.&lt;/p&gt;

&lt;p&gt;I think this is a smart play, regardless of some of the rough reviews of both the Surface RT and Windows 8. Version 1 has some awkward transitions on both devices, but that may be worth the cost to take advantage of a near-future where the power available on a large tablet will be comparable to that of a laptop or even desktop computer. Just as the Macbook Air is every bit as good a consumer computer as &amp;ldquo;the fatter&amp;rdquo; laptop market, soon tablets will be every bit as good a consumer computer that exists. Microsoft&amp;rsquo;s bet is that with that power will come more sophisticated and complex uses, better suited to applications at home on the desktop. They are betting the future is the past&amp;ndash; a full multitasking enabled, file-system revealing environment. If that&amp;rsquo;s what users will eventually want from their tablets, Windows 8 will have these capabilities baked in from the start while iOS struggles to pump out new features and APIs to mimic (or create) these capabilities.&lt;/p&gt;

&lt;p&gt;The future is frictionless. Apple&amp;rsquo;s true advantage is they can already offer one version of that future. If Microsoft plays its cards right, and if it is not too late &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:toolate&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:toolate&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, they can offer an equally compelling alternative. It won&amp;rsquo;t win over the real, dyed-in-the-wool Apple fans, but it may stem the tide carrying the consumer market swiftly away.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:hardware&#34;&gt;Hardly a given in the past from either Google (LG/ASUS) and Microsoft partners. Although Microsoft&amp;rsquo;s actual hardware, until now primarily keyboards and mice (do you pluralize a computer mouse? It seems strange.)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:hardware&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:toolate&#34;&gt;I really think it might be. Windows Phone 7 was brilliant, but released 2 years too late behind at least 1 year of development.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:toolate&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Paul Cuffee Middle School, Addressing Emotional Needs</title>
      <link>http://www.json.blog/2012/11/paul-cuffee-middle-school-addressing-emotional-needs/</link>
      <pubDate>Fri, 02 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/11/paul-cuffee-middle-school-addressing-emotional-needs/</guid>
      <description>&lt;p&gt;I like this piece in Slate on &lt;a href=&#34;http://www.paulcuffee.org&#34;&gt;Paul Cuffee Middle School&lt;/a&gt;, a charter school right here in Providence. Most of what I know about child development seems to suggest that middle schools are sort of ridiculous. At the moment children are looking for role models and close relationships with adults (and not just the kids around them), we decide that kids should have many teachers, teachers should have higher loads, and the kids stay consistent while the adults change constantly.&lt;/p&gt;

&lt;p&gt;In many ways, the elementary school model works better for middle school students and vice versa.&lt;/p&gt;

&lt;p&gt;Anyway, some research showing K-8 schools have a built-in advantage against the traditional middle school:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://educationnext.org/the-middle-school-plunge/&#34;&gt;The Middle School Plunge&lt;/a&gt;
&lt;a href=&#34;http://www0.gsb.columbia.edu/faculty/jrockoff/papers/Rockoff%20Lockwood%20JPubE%202nd%20Revision%20June%202010.pdf&#34;&gt;Stuck in the Middle&lt;/a&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;A more &amp;ldquo;popular&amp;rdquo; version on Education Next &lt;a href=&#34;http://educationnext.org/stuck-in-the-middle/&#34;&gt;here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Where I Share</title>
      <link>http://www.json.blog/2012/10/where-i-share/</link>
      <pubDate>Tue, 09 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/10/where-i-share/</guid>
      <description>&lt;p&gt;I have been meaning to write this post for the past couple of weeks. Like most other people, I am constantly experimenting with different ways to publish and share my thoughts and engage with social networking. Lately, I have settled into what feels like an &amp;ldquo;end state&amp;rdquo; workflow&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I will devote a future post to the details of how I manage my online reading, writing, and sharing workflow but for now I just wanted to let folks know where they can find me.&lt;/p&gt;

&lt;p&gt;For random thoughts throughout the day I mostly turn to &lt;a href=&#34;http://www.twitter.com/#!/jasonpbecker&#34;&gt;my Twitter account&lt;/a&gt; or increasingly &lt;a href=&#34;https://alpha.app.net/jbecker&#34;&gt;my App.net account&lt;/a&gt;. I am a retweet abuser, so if you follow me there be warned. I often just retweet things I find funny or interesting, write some random complaint about coding, policy, or education when I&amp;rsquo;m frustrated and don&amp;rsquo;t understand the world, and try syndicate some of the other sources I&amp;rsquo;ll list here. I also like to talk to people on Twitter, so if you&amp;rsquo;re looking for conversation that&amp;rsquo;s the place to go. I almost use it like it&amp;rsquo;s the new IRC/AIM Chatroom. My Twitter account is a bit more Providence/Rhode Island heavy than most other ways to follow me.&lt;/p&gt;

&lt;p&gt;Some of you may know that &lt;a href=&#34;http://tumblr.jasonpbecker.com&#34;&gt;I also have a Tumblr&lt;/a&gt; that has fallen in and out of favor. I used to blog over there before creating this Wordpress site&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Recently, I have used my Tumblr account much more. Since Google Reader &lt;a href=&#34;http://techcrunch.com/2011/10/20/google-reader-getting-overhauled-removing-your-friends/&#34;&gt;removed its social features&lt;/a&gt; I have tried to find the best way to share the best stuff I read each day with a few thoughts. I &lt;a href=&#34;https://plus.google.com/103283548915451814191/&#34;&gt;toyed with Google Plus&lt;/a&gt;, but it really is dead. I don&amp;rsquo;t find good content there and engagement with my sharing has been very low. Also, the &lt;a href=&#34;https://groups.google.com/forum/?fromgroups=#!topic/google-plus-developers/zBMOUy9pHFc&#34;&gt;lack of a write API&lt;/a&gt; makes it very challenging to incorporate in a non-disruptive way.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;So right now, head over and follow my Tumblr (natively or RSS) if you want to get 5-10 link posts each day of things I&amp;rsquo;ve collected across the web. Some of my favorite online friends found me through my Google Reader sharing and I suspect that they would enjoy my Tumblr most of all. If I start getting more engagement around what type of links folks are enjoying I can begin to shift the topics I post on. I collect many more links than what end up in Tumblr in Google Reader and Pinboard. I have a very specific path to end up in Tumblr that leans more towards long reads and shares from friends and not what I am watching on RSS.&lt;/p&gt;

&lt;p&gt;A few months ago I ditched my original Facebook account from 2005 and reopened &lt;a href=&#34;https://www.facebook.com/jasonpaulbecker&#34;&gt;a fresh one&lt;/a&gt;. I did this for two reasons: 1) I had collected many friends that I was not truly in contact with. Because of the layers and layers of privacy changes that Facebook went through, it became very difficult to maintain settings I was comfortable with. I wanted to start fresh with friends and fresh with how I manage privacy. 2) Related to 1, I never used Facebook as a networking tool. To me, it was always supposed to be a way to interact and keep in touch with friends from &amp;ldquo;real life&amp;rdquo;. Ultimately, I didn&amp;rsquo;t find that aspect of Facebook to be all that valuable. So I&amp;rsquo;m trying to be a believer and use Facebook more like I use other social media&amp;ndash;  a way to tap into my &amp;ldquo;interest graph&amp;rdquo; and meet new people and read new things and have new conversations. You can follow me there with a few caveats. I hate using Facebook, so it is probably going to have the least content. There will still be some personal stuff as most of my friends still see Facebook as an intimate space, shocking though that may seem. Finally, I may not friend you back. Yes, the point of this account is to be more open, but Facebook still creeps me out and on any given day I may feel more or less incline to be open on there.&lt;/p&gt;

&lt;p&gt;This blog will remain where I write longer pieces that are primarily &amp;ldquo;original&amp;rdquo; analysis/thoughts and less news/broadcast-like. I hope to share a lot more code and thoughts on current research in the near future now that I&amp;rsquo;m changing jobs.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Subject to change, but I&amp;rsquo;m betting it&amp;rsquo;s more tweaks at this point than dramatic shifts
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;and I really want to leave Wordpress, but that is going to be a big project
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Definitely more on this in my future workflow post
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts on Grit</title>
      <link>http://www.json.blog/2012/10/thoughts-on-grit/</link>
      <pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/10/thoughts-on-grit/</guid>
      <description>

&lt;p&gt;I have not had the opportunity to read &lt;a href=&#34;http://www.paultough.com/the-books/how-children-succeed/&#34;&gt;Paul Tough&amp;rsquo;s newest book&lt;/a&gt; on &amp;ldquo;grit&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I have, however, read Paul Tough&amp;rsquo;s &lt;a href=&#34;http://www.nytimes.com/2011/09/18/magazine/what-if-the-secret-to-success-is-failure.html&#34;&gt;New York Times Magazine article on grit&lt;/a&gt; and recently listened to an &lt;a href=&#34;http://www.econtalk.org/archives/2012/09/paul_tough_on_h.html&#34;&gt;EconTalk podcast&lt;/a&gt; where he discussed &lt;span style=&#34;text-decoration: underline;&#34;&gt;How Children Succeed&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The thrust of Tough&amp;rsquo;s argument, if I were to be so bold, is that there is a definable set of non-cognitive skills, called &amp;ldquo;grit&amp;rdquo;, that are at least as important as academic achievement in determining long-term positive outcomes for kids. Great schools, therefore, would do well to focus on developing these habits as much and as intentionally as they do developing content knowledge and academic prowess. This, according to Tough, is a big part of the &amp;ldquo;magic sauce&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;of &amp;ldquo;No Excuses&amp;rdquo; schools like KIPP. They teach &amp;ldquo;grit&amp;rdquo; as a part of their intense behavioral management and culture efforts.&lt;/p&gt;

&lt;p&gt;I think Tough is an engaging writer and has a great knack for finding some of the most interesting research not often read in education policy circles, but which is clearly relevant. While listening to the EconLog podcast I found myself often disagreeing with his interpretations/conclusions. But more often, I found myself desperately wishing for a different, slower format because so much of this work begged deeper questioning and conversation. What better reason could there be to buy and read a book-length treatment of these ideas?&lt;/p&gt;

&lt;p&gt;Anyway, I thought I&amp;rsquo;d share just a few of my thoughts on &amp;ldquo;grit&amp;rdquo; based on this interview and the earlier New York Times Magazine piece.&lt;/p&gt;

&lt;h2 id=&#34;teaching-conscientiousness-in-a-society-that-has-been-so-unconscientious&#34;&gt;Teaching conscientiousness in a society that has been so unconscientious&lt;/h2&gt;

&lt;p&gt;It seems fairly obvious that people who don&amp;rsquo;t &amp;ldquo;play by the rules&amp;rdquo; and aren&amp;rsquo;t easily motivated to conform to certain habits are less likely to be successful. It is unsurprising that Tough finds research that suggests that there is a &amp;ldquo;grit&amp;rdquo; gap between rich and poor. I want to know more about why, and I have, what I hope, is one interesting idea of what contributes to the &amp;ldquo;grit gap&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I believe that deterioration of the built environment, especially among the urban and truly rural poor, is a major contributor to low grit. Some parts of this country with high concentrations of poverty look&amp;ndash; bombed out. Roads are littered with deep potholes and scars. The houses have chipped paint, rotting wood exterior elements, and unkept yards. Storefronts were built decades ago on the cheap, aged poorly, and were never updated. Their schools lack good lighting, decent HVAC systems, and functioning toilets. There is no pride found in any of these spaces.&lt;/p&gt;

&lt;p&gt;Children growing up in poverty do not see neighbors obsessing over their lawn. They do not watch one house after another repaint and reface their exteriors to ensure they weren&amp;rsquo;t the ugliest house on the block. They do not see brand new cars, fresh asphalt roads, and schools that resemble palaces. I don&amp;rsquo;t think virtually any of this has to do with the people who live in these neighborhoods. I do think it reflects the pathetic state that society has deemed acceptable, so long as it remains sight unseen by those with resources.&lt;/p&gt;

&lt;p&gt;Growing up in poverty often means being surrounded by spaces that society has left to rot. How can these children learn conscientiousness when the privileged have been so unconscientious?&lt;/p&gt;

&lt;h2 id=&#34;the-m-m-study&#34;&gt;The M&amp;amp;M Study&lt;/h2&gt;

&lt;p&gt;Tough mentions &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1310767/&#34;&gt;a study&lt;/a&gt; where students first take an IQ test under normal conditions. These same students are then given an IQ test but are rewarded with an M&amp;amp;M each time they get a question right. This tiny immediate incentive resulted in a massive, 1.8SD improvement in mean IQ. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:0&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The implications are fascinating. It demonstrates the importance of motivation even while taking a test that is supposedly measuring an intractable, permanent attribute people have. This seems obvious and is fairly well known, but forgotten in many policy circles. I have often lamented that the New England Common Assessment Program has a sizable downward bias when measuring achievement because the exam is low stakes for students. The dramatic decrease in performance observed on the 11th grade NECAP math exam is almost certainly in part due to lower intrinsic motivation amongst high school students compared to their 8th grade and younger selves.&lt;/p&gt;

&lt;p&gt;There are some students that have no measurable response to the M&amp;amp;M incentive. These students are exhibiting qualities of Tough&amp;rsquo;s &amp;ldquo;grit&amp;rdquo;, conscientiousness that leads one to do well simply because they are being measured, or perhaps because there is no reason to do something if it is not going to be done well. I believe that there is also a bias against schools with concentrated poverty because of an uneven distribution of &amp;ldquo;grit&amp;rdquo;&amp;ndash; suburban middle to upper class students with college ambitions will likely be the students who will sit down and try hard on a test just because they are being measured whereas urban students living in poverty are far less likely to exert that same effort for an exercise with no immediate or clear long-term consequences.&lt;/p&gt;

&lt;p&gt;All of this would be pretty blasé were it not for the more distal outcomes observed. The group of students that did not respond to the M&amp;amp;M incentives had significantly and practically better outcomes than those that responded to the incentive. I can&amp;rsquo;t recall exactly which outcomes were a part of this study, but Tough cites several independent studies that measure a similar set of qualities and find far better outcomes with GPA, graduation from high school, post-secondary degree attainment, juvenile delinquency or adult criminal activity, and wages.&lt;/p&gt;

&lt;p&gt;Tough&amp;rsquo;s interpretation of these results seems to &lt;a href=&#34;http://blog.jasonpbecker.com/2012/09/19/thoughts-on-grading/&#34;&gt;mirror my feelings on grading&lt;/a&gt;. Low stakes testing (or in this case, no-incentive testing) has omitted variable bias which leads to observing students who lack &amp;ldquo;grit&amp;rdquo; as lower achieving than they are. The test results are still excellent predictors of later success but lack validity as a pure measure of academic achievement. My complaint about grades that use behavior, attendance, and participation&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;does not stem from their lack of validity at predicting later outcomes. These grades are excellent predictors of later outcomes. Rather, it stems from these grade conflating two very different qualities into a single measure, making it far more difficult to design appropriate interventions and supports that target individual needs.&lt;/p&gt;

&lt;p&gt;Tough seems thinks this means that high stakes placed on test scores over emphasizes one quality over the other when both are very important. I disagree. I feel that high stakes test scores recreate the M&amp;amp;M incentive and leads to a better measure of academic ability. That is not to say that we don&amp;rsquo;t need to cultivate and measure non-cognitive skills. It just means that trying to measure both at once&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;results in less clear and actionable interpretations.&lt;/p&gt;

&lt;h2 id=&#34;is-the-grit-problem-properly-described-as-a-failure-to-recognize-long-term-benefits&#34;&gt;Is the &amp;ldquo;grit&amp;rdquo; problem properly described as a failure to recognize long-term benefits?&lt;/h2&gt;

&lt;p&gt;Repeatedly both Tough and host Russ Roberts point to the need to provide students who lack grit more information on the long-term benefits of &amp;ldquo;doing well&amp;rdquo;. For example, Tough cites KIPP&amp;rsquo;s posting of the economic benefits of a bachelor&amp;rsquo;s degree on walls in the halls of their schools as a way to build grit. Somewhat left unsaid is the idea that grit-like behaviors may not describe some kind of &amp;ldquo;intrinsic&amp;rdquo; motivation, but instead represent an understanding of the long-term extrinsic benefits of certain actions. Grit really means understanding that, &amp;ldquo;If I behave appropriately, I will gain the respect of this authority and earn greater autonomy/responsibility,&amp;rdquo; or perhaps, &amp;ldquo;Doing my homework each night will teach me good habits of work and help me to learn this academic material so I can succeed in college and get a better job.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Can grit really be just a heuristic developed to better respond to long-term incentives?&lt;/p&gt;

&lt;p&gt;I am not sure. I am equally unsure that the activities of a &amp;ldquo;No Excuses&amp;rdquo; school actually generate the long-term benefits of &amp;ldquo;grit&amp;rdquo;. If grit is a powerful heuristic to optimize long-term outcomes, how do we know that many short-term incentives that build behaviors toward academic success mean that students better respond to a broad set of long-term outcomes? Should we believe that behavior bucks/demerit systems, constant small corrections, repeatedly stating the goals of education and its benefits, and other KIPP-like culture-building strategies build a bend toward acting in ways that maximize long-term outcomes? Do students aspire to college because they have internalized its importance, or do the stack of short-term incentives build a desire for sprokets, wignuts, and widgets that just happened to be called a &amp;ldquo;bachelor&amp;rsquo;s degree&amp;rdquo; in this case?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;I use &amp;ldquo;grit&amp;rdquo; a lot in this post. Please insert quotes each time. It got obnoxious reading it with the quotes actually in place
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;My term, not his. Probably stolen from one of my colleagues who uses this term a lot.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:0&#34;&gt;From 79 to 97 according to EconTalk
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;among other non-cognitive, non-academic skills and activities
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Or inadvertently measuring both at once, as many low-stakes standardized tests do
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What Can Management Do? iOS6 Maps Monday-Morning Quarterbacking</title>
      <link>http://www.json.blog/2012/10/what-can-management-do-ios6-maps-monday-morning-quarterbacking/</link>
      <pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/10/what-can-management-do-ios6-maps-monday-morning-quarterbacking/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/philiped&#34;&gt;Philip Elmer-DeWitt&lt;/a&gt; has &lt;a href=&#34;http://tech.fortune.cnn.com/2012/09/29/does-apple-have-a-scott-forstall-problem/&#34;&gt;suggested&lt;/a&gt; the &lt;a href=&#34;http://theamazingios6maps.tumblr.com/&#34;&gt;iOS6 Maps debacle&lt;/a&gt; falls on the shoulders of &lt;a href=&#34;http://en.wikipedia.org/wiki/Scott_Forstall&#34;&gt;Scott Forstall&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. When I first read the piece, I felt like it was unfair to blame management for this kind of failure. In my experience, the Maps application is wonderful software. The turn-by-turn directions are elegant and beautiful. The vector-based maps load fast and use &lt;a href=&#34;http://www.loopinsight.com/2012/10/01/apple-maps-up-to-five-times-more-data-efficient-than-google-maps/&#34;&gt;substantially less data&lt;/a&gt;. The reality is the Map app is great; the data are less so.&lt;/p&gt;

&lt;p&gt;Building great mapping data is no easy task. It takes years. &lt;a href=&#34;http://www.theatlantic.com/technology/archive/2012/09/how-google-builds-its-maps-and-what-it-means-for-the-future-of-everything/261913/&#34;&gt;It takes human intervention&lt;/a&gt;. It takes users. Short of a massive acquisition of an existing player, like Garmin, there was little hope of Apple developing a great map application for day one of release. Hell, in my experience, most stand alone GPS data is pretty awful in all the ways the Apple data is awful. That&amp;rsquo;s why I primarily used my iPhone as my GPS the last few years. The experience was consistently better and less frustrating. Perhaps even more critically, Apple is just not a data company. Google is the king of data. The skills required to build great geographic data simply doesn&amp;rsquo;t map well against previous Apple competencies. None of this means that the Apple Map situation is good or even &amp;ldquo;excusable&amp;rdquo;. I just think the map situation is &amp;ldquo;understandable&amp;rdquo; and would not be with different guidance.&lt;/p&gt;

&lt;p&gt;But then I reevaluated and realized that there is a major way that management could have improved Apple Maps for iOS. Managers should set the bar for quality, make sure that bar is met, and adjust both resources and expectations when a project is not meeting user expectations. It must have been obvious to Apple management that the quality expectations were not going to be met.&lt;/p&gt;

&lt;p&gt;What could Forstall have done? Some have suggested thrown substantially more money at the project. Others say he should have &amp;ldquo;&lt;a href=&#34;http://www.mondaynote.com/2012/09/23/apple-maps/&#34;&gt;winked&lt;/a&gt;&amp;rdquo; at Apple users and clearly signaled that Maps were in their infancy. And of course there were those who said he should have waited another year for the Google Maps contract to expire. John Gruber is &lt;a href=&#34;http://daringfireball.net/2012/09/timing_of_apples_map_switch&#34;&gt;rather convincing&lt;/a&gt; that simply waiting another year was not an option. Apple really couldn&amp;rsquo;t swap maps out of iOS in the middle of the OS cycle. It would be jarring and far more frustrating than the current situation.&lt;/p&gt;

&lt;p&gt;I would have recommended a third option.&lt;/p&gt;

&lt;p&gt;Apple should have released iOS6 Maps as US only.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;@&lt;a href=&#34;https://twitter.com/jdalrymple&#34;&gt;jdalrymple&lt;/a&gt; what if Apple execs realized it wasn&amp;rsquo;t going well &amp;amp;
made maps US only &amp;amp; world in 6-12mo. Still had Google contract time
for that&lt;/p&gt;

&lt;p&gt;— Jason Becker (@jasonpbecker) &lt;a href=&#34;https://twitter.com/jasonpbecker/status/252128849469526017&#34;&gt;September 29, 2012&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the major themes of the iPhone 5 release was that this was a global phone. Global LTE, with day one [launches in more countries and reaches far more countries, faster, than ever before after that][]. In fact, the Verizon CDMA iPhone comes with an &lt;a href=&#34;http://www.theverge.com/2012/9/25/3405610/verizon-iphone-5-unlocked-open-access-fcc&#34;&gt;unlocked GSM radio&lt;/a&gt;. But mapping is hard, and that problem becomes orders of magnitude more difficult with each inch of the planet that needs to be covered. When it became clear that Apple had a beautiful application, but awful data, Forstall and the rest of Apple management should have adjusted expectations and promised a US-only release that met the quality that consumers have come to expect. This would serve to increase resources, while winking at users, and utilizing the remainder of the Google contract for international mapping. With six additional months Apple could make great strides improving international data and possibly signing some additional, high-profile maps data deals with local sources/competitors that would love to be associated with Apple, even if it is just in a footnote. US users would rave about the great vector mapping, the turn by turn directions that are brilliantly integrated into the lock screen and always provide just enough information, and the cool integration into Open Table and Yelp. US maps would get better because they would have constant users. The rest of the world would lap up iPhone 5s and wait anxiously for their chance to taste the Great Apple Maps.&lt;/p&gt;

&lt;p&gt;In this scenario, it is possible that Apple could have had the best of both worlds: a far worse data set in an application that cost just as much, but by limiting the scope to their key market, a reputation for excellence that would lead to excitement for the end of a competitor&amp;rsquo;s product.&lt;/p&gt;

&lt;p&gt;I am sure there were other challenges with producing a US-only&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;that I am not considering. I think this is at least one typical techniques in IT management that Apple could have employed for a smoother, better release of their first efforts into a complicated and competitive space.&lt;/p&gt;

&lt;p&gt;[launches in more countries and reaches far more countries, faster,
than ever before after that]: &lt;a href=&#34;http://appleinsider.com/articles/12/09/13/apples_aggressive_iphone_5_launch_schedule_to_reach_31_countries_in_sept_quarter&#34;&gt;http://appleinsider.com/articles/12/09/13/apples_aggressive_iphone_5_launch_schedule_to_reach_31_countries_in_sept_quarter&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Of &lt;a href=&#34;http://www.fastcodesign.com/1670760/will-apples-tacky-software-design-philosophy-cause-a-revolt&#34;&gt;iOS skeumorphism fame&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Or North America only. There are barely any roads in Canada, right?
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts on Grading</title>
      <link>http://www.json.blog/2012/09/thoughts-on-grading/</link>
      <pubDate>Wed, 19 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/09/thoughts-on-grading/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.twitter.com/MrPABruno&#34;&gt;Bruno&lt;/a&gt; is a skeptic on &lt;a href=&#34;http://scholasticadministrator.typepad.com/thisweekineducation/2012/09/bruno-is-standards-based-grading-a-good-idea.html&#34;&gt;standards-based grading&lt;/a&gt;. He seems to think
that &amp;ldquo;mastery of content&amp;rdquo; is too abstract for students to work toward
and rightly cites evidence that motivation and changed behavior are
tightly linked to a sense of efficacy, which in turn is tightly linked
to feeling as though you know precisely what to do to get to a
particular outcome.&lt;/p&gt;

&lt;p&gt;But isn&amp;rsquo;t mastery of content essentially, &amp;ldquo;Do well on your assignments
and tests&amp;rdquo;? And while a massive, standards-based report card may be hard
for a parent to read, is it any more confusing than seeing awful results
on standardized tests and a student who clearly doesn&amp;rsquo;t read on
grade-level receive good grades because of participation, attendance,
and behavior? As a parent, how do you know to intercede on your child&amp;rsquo;s
behalf when you see a &amp;ldquo;B&amp;rdquo; which actually represents a C- on content
knowledge and skills and an A+ for effort, behavior, and completion?&lt;/p&gt;

&lt;p&gt;Ultimately, I am against including behavior, attendance, and effort as a
part of the same grade as academics. I think there needs to be a clear
place to present evidence of academic ability and growth independent of
behavioral growth. Both are important, and while linked, are certainly
not moving in lockstep for the typical child. Accurate information in
both domains is far better than falsely presenting a singular, mixed-up
&amp;ldquo;truth&amp;rdquo; about a child&amp;rsquo;s success in school.&lt;/p&gt;

&lt;p&gt;For the same reason I am not a fan of school report cards with a single
letter grade rating, I am not for just a single letter grade for
students. Ultimately, they both represent poor combinations of data that
obscure more than they reveal.&lt;/p&gt;

&lt;p&gt;Developing report cards or &amp;ldquo;grading&amp;rdquo; systems, both for program
evaluation and for students, always conjures one of the few concepts I
recall from linear algebra. It seems to me that any good grading system
should provide a basis, that is, a minimal set of linearly independent
vectors which, via linear combination, can describe an entire vector
space. Remove the jargon and you&amp;rsquo;re left with:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Measure the least amount of unrelated things possible that, taken
together, describe all there is to know about what you are measuring.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A single grade that combines all the effort, behavior, attendance, and
various unrelated academic standards I might get an overall description
that says &amp;ldquo;round&amp;rdquo;. But by separating out the data at some other level,
the picture might describe a golf ball and its dimples, a baseball and
its stitches, or a soccer ball with its hexagon-pentagon pattern.&lt;/p&gt;

&lt;p&gt;I think we need to find a way to let people know what kind of ball they
have.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>There must be an easier way... survey questions in R</title>
      <link>http://www.json.blog/2012/08/there-must-be-an-easier-way...-survey-questions-in-r/</link>
      <pubDate>Wed, 22 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/08/there-must-be-an-easier-way...-survey-questions-in-r/</guid>
      <description>&lt;p&gt;So I have this great little custom function I&amp;rsquo;ve used when looking at survey data in R. I call this function &lt;code&gt;pull()&lt;/code&gt;. The goal of &lt;code&gt;pull()&lt;/code&gt; is to quickly produce frequency tables with n sizes from individual-level survey data.&lt;/p&gt;

&lt;p&gt;Before using &lt;code&gt;pull()&lt;/code&gt;, I create a big table that includes information about the survey questions I want to pull. The data are structured like this:&lt;/p&gt;

&lt;p&gt;&lt;table align=&#34;center&#34;&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tbody&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
quest&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
survey&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
year&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
break&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
ss01985&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
elementary&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
2011_12&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
schoolcode&lt;/p&gt;

&lt;p&gt;&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tbody&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/table&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;quest represents the question coding in the raw survey data.&lt;/li&gt;
&lt;li&gt;survey is the name of the survey (in my case, the elementary school students, middle school students, high school students, parents, teachers, or administrators).&lt;/li&gt;
&lt;li&gt;year is the year that the survey data are collected.&lt;/li&gt;
&lt;li&gt;break is the ID I want to aggregate on like schoolcode or districtcode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They key is that &lt;code&gt;paste(survey, year,sep=&#39;&#39;)&lt;/code&gt; produces the name of the &lt;code&gt;data.frame&lt;/code&gt; where I store the relevant survey data. Both quest and break are columns in the survey data.frame. Using a data.frame with this data allows me to apply through the rows and produce the table for all the relevant questions at once. &lt;code&gt;pull()&lt;/code&gt; does the work of taking one row of this &lt;code&gt;data.frame&lt;/code&gt; and producing the output that I&amp;rsquo;m looking for. I also use &lt;code&gt;pull()&lt;/code&gt; one row at a time to save a data.frame that contains these data and do other things (like the visualizations in this post).&lt;/p&gt;

&lt;p&gt;In some sense, &lt;code&gt;pull()&lt;/code&gt; is really just a fancy version of &lt;code&gt;prop.table&lt;/code&gt; that takes in passed paramaters and adds an &amp;ldquo;n&amp;rdquo; to each row and adding a &amp;ldquo;total&amp;rdquo; row. I feel as though there must be an implementation of an equivalent function in a popular package (or maybe even base) that I should be using rather than this technique. It would probably be more maintainable and easier for collaborators to work with this more common implementation, but I have no idea where to find it. So, please feel free to use the code below, but I&amp;rsquo;m actually hoping that someone will chime in and tell me I&amp;rsquo;ve wasted my time and I should just be using some function foo::bar.&lt;/p&gt;

&lt;p&gt;P.S. This post is a great example of why I really need to change this blog to Markdown/R-flavored Markdown. All those inline references to functions, variables, or code should really be formatted in-line which the syntax highlighter plug-in used on this blog does not support. I&amp;rsquo;m nervous that using WP-Markdown plugin will botch formatting on older posts, so I may just need to setup a workflow where I pump out HTML from the Markdown and upload the posts from there. If anyone has experience with Markdown + Wordpress, advice is appreciated.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pull &amp;lt;- function(rows){
  # Takes in a vector with all the information required to create crosstab with
  # percentages for a specific question for all schools.
  # Args:
  #  rows: Consists of a vector with four objects.
  #        quest: the question code from SurveyWorks
  #        level: the &amp;quot;level&amp;quot; of the survey, i.e.: elem, midd, high, teac, admn,
  #        pare, etc.
  #        year: the year the survey was administered, i.e. 2011_12
  #        sch_lea: the &amp;quot;break&amp;quot; indicator, i.e. schoolcode, districtcode, etc.
  # Returns:
  # A data.frame with a row for each &amp;quot;break&amp;quot;, i.e. school, attributes for
  # each possible answer to quest, i.e. Agree and Disagree, and N size for each
  # break based on how many people responded to that question, not the survey as
  # a whole, i.e. 

  # Break each component of the vector rows into separate single-element vectors
  # for convenience and clarity.
  quest &amp;lt;- as.character(rows[1])
  survey &amp;lt;- as.character(rows[2])
  year  &amp;lt;- as.character(rows[3])
  break &amp;lt;- as.character(rows[4])
  data &amp;lt;- get(paste(level,year,sep=&#39;&#39;))
  # Data is an alias for the data.frame described by level and year.
  # This alias reduces the number of &amp;quot;get&amp;quot; calls to speed up code and increase
  # clarity.
  results &amp;lt;- with(data,
                  dcast(data.frame(prop.table(table(data[[break]],
                                                    data[[quest]]),
                                              1))
                        ,Var1~Var2,value.var=&#39;Freq&#39;))
  # Produces a table with the proportions for each response in wide format.
  n &amp;lt;- data.frame(Var1=rle(sort(
    subset(data, 
           is.na(data[[quest]])==F &amp;amp; is.na(data[[break]])==F)[[break]]))$values,
                  n=rle(sort(
                    subset(data,
                           is.na(data[[quest]])==F &amp;amp;
                             is.na(data[[break]])==F)[[break]]))$lengths)
  # Generates a data frame with each break element and the &amp;quot;length&amp;quot; of that break
  # element. rle counts the occurrences of a value in a vector in order. So first
  # you sort the vector so all common break values are adjacent then you use rle
  # to count their uninterupted appearance. The result is an rle object with 
  # two components: [[values]] which represent the values in the original, sorted
  # vector and [[length]] which is the count of their uninterupted repeated
  # appearance in that vector.
  results &amp;lt;- merge(results, n, by=&#39;Var1&#39;)
  # Combines N values with the results table.

  state &amp;lt;- data.frame(t(c(Var1=&#39;Rhode Island&#39;, 
                          prop.table(table(data[[quest]])),
                          n=dim(subset(data,is.na(data[[quest]])==F))[1])))
  names(state) &amp;lt;- names(results)
  for(i in 2:dim(state)[2]){
    state[,i] &amp;lt;- as.numeric(as.character(state[,i]))
  }
  # Because the state data.frame has only one row, R coerces to type factor.
  # If I rbind() a factor to a numeric attribute, R will coerce them both to
  # characters and refuses to convert back to type numeric.
  results &amp;lt;- rbind(results, state)
  results
}   
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Pay For Quality: Join App.net</title>
      <link>http://www.json.blog/2012/08/pay-for-quality-join-app.net/</link>
      <pubDate>Mon, 06 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/08/pay-for-quality-join-app.net/</guid>
      <description>&lt;p&gt;I like paying for good software. There are applications I use every day, some for hours a day, that make my experience on the web and on my computers better. I have paid for &lt;a href=&#34;http://reederapp.com/&#34;&gt;Reeder&lt;/a&gt; on three platforms, &lt;a href=&#34;http://tapbots.com/software/tweetbot/&#34;&gt;Tweetbot&lt;/a&gt; on two&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, &lt;a href=&#34;http://pinboard.in&#34;&gt;Pinboard&lt;/a&gt;, and many others. I like to pay, because I value my time, my experience, and my productivity.&lt;/p&gt;

&lt;p&gt;I also like to pay because I value my privacy.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t get me wrong&amp;ndash; I am a Google addict, using Gmail practically from the beginning, GChat, Google Calendar, &lt;a href=&#34;https://plus.google.com/103283548915451814191/about&#34;&gt;Google+&lt;/a&gt;, Google Reader, etc, etc. I have a &lt;a href=&#34;https://www.facebook.com/jasonpaulbecker&#34;&gt;Facebook account&lt;/a&gt; (although I recently removed my original account from 2005). I spent quite a bit of time on &lt;a href=&#34;http://www.twitter.com/#!/jasonpbecker&#34;&gt;Twitter&lt;/a&gt;. These are g are reat places to do great work and to have a lot of fun. They are key parts of my professional and personal life. All of these services, however, built around the model of selling &lt;em&gt;me&lt;/em&gt;. They offer a real modern day example of &lt;a href=&#34;http://en.wikipedia.org/wiki/There_ain&#39;t_no_such_thing_as_a_free_lunch&#34;&gt;TANSTAAFL&lt;/a&gt;. Nothing leaves my pocket, but massive hoards of data are used to direct advertising my way and some of that data is even sold to other companies. Knowing your customers has always been valuable, and the price of &amp;ldquo;free&amp;rdquo; is my very identity.&lt;/p&gt;

&lt;p&gt;Now, generally I think that these major companies are good stewards of my privacy. As a &lt;a href=&#34;http://www.linkedin.com/pub/jason-becker/20/94a/80a&#34;&gt;budding data professional&lt;/a&gt;, I know just how difficult and meaningless it would be for any of these companies to truly target &lt;em&gt;me&lt;/em&gt; rather than learn about a cloud of millions people moving at once. I also believe they realize how much of their business model requires trust. Without trust, giving up our privacy will feel like an increasingly large ask.&lt;/p&gt;

&lt;p&gt;I value my privacy, but I value good software as well. Right now, I have not found alternatives for many &amp;ldquo;free&amp;rdquo; services that are good enough to make up for the cost of my privacy. I am a willing participant in selling my privacy, because I feel I get more value back than I am losing.&lt;/p&gt;

&lt;p&gt;But, privacy is not the only reason I wish there were alternative services and software I could buy.&lt;/p&gt;

&lt;p&gt;I was probably pretty sloppy in this post interchanging &amp;ldquo;software&amp;rdquo; and &amp;ldquo;services&amp;rdquo;. Many of the websites or software I mentioned are merely front ends for a more valuable service. Gmail is not the same thing as email. Reeder is actually a software alternative (and more) to Google Reader&amp;rsquo;s web-based front end for a &lt;a href=&#34;http://en.wikipedia.org/wiki/News_aggregator&#34;&gt;news aggregator&lt;/a&gt;. GChat is just a &lt;a href=&#34;http://www.jabber.org/&#34;&gt;Jabber&lt;/a&gt;/&lt;a href=&#34;http://xmpp.org/&#34;&gt;XMPP&lt;/a&gt; client. Ultimately, much of what I do around the internet is about moving structured data around between peers and producer-consumer relationships. All of the great things that made the web possible were protocols like &lt;a href=&#34;http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol&#34;&gt;HTTP&lt;/a&gt;, &lt;a href=&#34;http://en.wikipedia.org/wiki/Internet_protocol_suite&#34;&gt;TCP/IP&lt;/a&gt;, etc. And the protocols of todays web are the standardized &lt;a href=&#34;http://en.wikipedia.org/wiki/Application_programming_interface&#34;&gt;API&lt;/a&gt;s that allow programmers a way to interact with data. Great, innovative software for the web is being built that ultimately change the way we see and edit data on these services. The common analogy here is that of a utility. The API helps users tap into vast networks of pipes and interact with the flow of information in new, exciting ways.&lt;/p&gt;

&lt;p&gt;To get a sense of how amazing new things can be done with an API look no further than &lt;a href=&#34;http://ifttt.com&#34;&gt;IFTTT&lt;/a&gt;. It is like a masterful switching station for some of the most useful APIs on the web. Using Recipes on IFTTT, I can do something amazing like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Find a really cool link from a friend on Twitter.&lt;/li&gt;
&lt;li&gt;Save that link to Pinboard, a great bookmarking site, with tags and a description so that I can find it later easily.&lt;/li&gt;
&lt;li&gt;Add tags to the Pinboard bookmark for the social sites I want to share on, e.g. to:twitter to:facebook to:linkedin to:tumblr, all of which are special tags that I use with the IFTTT API.&lt;/li&gt;
&lt;li&gt;IFTTT, which is linked to my Pinboard account, looks occasionally to see any recently saved links. It finds a new link with those special tags (called Triggers).&lt;/li&gt;
&lt;li&gt;Each of those special tags tells IFTTT to make a new post on a different social networking site sharing my link (sometimes with tags, sometimes with the description, sometimes with nothing, all of which I set up) seamlessly without any user interaction.&lt;/li&gt;
&lt;li&gt;My cool link gets sent strategically where I want it to be sent without every leaving the site. I just clicked one button and added the right tags.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This kind of interaction model is impossible without agreed upon standards for sites to read and write information to one another.&lt;/p&gt;

&lt;p&gt;The open API which made it so easy to innovate quickly from the outside&amp;ndash; Facebook&amp;rsquo;s Platform, the Twitter API, etc&amp;ndash; is under a serious existential threat. The truth is, these darlings of Web 2.0 don&amp;rsquo;t have a great idea about how to make money. The free web has almost entirely depended on advertising revenues to turn a profit. But how can these companies make money if I&amp;rsquo;m not using their webpage or their website to get access to &lt;strong&gt;my&lt;/strong&gt; data?&lt;/p&gt;

&lt;p&gt;Do you see the part that I slipped in there? These companies have lost site of one very important part of the equation&amp;ndash; the content was free because the users created it. Its &lt;strong&gt;our&lt;/strong&gt; data.&lt;/p&gt;

&lt;p&gt;Twitter seems to be on the verge of removing or limiting critical portions of the their API at the expense of many developers building new ways to interact with Twitter data, and, more importantly, all of their users who have joined Twitter because it was a powerful platform, not just a fun interactive website. Their tumultuous corporate culture has landed here because they decided that the promise of big revenues for their investors is not enhanced by people accessing Twitter through unofficial channels. Facebook has made similar moves in light of its short, but disastrous history as a public company.&lt;/p&gt;

&lt;p&gt;If things shake out the way they seem to be, the sexy startups of Web 2.0 will turn away from the openness conducive to gaining users as they mature. These sites will consolidate and limit the experience, pushing
for more page views and time on their site by making it hard to leave. They are rebuilding America Online&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, trying to make it so that their webpage becomes synonymous with &amp;ldquo;the Internet&amp;rdquo; for their users. Want your ads to be worth more money? Make it hard to change the channel.&lt;/p&gt;

&lt;p&gt;It is for this reason that I am supporting &lt;a href=&#34;http://join.app.net&#34;&gt;App.net&lt;/a&gt;. The commitment is a little steep, until you consider how valuable these services have become. For the cost of one pretty nice meal out with my girlfriend, I am purchasing one of the best ways to communicate on the web. I am supporting a model for good software that means that user experience and needs are paramount. I am purchasing customer service because I am sick of ad companies being the customer while I am stuck as the product. I am paying for access so that there is a large competitive market vying for the best way for me to interact with my data. I am paying because I am ready, no desperate, for great consumer software and services that live and breathe to optimize my experience. I used to trust the free web for this, but their business model and their success means they don&amp;rsquo;t need me as much as they need their advertisers anymore.&lt;/p&gt;

&lt;p&gt;Please join me in supporting App.net. Even better, please join me in finding ways to buy great software to support the products make our lives more fun and our work more efficient and productive&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This is the path to a successful Web 3.0.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;almost certainly three when they are out of alpha with their OSX application
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Facebook, especially in my opinion
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;and please choose and support FOSS solutions with your time, labor, and/or money
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Follow up to Nesi&#39;s Notes Guest Post: Woonsocket School Funding</title>
      <link>http://www.json.blog/2012/07/follow-up-to-nesis-notes-guest-post-woonsocket-school-funding/</link>
      <pubDate>Thu, 12 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/follow-up-to-nesis-notes-guest-post-woonsocket-school-funding/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.twitter.com/#!/tednesi&#34;&gt;Ted Nesi&lt;/a&gt; was gracious in offering me a &lt;a href=&#34;http://blogs.wpri.com/?p=62059&#34;&gt;guest spot on his blog&lt;/a&gt;, &lt;a href=&#34;http://blogs.wpri.com/category/nesis-notes/&#34;&gt;Nesi&amp;rsquo;s Notes&lt;/a&gt; this week to discuss education funding in Woonsocket. The main conclusions of my post are:&lt;/p&gt;

&lt;p&gt;​1. Woonsocket has not increased local funding for education over the last fifteen years despite massive increases in education expenditures in Rhode Island and nationwide.&lt;/p&gt;

&lt;p&gt;​2. General education aid from the state has rapidly increased over the same period, demonstrating that a lack of sufficient revenue at Woonsocket Public Schools is first, if not exclusively, a local revenue
problem.&lt;/p&gt;

&lt;p&gt;I wanted to provide three additional bits of information on my personal
blog.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;|filename|limitations-of-the-nesis-notes-analysis-and-some-additional-questions.md&#34;&gt;First&lt;/a&gt;, I want to outline some analyses that I have not done that I think are critical to understanding education funding in Woonsocket. I will also describe more completely what conclusions cannot be drawn from
the analysis on Nesi&amp;rsquo;s Notes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;|filename|legal-context-can-woonsocket-successfully-sue-the-state-for-additional-aid.md&#34;&gt;Second&lt;/a&gt;, I want to discuss the legal context of school funding in Rhode Island. This is especially interesting since Pawtucket and Woonsocket are both currently suing the state for additional funds for
the second time. I am going to review what happened the first time these communities brought their fight for education aid to the courthouse and explain why I believe this strategy will fail once again.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;|filename|using-the-common-core-data-on-nces.md&#34;&gt;Third&lt;/a&gt;, I want to provide instructions on precisely how I retrieved the data and created the graphs in that post. I am a firm believer in &amp;ldquo;reproducible research&amp;rdquo;, so I want to be entirely transparent on my data
sources and methods. I also think that too few people are acquainted with the Common Core Data provided by the National Center for Education Statistics that I relied on exclusively for my guest blog. Hopefully these instructions will help more concerned citizens and journalists in Rhode Island use data to back up assertions about local education.&lt;/p&gt;

&lt;p&gt;Please reserve your comments on my original posts for Nesi&amp;rsquo;s Notes. I have disabled comments on this post, because I would like to keep the comments on the original analysis contained in one place. Feel free to
comment on each of the follow up posts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Legal Context: Can Woonsocket Successfully Sue the State for Additional Aid?</title>
      <link>http://www.json.blog/2012/07/legal-context-can-woonsocket-successfully-sue-the-state-for-additional-aid/</link>
      <pubDate>Thu, 12 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/legal-context-can-woonsocket-successfully-sue-the-state-for-additional-aid/</guid>
      <description>&lt;p&gt;My last post ended with an important question, &amp;ldquo;Who is responsible for ensuring students are receiving a certain minimum quality education?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;This is my attempt at answering that question.&lt;/p&gt;

&lt;p&gt;Does the state have a legal obligation to fiscally ensure that Woonsocket students are receiving an equitable, adequate, and meaningful education? &lt;em&gt;&lt;a href=&#34;http://www.oyez.org/cases/1970-1979/1972/1972_71_1332&#34;&gt;San Antonio v. Rodriquez&lt;/a&gt;&lt;/em&gt;, a landmark Supreme Court case decided in 1973 determined that there was no fundamental right to education guaranteed by the U.S. Constitution. Since that decision, advocates for fairer education funding have focused their efforts in state courts arguing over provisions in state constitutions that include some rights to education.&lt;/p&gt;

&lt;p&gt;In Rhode Island, the &lt;a href=&#34;http://www.educationjustice.org/states/rhodeisland.html&#34;&gt;&lt;em&gt;City of Pawtucket v. Sundlun&lt;/em&gt;&lt;/a&gt; in 1995 tested &lt;a href=&#34;http://www.rilin.state.ri.us/RiConstitution/C12.html&#34;&gt;Article XII&lt;/a&gt; of the state constitution which stated &amp;ldquo;it shall be the duty of the general assembly to promote public schools&amp;hellip;&amp;rdquo;. In this case, East Greenwich, Pawtucket, and Woonsocket sued the state claiming that the duty to promote public schools amounted to a guarantee of equitable, adequate education funding from the state, a burden not met by the current General Assembly education aid distribution.&lt;/p&gt;

&lt;p&gt;I am not a legal expert, but I find the &lt;a href=&#34;http://scholar.google.com/scholar_case?case=2372498001429988039&amp;amp;hl=en&amp;amp;as_sdt=2,40&amp;amp;as_vis=1&#34;&gt;conclusions of the Supreme Court&lt;/a&gt; abundantly clear. In &lt;em&gt;Pawtucket&lt;/em&gt;, the court decides to overturn a Superior Court decision which had earlier ruled that the state constitution guaranteed each child, &amp;ldquo;receive an equal, adequate, and meaningful education.&amp;rdquo; *Pawtucket* finds that the General Assembly&amp;rsquo;s responsibility to &amp;ldquo;promote&amp;rdquo; as &amp;ldquo;*it* sees fit&amp;rdquo; (emphasis added in the original decision) is quite narrow; the General Assembly clearly has the power to determine how to &amp;ldquo;promote&amp;rdquo; education, it has historically used that power in a way that relied on local appropriations to education, and the courts do not even have a judicable standard&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;to determine the General Assembly has failed to &amp;ldquo;promote&amp;rdquo; education.&lt;/p&gt;

&lt;p&gt;The current lawsuit asserts two things have dramatically changed since *Pawtucket* that justify a second look and new ruling&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. First, one portion of the state constitution has recently been changed that was used in the prior ruling. The Supreme Court&amp;rsquo;s decision stated:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Moreover, in no measure did the 1986 Constitution alter the plenary and exclusive powers of the General Assembly. In fact, the 1986 Constitution provided that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The general assembly shall continue to exercise the powers it has heretofore exercised, unless prohibited in this Constitution.&amp;rdquo; Art. 6, sec. 10.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Essentially, the judge stated that this section of the state constitution meant that the legislature was retaining the right to exercise its powers as it had historically. In the case of education, this means &amp;ldquo;the power to promote public education through a statutory funding scheme and through reliance on local property taxation,&amp;rdquo; in accordance with the findings in the decision. However, Article 6, section 10 of the state constitution has subsequently been repealed. It is worth repeating what I said previously, &lt;strong&gt;I am not a legal expert&lt;/strong&gt;. However, I find the argument to overturn *Pawtucket* on the basis that the General Assembly is no longer expressly continuing to exercise their power as previously to be weak. My understanding of the *Pawtucket* ruling is that the court had only strengthened importance of historical context in making this decision by leaning on this constitutional provision. The importance of historical context still remains, even without this provision. In the &lt;em&gt;Pawtucket&lt;/em&gt; decision, the &amp;ldquo;exercise of powers it has heretofore exercised&amp;rdquo; is interpreted to mean that unchanged constitutional language reflects unchanged powers. By maintaining the same language in 1986, despite amendments offered that would have more explicitly established a right to education, the General Assembly was, in effect, affirming its intent to continue to &lt;em&gt;promote&lt;/em&gt; education as it had in the past. The plaintiffs in the current case, presumably, will argue that without Article 6, section 10, the General Assembly is allowing the courts to reinterpret even the same language to imply a different set of rights and responsibilities than it has historically. I have to ask, if the General Assembly&amp;rsquo;s intent was to signal that Article XII should now be interpreted as establishing a right to education, why wouldn&amp;rsquo;t they have adopted new, clearer language as was proposed in 1986? Having full awareness of the decision in &lt;em&gt;Pawtucket&lt;/em&gt;, it is hard to see that the General Assembly would signal a change in its power and responsibility to promote education through a repeal of Article 6, section 10. I would assert this change simply shifts some of the burden to the finding that the General Assembly *sees fit* the *promot[ion]* of some judicable standard right to education that is the state&amp;rsquo;s fiscal responsibility.&lt;/p&gt;

&lt;p&gt;This is the critical piece that the plaintiffs will not find. Nowhere has the General Assembly exercised its power to *promote *in this way. In fact, one only has to look at how the General Assembly has acted to establish a judicable right to education to observe precisely how &lt;em&gt;it sees fit&lt;/em&gt;. &lt;a href=&#34;http://www.rilin.state.ri.us/statutes/title16/16-7/16-7-24.htm&#34;&gt;Rhode Island General Law 16-7-24&lt;/a&gt;, titled &amp;ldquo;Minimum appropriation by a community for approved school expenses,&amp;rdquo; is a provision that all school committees are quite familiar with. Here, the General Assembly do establish a judicable standard for education, set by the Board of Regents of Elementary and Secondary Education in regulations known as the &amp;ldquo;basic education program&amp;rdquo;. But where *Pawtucket* fails to establish a &lt;strong&gt;constitutional guarantee for state funding&lt;/strong&gt; in a particular amount for education, Rhode Island &lt;strong&gt;statute&lt;/strong&gt; is quite clear on a minimum standard for &lt;strong&gt;local&lt;/strong&gt; support. The law states that &amp;ldquo;Each community shall appropriate or otherwise make available&amp;hellip; an amount, which together with state education and federal aid&amp;hellip; shall be not less than the costs of the basic program&amp;hellip; The Board of Regents for Elementary and Secondary Education shall adopt regulations for determining the basic education program&amp;hellip;&amp;rdquo; In other words, Rhode Island statute squarely places the burden for meeting the Basic Education Program on cities and towns raising the required revenue. &amp;ldquo;A community that has a local appropriation insufficient to fund the basic education program … shall be required to increase its local appropriation…&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It seems pretty clear to me. While the plaintiffs in the current case will presumably argue that state regulations and laws *do* represent a judicable standard, they will be unable to find where the General Assembly, through action, has affirmed that it is the role of the state aid to meet this standard. Instead, the law directly states that &lt;strong&gt;local appropriations&lt;/strong&gt; are to be increased if the Basic Education Program cannot be met. I cannot imagine that the Supreme Court would exercise its power to assert that the General Assembly&amp;rsquo;s inaction implies more about the purpose of &lt;em&gt;unchanged&lt;/em&gt; constitutional language than the General Assembly&amp;rsquo;s actions.&lt;/p&gt;

&lt;p&gt;In summary, although the city is again suing the state for additional education aid, it is clear in the last 15 years that the state has substantially increased its support for Woonsocket Schools&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Furthermore, previous Rhode Island Supreme Court decisions and Rhode Island law clearly places the burden of adequate school funding squarely on the shoulders of cities and towns, not the General Assembly. In my view, the changes in education law and policy since &lt;em&gt;Pawtucket&lt;/em&gt; do not imply a change that would impact the court&amp;rsquo;s ruling.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is the second post of a three-part follow up on my &lt;a href=&#34;http://blogs.wpri.com/category/nesis-notes/&#34;&gt;guest post&lt;/a&gt; for Nesi&amp;rsquo;s Notes. Parts I and III can be found &lt;a href=&#34;http://blog.jasonpbecker.com/?p=224&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;meaning measurable and enforceable by court room activities
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Note: I have not read the complaint as I probably should have for this post. I ran out of time. However, I feel fairly certain from press coverage that I am correctly stating their main points
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;See my post on Nesi&amp;rsquo;s Notes
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Limitations of the Nesi&#39;s Notes Analysis and Some Additional Questions</title>
      <link>http://www.json.blog/2012/07/limitations-of-the-nesis-notes-analysis-and-some-additional-questions/</link>
      <pubDate>Thu, 12 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/limitations-of-the-nesis-notes-analysis-and-some-additional-questions/</guid>
      <description>&lt;p&gt;There are several questions that come to mind when looking over my analysis on &lt;a href=&#34;http://blogs.wpri.com/category/nesis-notes/&#34;&gt;Nesi&amp;rsquo;s Notes&lt;/a&gt;. The first thing I wondered was whether or not Woonsocket had raised local revenues by similar amounts to other communities but had chosen to spend this money on other municipal services. Ideally, I would use a chart that showed local education revenues compared to all other local revenues over the last 15 years by city in Rhode Island. Unfortunately, &lt;a href=&#34;http://www.muni-info.ri.gov/&#34;&gt;Municipal Finance&lt;/a&gt; does not separate local, state, and federal revenue sources in the &lt;a href=&#34;http://www.muni-info.ri.gov/finances/municipal_budget_survey.php&#34;&gt;Municipal Budget Survey&lt;/a&gt; so it is hard to know&lt;em&gt;how&lt;/em&gt; communities have funded different services. I am sure with a bit of finagling, I could come up with a fairly good guess as to whether or not Woonsocket has simply chosen to fund other municipal services with its taxes, but quite frankly it is not precise enough to make me feel like its worth the exercise of extracting data from PDF tables. I hope someone else will take up some form of this analysis, possibly by requesting the breakdowns from Municipal Finance.&lt;/p&gt;

&lt;p&gt;Another consideration is whether there is any truth to Woonsocket&amp;rsquo;s claims that it simply does not have the ability to generate enough local revenue for their schools. I am skeptical on this claim. Three pieces of evidence suggest to me that this may not be true.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The magnitude of the shortfall between the rest of the state and Woonsocket over the last 15 years when it comes to local education revenue&lt;/strong&gt;. On its face, I don&amp;rsquo;t find it credible that Woonsocket&amp;rsquo;s tax base is so weak that it could not increase local revenues for schools even at the rate of inflation. Not increasing local revenue for schools seems to leave only two possibilities: 1) local revenues in general were not increased, meaning Woonsocket would have to argue that its taxation in FY95 was so high relative to everyone else that it took nearly 15 years for the rest of the state to catch up, hence no additional revenues; or 2) Woonsocket did raise local revenues, and chose to spend the money elsewhere. Had Woonsocket&amp;rsquo;s local education aid risen 65-75% versus a state average of around 100%, I probably would not have even written my post on Nesi&amp;rsquo;s Notes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://twitter.com/#!/candrewmorse&#34;&gt;Andrew Morse&lt;/a&gt;&amp;lsquo;s analysis presented on &lt;a href=&#34;http://www.anchorrising.com/barnacles/013927.html&#34;&gt;Anchor Rising&lt;/a&gt;&lt;/strong&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Woonsocket appears to be on the low to typical end of revenues as a proportion of non-poverty income. It does not seem that they are anywhere near the &amp;ldquo;most&amp;rdquo; taxed city or town by this measure. I am not an expert on tax policy, but this measure seems fairly straightforward, fair, and informative.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The mammoth proportions of Woonsocket&amp;rsquo;s budget being spent on pensions (through debt service) and other post-employment benefits&lt;/strong&gt;. [A full 15% or so of Woonsocket&amp;rsquo;s local revenues are being spent in these areas][]. This suggests to me misappropriation and poor planning has led to the erosion of local support for schools, not a lack of revenue generating capacity. If this truly is the case, then Woonsocket residents are really in trouble. Their leaders have managed to generate all of the high costs and high taxes experienced in Rhode Island without providing the quality of service that should be expected given those investments.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of course, I failed to offer any recommendation for remedy in the Nesi&amp;rsquo;s Note post. How should Woonsocket schools become &amp;ldquo;whole&amp;rdquo; again? How can this possibly be accomplished in the context of a city on the brink of financial failure? Who has the legal responsibility to ensure that Woonsocket&amp;rsquo;s children get the education they deserve? I have no answers on the first two points. However, in the next section of this post I hope answer the last question, which is also the subject of a law suit filed by Pawtucket and Woonsocket against the state of Rhode Island.&lt;/p&gt;

&lt;p&gt;Who is responsible for ensuring students are receiving a certain minimum quality education?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is the first of a three-part follow up on my &lt;a href=&#34;http://blogs.wpri.com/category/nesis-notes/&#34;&gt;guest post&lt;/a&gt; for Nesi&amp;rsquo;s Notes. Parts II and III can be found
&lt;a href=&#34;http://blog.jasonpbecker.com/?p=224&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;[fourth lowest revenues from residential taxes as a proportion of
community wealth]: &lt;a href=&#34;http://www.anchorrising.com/barnacles/014501.html&#34;&gt;http://www.anchorrising.com/barnacles/014501.html&lt;/a&gt;
[A full 15% or so of Woonsocket&amp;rsquo;s local revenues are being spent in
these areas]: &lt;a href=&#34;http://blogs.reuters.com/muniland/2012/06/20/conservative-ideologues-arent-bankrupting-rhode-island/&#34;&gt;http://blogs.reuters.com/muniland/2012/06/20/conservative-ideologues-arent-bankrupting-rhode-island/&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Andrew has been writing quite a bit about Woonsocket. For his most recent post, Andrew demonstrates Woonsocket has the [fourth lowest revenues from residential taxes as a proportion of community wealth][]. A few things I&amp;rsquo;d like to point out on that post. First, I think Andrew was right to adjust for poverty in previous posts in a way he was unable to due to the structure of the new data. I support progressive taxation, so I don&amp;rsquo;t believe that it is fair to say that we should expect the same percentage of income tax from poorer communities that we do from wealthier ones. I also think that commercial taxes are very important revenue sources. I don&amp;rsquo;t think they should be universally dismissed when used as a substitute from residential revenues. There are times where marginally the greatest benefit can be had by lowering residents&amp;rsquo; taxes. However, I do think that commercial tax should not be used as a substitute when there isn&amp;rsquo;t enough revenue in the pie. In Woonsocket&amp;rsquo;s case, it seems pretty clear they needed both the residential and commercial taxes to have sufficient revenues.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using the Common Core Data on NCES</title>
      <link>http://www.json.blog/2012/07/using-the-common-core-data-on-nces/</link>
      <pubDate>Thu, 12 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/using-the-common-core-data-on-nces/</guid>
      <description>&lt;p&gt;My analysis on &lt;a href=&#34;http://blogs.wpri.com/?p=62059&#34;&gt;Nesi&amp;rsquo;s Notes&lt;/a&gt; depended entirely on the &lt;a href=&#34;http://nces.ed.gov/ccd/bat&#34;&gt;National Center for Education Statistics&amp;rsquo; Common Core Data&lt;/a&gt;. The per pupil amounts reported to NCES may look a bit different from state sources of this information. There are several explanations of this. First, the enrollment counts used to generate per pupil amounts are based on an October 1st headcount. In Rhode Island, we use something called &amp;ldquo;average daily membership&amp;rdquo; (ADM) as the denominator and not a headcount. The ADM of a district is calculated by taking all the students who attended the district at any point in the year and adding up the number of school days they were enrolled for. The total membership (i.e. all the student*days, for those who like to think about this in units) is divided by the number of school days per year, almost always 180 (so student*days / days/year = students/year). Additionally, NCES does not record the final three digits on most financial data. These rounding issues will also make the per pupil data seem different from state reports.&lt;/p&gt;

&lt;p&gt;I wanted to use the NCES to make sure that the data in my post was easily reproducible by any member of the public. I also thought using NCES would serve as a great learning opportunity for the wonks and nerds out there who never even realized how much rich data about schools and school finance are available through the federal government. That being said, I do believe that the state reported numbers are far more accurate than those available from the federal government. That is not to say that the federal data is bad. On the contrary, that data is substantially vetted and validated and is very useful for research. My concern was only that some of the tiny differences in the NCES data that deviated from what I would consider to be ideal data might reach the level where they affected the validity of the conclusions I wanted to draw.&lt;/p&gt;

&lt;p&gt;Although I was writing as a private citizen without the support of the Rhode Island Department of Education, I did use my access to RIDE data to ensure that differences in the federal reports were not significant enough to call into question my analysis. I found that both the direction and magnitude of all the trends that I describe in the Nesi&amp;rsquo;s Notes post held up with the state data. While all of that information is publicly available, it is less easily accessible than NCES data and doesn&amp;rsquo;t provide the same opportunity for analysis outside of financial data. For these reasons, I decided to stick with NCES.&lt;/p&gt;

&lt;p&gt;So how do you reproduce the data I used?&lt;/p&gt;

&lt;p&gt;First, go to  the &lt;a href=&#34;http://nces.ed.gov/ccd/bat&#34;&gt;NCES Common Core Data Build a Table&lt;/a&gt; site. On the drop down, select &amp;ldquo;District&amp;rdquo; as the row variable and select the last fifteen years excluding 2009-10 (since there is no current financial data available for that year).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces1.png&#34; alt=&#34;1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After clicking next, hit &amp;ldquo;I Agree&amp;rdquo; on the pop-up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces2.png&#34; alt=&#34;2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now select &amp;ldquo;Finance Per Pupil Ratios&amp;rdquo; for the first column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces3.png&#34; alt=&#34;3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Click the green arrow that selects all years for local sources per
student and state sources per student.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces4.png&#34; alt=&#34;4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Click &amp;ldquo;Next&amp;gt;&amp;gt;&amp;rdquo; on the top right. Now select only RI-Rhode Island for
your row variable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces5.png&#34; alt=&#34;5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, click view table to see the results. I recommend downloading
the Test (.csv) file to work with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/nces6.png&#34; alt=&#34;6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And finally, here&amp;rsquo;s the R code to reshape/rejigger the data I used and
produce the graphics from the Nesi&amp;rsquo;s Notes post.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Using NCES data to analyze education finances to Woonsocket over 15 years.
## Initialize required packages
require(plyr)
require(reshape2)
require(ggplot2)
require(scales)
## Best to ignore this function-- it&#39;s mostly magic to me too. Essentially,
## multiplot takes in a bunch of plots and then puts them into one image
## arranging them by columns equal to a paramter cols. Credit to:
## http://wiki.stdout.org/rcookbook/Graphs/Multiple%20graphs%20on%20one%20page%20(    ggplot2)/
multiplot &amp;lt;- function(..., plotlist=NULL, cols) {
  require(grid)
  # Make a list from the ... arguments and plotlist
  plots &amp;lt;- c(list(...), plotlist)
  numPlots = length(plots)
  # Make the panel
  plotCols = cols                          # Number of columns of plots
  plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from #    of cols
  # Set up the page
  grid.newpage()
  pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
  vplayout &amp;lt;- function(x, y)
    viewport(layout.pos.row = x, layout.pos.col = y)
  # Make each plot, in the correct location
  for (i in 1:numPlots) {
    curRow = ceiling(i/plotCols)
    curCol = (i-1) %% plotCols + 1
    print(plots[[i]], vp = vplayout(curRow, curCol ))
  }
}
## Load data from the modified CSV. I made the following changes from the NCES
## downloaded file: 1) I removed all of the description header so that row one
## of the CSV is the attribute names; 2) I pasted the transposed state values
## to the final observation so that I have a state observation row analogous to
## the other LEA rows.

raw_data &amp;lt;- read.csv(&#39;rawdata.csv&#39;)
## Change name of first column to make things easier for later.
names(raw_data)[1] &amp;lt;- c(&#39;distname&#39;)
## Creating Time Series Data for each community of interest.
## I&#39;m going to use a custom function to automate the steps required to create
## district level data in a time series.

create_ts &amp;lt;- function(name){
  # First create a column vector with the local funding
  # A few things to note: First, t() is the transpose function and helps to
  # make my &amp;quot;wide&amp;quot; data (lots of columns) &amp;quot;long&amp;quot; (lots of rows). Second, R
  # has a funny behavior that is very covenient for data anaylsts. It performs
  # many common mathematical operations element-wise, so the simple division
  # of two vectors below actually divides element by element through the
  # vector, e.g. column 17 is divided by column 2 to provide the first element
  # in the resulting vector. This makes calculating per pupil amounts very
  # convenient.
  local &amp;lt;- t(subset(raw_data,distname==name)[,c(17:31)]/
             subset(raw_data,distname==name)[,c(2:16)])
  # Performing the same operation for state per pupil amounts.
  state &amp;lt;- t(subset(raw_data,distname==name)[,c(32:46)]/
             subset(raw_data,distname==name)[,c(2:16)])
  # Putting state and local data together and getting rid of the nasty
  # attribute names from NCES by just naming the rows with a sequence
  # of integers.
  results &amp;lt;- data.frame(local,state,row.names=seq(1,15,1))
  # Naming my two attributes
  names(results) &amp;lt;- c(&#39;local&#39;,&#39;state&#39;)
  # Generating the year attribute
  results[[&#39;year&#39;]] &amp;lt;- seq(1995, 2009, 1)
  # This command is a bit funky, but basically it makes my data as long as
  # possible so that each line has an ID (year in this case) and one value
  # (the dollars in this case). I also have a label that describes that value,
  # which is local or state.
  results &amp;lt;- melt(results, id.vars=&#39;year&#39;)
  # Returning my &amp;quot;results&amp;quot; object
  results
}
## Create the Woonsocket data-- note that R is case sensitive so I must use all
## capitals to match the NCES convention.
woonsocket &amp;lt;- create_ts(&#39;WOONSOCKET&#39;)
pawtucket &amp;lt;- create_ts(&#39;PAWTUCKET&#39;)
providence &amp;lt;- create_ts(&#39;PROVIDENCE&#39;)
westwarwick &amp;lt;- create_ts(&#39;WEST WARWICK&#39;)
state &amp;lt;- create_ts(&#39;STATE&#39;)

## Developing a plot of JUST local revenues for the selected communities
## First I create a percentage change data frame. I think that looking at
## percent change overtime is generally more fair. While the nominal dollar
## changes are revealing, my analysis is drawing attention to the trend rather
## than the initial values.

## First, I pull out just the local dollars.
perwoonlocal &amp;lt;- subset(woonsocket,variable==&#39;local&#39;)
## Now I modify the value to be divided by the starting value - 100%
perwoonlocal[[&#39;value&#39;]] &amp;lt;- with(perwoonlocal, (value/value[1])-1)
## A little renaming for the combining step later
names(perwoonlocal) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perwoonlocal[[&#39;disname&#39;]]&amp;lt;-&#39;Woonsocket&#39;

## I repeat this procedure for all the districts of interest.
perpawlocal &amp;lt;- subset(pawtucket,variable==&#39;local&#39;)
perpawlocal[[&#39;value&#39;]] &amp;lt;- with(perpawlocal, (value/value[1])-1)
names(perpawlocal) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
awlocal[[&#39;disname&#39;]]&amp;lt;-&#39;Pawtucket&#39;

perprolocal &amp;lt;- subset(providence,variable==&#39;local&#39;)
perprolocal[[&#39;value&#39;]] &amp;lt;- with(perprolocal, (value/value[1])-1)
names(perprolocal) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perprolocal[[&#39;disname&#39;]]&amp;lt;-&#39;Providence&#39;

perwwlocal &amp;lt;- subset(westwarwick, variable==&#39;local&#39;)
perwwlocal[[&#39;value&#39;]] &amp;lt;- with(perwwlocal, (value/value[1])-1)
names(perwwlocal) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perwwlocal[[&#39;disname&#39;]]&amp;lt;-&#39;West Warwick&#39;

perrilocal &amp;lt;- subset(state,variable==&#39;local&#39;)
perrilocal[[&#39;value&#39;]] &amp;lt;- with(perrilocal, (value/value[1])-1)
names(perrilocal) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perrilocal[[&#39;disname&#39;]]&amp;lt;-&#39;State Average&#39;

## The same process can be used for state data
perwoonstate &amp;lt;- subset(woonsocket,variable==&#39;state&#39;)
## Now I modify the value to be divided by the starting value - 100%
perwoonstate[[&#39;value&#39;]] &amp;lt;- with(perwoonstate, (value/value[1])-1)
## A little renaming for the combining step later
names(perwoonstate) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perwoonstate[[&#39;disname&#39;]]&amp;lt;-&#39;Woonsocket&#39;

## I repeat this procedure for all the districts of interest.
perpawstate &amp;lt;- subset(pawtucket,variable==&#39;state&#39;)
perpawstate[[&#39;value&#39;]] &amp;lt;- with(perpawstate, (value/value[1])-1)
names(perpawstate) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perpawstate[[&#39;disname&#39;]]&amp;lt;-&#39;Pawtucket&#39;

perprostate &amp;lt;- subset(providence,variable==&#39;state&#39;)
perprostate[[&#39;value&#39;]] &amp;lt;- with(perprostate, (value/value[1])-1)
names(perprostate) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perprostate[[&#39;disname&#39;]]&amp;lt;-&#39;Providence&#39;

perwwstate &amp;lt;- subset(westwarwick, variable==&#39;state&#39;)
perwwstate[[&#39;value&#39;]] &amp;lt;- with(perwwstate, (value/value[1])-1)
names(perwwstate) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perwwstate[[&#39;disname&#39;]]&amp;lt;-&#39;West Warwick&#39;

perristate &amp;lt;- subset(state,variable==&#39;state&#39;)
perristate[[&#39;value&#39;]] &amp;lt;- with(perristate, (value/value[1])-1)
names(perristate) &amp;lt;-c(&#39;year&#39;,&#39;disname&#39;,&#39;value&#39;)
perristate[[&#39;disname&#39;]]&amp;lt;-&#39;State Average&#39;

## Pull together the data sets for the overall picture.
localfunding &amp;lt;- rbind(perwoonlocal, perpawlocal,perprolocal,perwwlocal,perrilocal)
statefunding &amp;lt;- rbind(perwoonstate, perpawstate,perprostate,perwwstate,perristate)

## A little ggplot2 line plot magic...
localperplot &amp;lt;- ggplot(localfunding,aes(year, value, color=disname)) +
                geom_line() +
                geom_text(data=subset(localfunding, year==2009),
                          mapping=aes(year,value,
                                      label=paste(100*round(value,3),&#39;%&#39;,sep=&#39;&#39;)),
                          vjust=-.4) +
                scale_y_continuous(&#39;Percent Change from FY1995&#39;,
                                   label=percent) +
                scale_x_continuous(&#39;Year&#39;) +
                opts(title=&#39;Percent Change in Local Per Pupil Revenue, FY1995-    FY2009&#39;) +
                opts(plot.title=theme_text(size=16,face=&#39;bold&#39;)) +
                opts(legend.title=theme_blank()) +
                opts(legend.position=c(.08,.82))
stateperplot &amp;lt;- ggplot(statefunding,aes(year, value, color=disname)) +
                geom_line() +
                geom_text(data=subset(statefunding, year==2008 | year==2009),
                          mapping=aes(year,value,
                          label=paste(100*round(value,3),&#39;%&#39;,sep=&#39;&#39;)),
                          vjust=-.4) +
                scale_y_continuous(&#39;Percent Change from FY1995&#39;,
                                   label=percent) +
                scale_x_continuous(&#39;Year&#39;) +
                opts(title=&#39;Percent Change in State Per Pupil Revenue, FY1995-    FY2009&#39;) +
                opts(plot.title=theme_text(size=16,face=&#39;bold&#39;)) +
                opts(legend.title=theme_blank()) +
                opts(legend.position=c(.08,.82))
ggsave(&#39;localperplot.png&#39;,localperplot,width=10,height=8,units=&#39;in&#39;,dpi=72)
ggsave(&#39;stateperplot.png&#39;,stateperplot,width=10,height=8,units=&#39;in&#39;,dpi=72)
    
## Proportion of Aid
proportion &amp;lt;- function(data){
  # This reshapes the data so that there is a year, local, and state column.
  # The mean function has no purpose, because this data is unique by year
  # variable combinations.
  prop &amp;lt;- dcast(data,year~variable,mean)
  # Adding local and state get our total non-federal dollars
  prop[[&#39;total&#39;]] &amp;lt;- apply(prop[,2:3],1,sum)
  prop[[&#39;perlocal&#39;]] &amp;lt;- with(prop, local/total)
  prop
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Prepare new data frames for proportion graphs

propwoon &amp;lt;- as.data.frame(c(disname=&#39;Woonsocket&#39;,
                            proportion(woonsocket)))
proppaw &amp;lt;- as.data.frame(c(disname=&#39;Pawtucket&#39;,
                           proportion(pawtucket)))
propprov &amp;lt;- as.data.frame(c(disname=&#39;Providence&#39;,
                            proportion(providence)))
propww &amp;lt;- as.data.frame(c(disname=&#39;West Warwick&#39;,
                          proportion(westwarwick)))
propri &amp;lt;- as.data.frame(c(disname=&#39;State Average&#39;,
                          proportion(state)))

## Note, I could have called proportion() inside of the rbind(), but I wanted
## my code to be clearer and felt there may be some use for the independent
## proportion data frames in further analysis. Sometimes more lines of code
## and more objects is easier to maintain and more flexible for exploratory,
## non-production code. This is especially true when handling such small
## data sets that there is no impact on performance.

locprop &amp;lt;- rbind(propwoon, proppaw,propprov,propww,propri)

## Some ggplot2 magic time!

localpropplot &amp;lt;- ggplot(locprop,aes(year, perlocal, color=disname)) +
                 geom_line() +
                 geom_text(data=subset(locprop, year==1995 | year==2008 |     year==2009),
                           mapping=aes(year,perlocal,
                           label=paste(100*round(perlocal,3),&#39;%&#39;,sep=&#39;&#39;)),
                           vjust=-.4) +
                 scale_y_continuous(&#39;Percent Change from FY1995&#39;,
                                     label=percent) +
                 scale_x_continuous(&#39;Year&#39;) +
                 opts(title=&#39;Percent Change in Local Proportion of Per Pupil    Revenue\n Excluding Federal Funding, FY1995-FY2009&#39;) +
                 opts(plot.title=theme_text(size=16,face=&#39;bold&#39;)) +
                 opts(legend.title=theme_blank()) +
                 opts(legend.position=c(.9,.65))
ggsave(&#39;localpropplot.png&#39;,localpropplot,width=10,height=8,units=&#39;in&#39;,dpi=72)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;This post is the third of a three-part follow up on my &lt;a href=&#34;http://blogs.wpri.com/category/nesis-notes/&#34;&gt;guest post&lt;/a&gt; for Nesi&amp;rsquo;s Notes. Parts I and II can be found &lt;a href=&#34;|filename|follow-up-to-nesis-notes-guest-post-woonsocket-school-funding.md&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ranked Likert-Scale Visualization</title>
      <link>http://www.json.blog/2012/07/ranked-likert-scale-visualization/</link>
      <pubDate>Tue, 10 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/ranked-likert-scale-visualization/</guid>
      <description>

&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;

&lt;p&gt;See below for more information now that Ethan Brown has &lt;a href=&#34;http://statisfactions.com/2012/improved-net-stacked-distribution-graphs-via-ggplot2-trickery/&#34;&gt;weighed in with some great code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;http://blog.ouseful.info/2012/07/09/fumblings-with-ranked-likert-scale-data-in-r/&#34;&gt;recent post&lt;/a&gt; I came across on &lt;a href=&#34;http://www.r-bloggers.com/&#34;&gt;r-bloggers&lt;/a&gt; asked for input on visualizing ranked &lt;a href=&#34;http://en.wikipedia.org/wiki/Likert_scale&#34;&gt;Likert-scale data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I happen to be working on a substantial project using very similarly structured data so I thought I would share some code. In my efforts to be generic as possible, I decided to generate some fake data from scratch. As I peeled away the layers of context-specific aspects of my nearing-production level code, I ran into all kinds of trouble. So I apologize for the somewhat sloppy and unfinished code&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/rankedlikert.png&#34; alt=&#34;Greaterthan&#34; title=&#34;Example of a Net Stacked Likert&#34; /&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:netstacked&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:netstacked&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;My preferred method for visualizing Likert-scale data from surveys is using &lt;a href=&#34;http://www.organizationview.com/net-stacked-distribution-a-better-way-to-visualize-likert-data&#34;&gt;net stacked distribution graphs&lt;/a&gt;. There are two major benefits of these kinds of graphs. First, they immediately draw attention to &lt;em&gt;how strongly&lt;/em&gt; respondents feel about a question, particularly when multiple questions are visualized at once. The total width of any bar is equal to the total number of responded who had a non-neutral answer. Second, these graphs make it very easy to distinguish between positive and negative responses. In some cases, it is critical to view the distribution of data to visualize the differences in responses to one question or another. However, most of the time it is informative enough to simply know how positive or negative responses are. I find this is particularly true with 3, 4, and 5-point Likert scales, the most common I come across in education research.&lt;/p&gt;

&lt;p&gt;Anyway, without further ado, some starter code for producing net stacked distribution graphs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;require(ggplot2)
require(scales)
require(plyr)
dataSet &amp;lt;- data.frame(
  q1=as.ordered(round(runif(1500, 1, 15) + runif(1500,1,15))),
  q2=as.ordered(round(runif(1500, 1, 15) + runif(1500,1,15))))
dataSet[[&#39;q1&#39;]] &amp;lt;- with(dataSet, ifelse(q1&amp;lt;7,1,
                                       ifelse(q1&amp;gt;=7 &amp;amp; q1&amp;lt;13,2,
                                             ifelse(q1&amp;gt;=13 &amp;amp; q1&amp;lt;20,3,
                                                   ifelse(q1&amp;gt;=20 &amp;amp; q1&amp;lt;26,4,5)))))
dataSet[[&#39;q2&#39;]] &amp;lt;- with(dataSet, ifelse(q2&amp;lt;3,1,
                                       ifelse(q2&amp;gt;=3 &amp;amp; q2&amp;lt;14,2,
                                             ifelse(q2&amp;gt;=14 &amp;amp; q2&amp;lt;26,3,
                                                   ifelse(q2&amp;gt;=26 &amp;amp; q2&amp;lt;28,4,5)))))
dataSet[[&#39;q1&#39;]] &amp;lt;- as.ordered(dataSet[[&#39;q1&#39;]])
dataSet[[&#39;q2&#39;]] &amp;lt;- as.ordered(dataSet[[&#39;q2&#39;]])
levels(dataSet[[&#39;q1&#39;]]) &amp;lt;- c(&#39;Strongly Disagree&#39;,
                             &#39;Disagree&#39;,
                             &#39;Neither Agree or Disagree&#39;,
                             &#39;Agree&#39;,
                             &#39;Strongly Agree&#39;)
levels(dataSet[[&#39;q2&#39;]]) &amp;lt;- c(&#39;Strongly Disagree&#39;,
                             &#39;Disagree&#39;,
                             &#39;Neither Agree or Disagree&#39;,
                             &#39;Agree&#39;,
                             &#39;Strongly Agree&#39;)
# Convert the integer levels to have meaning.
q1Proportions &amp;lt;- data.frame(Name=&#39;q1&#39;, prop.table(table(dataSet[[&#39;q1&#39;]])))
q2Proportions &amp;lt;- data.frame(Name=&#39;q2&#39;, prop.table(table(dataSet[[&#39;q2&#39;]])))
# Produces a data frame with the proportions of respondents in each level.

# ggplot2 function for graphs
visualize &amp;lt;- function(data,
                      responses=c(&#39;Strongly Disagree&#39;,
                                  &#39;Disagree&#39;,
                                  &#39;Neither Agree or Disagree&#39;,
                                  &#39;Agree&#39;,
                                  &#39;Strongly Agree&#39;),
                      desc=&#39;Title&#39;,
                      rm.neutral=TRUE){
  # This function will create net stacked distribution graphs. These are
  # a particularly useful visualization of Likert data when there is a neutral
  # option available and/or when emphasizing the difference between positive and
  # negative responses is a goal.
  # Args:
  #   data: This is a dataframe with percentages labeled with responses.
  #   responses: This is a vector with the response labels.
  #   desc: This is the title of the output ggplot2 graphic, typically the
  #         question text.
  #   rm.neutral: This is a single element logical vector that determines if the
  #               neutral response should be removed from the data. The default
  #               value is TRUE.
  for(i in 1:ceiling(length(responses)/2)-1){
      # This loop negates all the negative, non-neutral responses regardless of
      # the number of possible responses. This will center the non-neutral
      # responses around 0.
      data[i,3] &amp;lt;- -data[i,3]
  }
  if(rm.neutral==T){
    data &amp;lt;- ddply(data,.(Name), function(x) x[-(ceiling(length(responses)/2)),])
    responses &amp;lt;- responses[-(ceiling(length(responses)/2))]
  }
  else{

  }
  print(data)
  stackedchart &amp;lt;- ggplot() +
                  layer(data=data[1:2,],
                        mapping=aes(Name,Freq,fill=Var1,order=-as.numeric(Var1)),
                        geom=&#39;bar&#39;,
                        position=&#39;stack&#39;,
                        stat=&#39;identity&#39;)
  stackedchart &amp;lt;- stackedchart +
                  layer(data=data[3:4,],
                        mapping=aes(Name,Freq,fill=Var1,order=Var1),
                        geom=&#39;bar&#39;,
                        position=&#39;stack&#39;,
                        stat=&#39;identity&#39;)
  stackedchart &amp;lt;- stackedchart +
                  geom_hline(yintercept=0) +
                  opts(legend.title=theme_blank()) +
                  opts(axis.title.x=theme_blank()) +
                  opts(axis.title.y=theme_blank()) +
                  opts(title=desc) +
                  scale_y_continuous(labels=percent,
                                     limits=c(-1,1),
                                     breaks=seq(-1,1,.2)) +
                  scale_fill_manual(limits=responses,
                                    values=c(&#39;#AA1111&#39;,
                                             &#39;#BB6666&#39;,
                                             &#39;#66BB66&#39;,
                                             &#39;#11AA11&#39;)) +
                  coord_flip()
  stackedchart
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the results of all that?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/fakeexample.png&#34; alt=&#34;I wish it were prettier, but this is where I got.&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;update-1&#34;&gt;UPDATE:&lt;/h2&gt;

&lt;p&gt;So now that Ethan has weighed in with his code I thought I would add some things to make this post better reflect my production code. Below, I have included my comment on his blog as well as an actual copy of my current production code (which definitely is not sufficiently refactored for easy use across multiple projects). Again, excuse what I consider to be incomplete work on my part. I do intend on refactoring this code and eventually including it in my broader set of custom functions available across all of my projects. I suspect along that path that I will be &amp;ldquo;stealing&amp;rdquo; some of Ethan&amp;rsquo;s ideas.&lt;/p&gt;

&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;

&lt;p&gt;Hi Ethan! Super excited to see this post. This is exactly why I put up my code&amp;ndash; so others could run with it. There are a few things that you do here that I actually already had implemented into my code and removed in an attempt to be more neutral to scale that I really like.&lt;/p&gt;

&lt;p&gt;For starters, in my actual production code I also separate out the positive and negative responses. In my code, I have a parameter called &lt;code&gt;scaleName&lt;/code&gt; that allows me to switch between all of the scales that are available in my survey data. This includes Strongly Disagree to Strongly Agree (&lt;code&gt;scaleName==&#39;sdsa&#39;&lt;/code&gt;), Never -&amp;gt; Always (&lt;code&gt;scaleName==&#39;sdsa&#39;&lt;/code&gt;) and even simple yes/no (&lt;code&gt;scaleName==&#39;ny&#39;&lt;/code&gt;). This is not ideal because it does require 1) knowing all possible scales and including some work in the function to treat them differently 2) including an additional parameter. However, because I use this work to analyze just a few surveys, the upfront work of including this as a parameter has made this very flexible in dealing with multiple scales. As a result, I do not need to require that the columns are ordered in any particular way, just that the titles match existing scales. So I have a long set of if elseif statements that look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt; if(scaleName==&#39;sdsa&#39;){
 scale &amp;lt;- c(&#39;Strongly Disagree&#39;,&#39;Disagree&#39;,&#39;Agree&#39;,&#39;Strongly Agree&#39;)
 pos &amp;lt;- c(&#39;Agree&#39;,&#39;Strongly Agree&#39;)
 neg &amp;lt;- c(&#39;Strongly Disagree&#39;,&#39;Disagree&#39;)
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is actually really helpful for producing negative values and including some scales in my function which do not have values that are negative (so that it can be used for general stacked charts instead of just net-stacked):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(length(neg)&amp;gt;0){
quest[,names(quest) %in% c(neg)] &amp;lt;- -(quest[,names(quest) %in% c(neg)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Recall that quest is what I call the dataframe and is equivalent to x in your code)&lt;/p&gt;

&lt;p&gt;Another neat trick that I have instituted is having dynamic x-axis limits rather than always going from -100 to 100. I generally like to keep my scales representing the full logical range of data (0 - 100 for percentages, etc) so I might consider this a manipulation. However, after getting many charts with stubby centers, I found I was not really seeing sufficient variation by sticking to my -100 to 100 setup. So I added this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pos_lims =0)+1))[1,]),
sum(subset(quest,select=c(which(quest[2,-1]&amp;gt;=0)+1))[2,])))
neg_lims &amp;lt;- max(abs(c(sum(subset(quest, 
                                 select=c(which(quest[1,-1]&amp;lt;=0) + 1))[1,]),
sum(subset(quest,select=c(which(quest[2,-1]&amp;lt;=0)+1))[2,]))))
x_axis_lims &amp;lt;- max(pos_lims,neg_lims)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which helps to determine the value furthest from 0 in either direction across the data frame (I have to admit, this code looks a bit like magic reading it back. My comments actually are quite helpful:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# pos_lims and neg_lims subset each row of the data based on sign, then
# sums the values that remain (gettting the total positive or negative
# percentage for each row). Then, the max of the rows is saved as a candidate
# for the magnitude of the axis.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make this more generalizable (my production code always compares two bars at once) , it would be fairly trivial to loop over all the rows (or use the apply functions which I&amp;rsquo;m still trying to get a hang of).&lt;/p&gt;

&lt;p&gt;I then pad the &lt;code&gt;x_limits&lt;/code&gt; value by some percent inside the &lt;code&gt;limits&lt;/code&gt; attribute.&lt;/p&gt;

&lt;p&gt;In my production code I also have the &lt;code&gt;scale_fill_manual&lt;/code&gt; attribute
added separately to the ggplot object. However, rather than add this
after the fact like at the point of rendering, I include this in my
function again set by &lt;code&gt;scaleName&lt;/code&gt;. However, I think the best organization
is probably to have a separate function that makes it easy to select the
color scheme you want and apply it so that your final call could be
something like &lt;code&gt;colorNetStacked(net_stacked(x), &#39;blues&#39;)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;My actual final return looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    return(stackedchart + scale_fill_manual(limits=scale,
    values=colors) +
    coord_flip())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where colors is set by a line like: &lt;code&gt;colors &amp;lt;- brewer.pal(name=&#39;Blues&#39;,n=7)[3:7]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Seriously though, I am super excited you found my post and thought it was useful and improved what I presented!&lt;/p&gt;

&lt;h3 id=&#34;current-production-code&#34;&gt;Current production code:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;visualize &amp;lt;- function(quest,scaleName=&#39;sdsa&#39;,desc){
  # Produces the main net-stacked Likert graphic used for survey data in the
  # diagnostic tool
  # Args:
  #  quest: data.frame from pull() or pullByLevel() output
  #  scaleName: string code for the type of scale that is used for the question.
  #  desc: string for the title that will be displayed on the graphic.
  # Returns:
  # Net-Stacked Likert chart with a bar/row for each row in quest. Most scales
  # center around 0 with a distinct positive and negative set of responses.
  # The graphs are custom colored based on what best reflects the scale.
  # The x-axis limits are set dynamically based on a 10% buffer of the largest
  # magnitude to either the positive or negative responses.

  if(scaleName==&#39;sdsa&#39;){
    scale &amp;lt;- c(&#39;Strongly Disagree&#39;,&#39;Disagree&#39;,&#39;Agree&#39;,&#39;Strongly Agree&#39;)
    pos   &amp;lt;- c(&#39;Agree&#39;,&#39;Strongly Agree&#39;)
    neg   &amp;lt;- c(&#39;Strongly Disagree&#39;,&#39;Disagree&#39;)
  }else if(scaleName==&#39;da&#39;){
    scale &amp;lt;- c(&#39;Disagree&#39;,&#39;Agree&#39;)
    pos   &amp;lt;- c(&#39;Agree&#39;)
    neg   &amp;lt;- c(&#39;Disagree&#39;)
  }else if(scaleName==&#39;neal&#39;){
    scale &amp;lt;- c(&#39;Never&#39;,&#39;Sometimes&#39;,&#39;Usually&#39;,&#39;Always&#39;)
    pos   &amp;lt;- c(&#39;Usually&#39;,&#39;Always&#39;)
    neg   &amp;lt;- c(&#39;Never&#39;,&#39;Sometimes&#39;)
  }else if(scaleName==&#39;noalot&#39;){
    scale &amp;lt;- c(&#39;None&#39;,&#39;A Little&#39;,&#39;Some&#39;,&#39;A Lot&#39;)
    pos   &amp;lt;- c(&#39;None&#39;,&#39;A Little&#39;,&#39;Some&#39;,&#39;A Lot&#39;)
    neg   &amp;lt;- c()
  }else if(scaleName==&#39;noall&#39;){
    scale &amp;lt;- c(&#39;None of them&#39;,&#39;Some&#39;,&#39;Most&#39;,&#39;All of them&#39;)
    pos   &amp;lt;- c(&#39;None of them&#39;,&#39;Some&#39;,&#39;Most&#39;,&#39;All of them&#39;)
    neg   &amp;lt;- c()
  }else if(scaleName==&#39;neda&#39;){
    scale &amp;lt;- c(&#39;Never&#39;,&#39;A Few Times a Year&#39;,&#39;Monthly&#39;,&#39;Weekly&#39;,&#39;Daily&#39;)
    pos   &amp;lt;- c(&#39;Never&#39;,&#39;A Few Times a Year&#39;,&#39;Monthly&#39;,&#39;Weekly&#39;,&#39;Daily&#39;)
    neg   &amp;lt;- c()
  }else if(scaleName==&#39;ny&#39;){
    scale &amp;lt;- c(&#39;No&#39;,&#39;Yes&#39;)
    pos   &amp;lt;- c(&#39;Yes&#39;)
    neg   &amp;lt;- c(&#39;No&#39;)
  }else{
    print(&#39;Unrecognized Scale Name&#39;)
  }
  # Remove neutral and non-response based values in the pull tables like
  # n-size, Not Applicable, etc.
  quest &amp;lt;- quest[,!names(quest) %in%
    c(&#39;n&#39;,&#39;Not Applicable&#39;,&amp;quot;I don&#39;t know&amp;quot;)]

  # Produce values less than 0 for negative responses
  if(length(neg)&amp;gt;0){
  quest[,names(quest) %in% c(neg)] &amp;lt;-
    -(quest[,names(quest) %in% c(neg)])
  # pos_lims and neg_lims subset each row of the data based on sign, then
  # sums the values that remain (gettting the total positive or negative
  # percentage for each row). Then, the max of the rows is saved as a candidate
  # for the magnitude of the axis.
  pos_lims &amp;lt;- max(c(sum(subset(quest,select=c(which(quest[1,-1]&amp;gt;=0)+1))[1,]),
                    sum(subset(quest,select=c(which(quest[2,-1]&amp;gt;=0)+1))[2,])))
  neg_lims &amp;lt;- max(abs(c(sum(subset(quest,select=c(which(quest[1,-1]&amp;lt;=0)+1))[1,]),
                        sum(subset(quest,select=c(which(quest[2,-1]&amp;lt;=0)+1))[2,]))))

  # The actual magnitude of the axis is the largest magnitude listed in pos_lims
  # or neg_lims, and will be inflated by .1 in each direction in the scale later
  x_axis_lims &amp;lt;- max(pos_lims,neg_lims)
  }else{

  }
  # Reshape the data so that each row has one value with a variable label.
  quest &amp;lt;- melt(quest,id.vars=&#39;Var1&#39;)

  # Factoring and ordering the response label ensures they are listed in the
  # proper order in the legend and on the stacked chart, i.e. strongly disagree
  # is furthest left and strongly agree is furthest right.
  quest[[&#39;variable&#39;]] &amp;lt;- factor(quest[[&#39;variable&#39;]],
                                levels=scale,
                                ordered=TRUE)

  # Build the plot using ggplot(). Layers are used so that positive and negative
  # can be drawn separately. This is important because the order of the negative
  # values needs to be switched.

  ##### Control flow required to change the behavior for the questions that
  ##### business requirements call for 0-100 scale with no indication of
  ##### positive or negative, i.e. the neda, noalot, and noall scaleName.
  stackedchart &amp;lt;- ggplot() +
    layer(data=subset(quest,
                      variable %in% pos),
          mapping=aes(Var1,
                      value,
                      fill=factor(variable)),
          geom=&#39;bar&#39;,
          stat=&#39;identity&#39;,
          position=&#39;stack&#39;) +
    geom_hline(yintercept=0) +
    opts(legend.title=theme_blank()) +
    opts(axis.title.x=theme_blank()) +
    opts(axis.title.y=theme_blank()) +
    opts(title=desc)
  if(length(neg)&amp;gt;0){
    stackedchart &amp;lt;- stackedchart +
      layer(data=subset(quest,
                        variable %in% neg),
            mapping=aes(Var1,
                        value,
                        fill=factor(variable),
                        order=-as.numeric(variable)),
            geom=&#39;bar&#39;,
            stat=&#39;identity&#39;,
            position=&#39;stack&#39;)
  }else{

  }
  if(scaleName %in% c(&#39;sdsa&#39;,&#39;neal&#39;)){
    colors &amp;lt;- c(&#39;#AA1111&#39;,&#39;#BB6666&#39;,&#39;#66BB66&#39;,&#39;#11AA11&#39;)
    stackedchart &amp;lt;-  stackedchart +
      scale_y_continuous(labels=percent,
                         limits=c(-x_axis_lims-.1, x_axis_lims+.1),
                         breaks=seq(-round(x_axis_lims,1)-.1,
                                    round(x_axis_lims,1)+.1,
                                    .2))
  }else if(scaleName %in% c(&#39;ny&#39;,&#39;da&#39;)){
    colors &amp;lt;- c(&#39;#BB6666&#39;,&#39;#66BB66&#39;)
    stackedchart &amp;lt;-  stackedchart +
      scale_y_continuous(labels=percent,
                         limits=c(-x_axis_lims-.1, x_axis_lims+.1),
                         breaks=seq(-round(x_axis_lims,1)-.1,
                                    round(x_axis_lims,1)+.1,
                                    .2))
  }else if(scaleName %in% c(&#39;noalot&#39;,&#39;noall&#39;)){
    colors &amp;lt;- brewer.pal(name=&#39;Blues&#39;,n=6)[3:6]
    stackedchart &amp;lt;-  stackedchart +
      scale_y_continuous(labels=percent,
                         limits=c(0,1.05),
                         breaks=seq(0,1,.1))
  }else if(scaleName %in% c(&#39;neda&#39;)){
    colors &amp;lt;- brewer.pal(name=&#39;Blues&#39;,n=7)[3:7]
    stackedchart &amp;lt;-  stackedchart +
      scale_y_continuous(labels=percent,
                         limits=c(0,1.05),
                         breaks=seq(0,1,.1))
  }else{
    print(&#39;Unrecognized scaleName&#39;)
  }
  return(stackedchart + scale_fill_manual(limits=scale,
                                          values=colors) +
                        coord_flip())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Mainly, I would like to abstract this code further. I am only about halfway there to assuring that I can use Likert-scale data of any size. I also would like to take in more than one question simultaneously with the visualize function. The latter is already possible in my production code and is particularly high impact for these kinds of graphics
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:netstacked&#34;&gt;Net Stacked Likert graphs are excellent for comparing how different groups responded to the same question. There is both high information density and clarity.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:netstacked&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Update on Social Promotion</title>
      <link>http://www.json.blog/2012/07/update-on-social-promotion/</link>
      <pubDate>Tue, 10 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/update-on-social-promotion/</guid>
      <description>&lt;p&gt;This &lt;a href=&#34;http://www.startinganedschool.org/2012/07/09/letter-from-an-8th-grader/&#34;&gt;poignant post&lt;/a&gt; from &lt;a href=&#34;http://www.startinganedschool.org/author/mike/&#34;&gt;Michael Goldstein&lt;/a&gt; ends with a few policy thoughts that largely support my &lt;a href=&#34;http://blog.jasonpbecker.com/2012/02/15/social-promotion-tutoring-and-funding/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Goldstein&amp;rsquo;s second point is worth highlighting:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Anyway, in a small school, large-scale research isn&amp;rsquo;t the key
determinant anyway. The team&amp;rsquo;s implementation is.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On the same day that &lt;a href=&#34;http://shankerblog.org&#34;&gt;Shanker Blog&lt;/a&gt; is assuring us that &lt;a href=&#34;http://shankerblog.org/?p=6180&#34;&gt;rigorous social science is worth it&lt;/a&gt;, Goldstein delivers researchers a healthy dose of humility. Rigorous research is all about doing the best we can to remove all the confounding explanatory factors that have an impact on our observed outcomes to isolate an intervention. But even in the most rigorous studies social scientists are often measuring the &lt;a href=&#34;http://en.wikipedia.org/wiki/Average_treatment_effect&#34;&gt;&lt;strong&gt;A&lt;/strong&gt;verage &lt;strong&gt;T&lt;/strong&gt;reatment &lt;strong&gt;E&lt;/strong&gt;ffect&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;How rarely do we truly encounter a completely average situation? The real impact in any particular school or organization can be dramatically different in magnitude and even direction because of all the pesky observed and &lt;a href=&#34;http://en.wikipedia.org/wiki/Omitted-variable_bias&#34;&gt;unobserved&lt;/a&gt; confounding factors that researchers work so hard to be able to &lt;a href=&#34;http://en.wikipedia.org/wiki/Validity_(statistics)&#34;&gt;ignore&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So my advice? If you are down on the ground keep close to the research but keep closer to your intuition, provided you are ready, willing, and able to monitor, evaluate, and adjust.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Work Place Liberty in the Context of Education</title>
      <link>http://www.json.blog/2012/07/work-place-liberty-in-the-context-of-education/</link>
      <pubDate>Mon, 09 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/07/work-place-liberty-in-the-context-of-education/</guid>
      <description>&lt;p&gt;How can we tell if principal directives are fair to teachers?&lt;/p&gt;

&lt;p&gt;There has been a great conversation circling some blogs I read over the last week about liberty in the work place.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
Issues of fairness in the work place are a constant in today&amp;rsquo;s education conversation. Whether some view it as a form of metaphoric violence on teachers and their profession, while others see a concerted effort to change rigid, bureaucratic systems that prevent effective change&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, at the heart of education reform &lt;em&gt;du jour&lt;/em&gt; are changes to workplace freedom. Improving human capital systems has meant dismantling questionable licensing requirements&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, dramatic changes in &lt;a href=&#34;http://www.ride.ri.gov/EducatorQuality/EducatorEvaluation/Default.aspx&#34;&gt;teacher&lt;/a&gt; &lt;a href=&#34;http://dcps.dc.gov/DCPS/In+the+Classroom/Ensuring+Teacher+Success/IMPACT+(Performance+Assessment)/An+Overview+of+IMPACT&#34;&gt;evaluation&lt;/a&gt;, and other dramatic changes to who gets &lt;a href=&#34;http://www.commercialappeal.com/news/2012/may/11/reviews-to-favor-effective-teachers/?print=1&#34;&gt;hired&lt;/a&gt; or &lt;a href=&#34;http://en.wikipedia.org/wiki/LIFO_(education)&#34;&gt;fired&lt;/a&gt;. Using extended learning time, either through additional instructional days and/or longer school days, to increase student achievement is often considered too costly, because teachers demand more pay for more work. Additional professional development days are similarly costly; teachers are loath to give up additional days in the summer or during school vacations without receiving additional pay. I could go on.&lt;/p&gt;

&lt;p&gt;All of these reforms seek to radically change terms and benefits in teacher contracts and state law that represent a string of hard-fought (and won) battles that teachers and their unions pursued for years. The political left, and more specifically the progressive movement, has generally picked up on these attempts as anti-union, anti-collective bargaining, anti-democratic, anti-teacher, and anti-education. There are even a host of conspiracy theories decrying the &amp;ldquo;corporate reformers&amp;rdquo; who are coming into the education realm to break down good, public, democratic systems that are good for Democrats, largely to hurt poor kids and make profits&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Fundamentally, most of this argument is about what individuals ideologies have led them to believe about employee rights and employer rights. I find it increasingly frustrating that these conversations do not address the deeper philosophical differences. This is why I have really enjoyed observing the current conversation between &lt;a href=&#34;http://crookedtimber.org/&#34;&gt;Crooked Timber&lt;/a&gt;, &lt;a href=&#34;http://bleedingheartlibertarians.com/&#34;&gt;Bleeding Heart Libertarians&lt;/a&gt;, and others.&lt;/p&gt;

&lt;p&gt;One of the key aspects of the BRG argument&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;is that worker contracts are unique because many of the terms of employment are ambiguous. Employers should only be permitted to demand that employees partake in activities to which they have consented. The contract is supposedly a signal of this consent, however, because the terms are so often ambiguous, disputes over whether or not it covers an activity are practically a guarantee. So how should these disputes be settled and by whom? BRG would argue that there should be strong worker freedom to make sure that their consent is truly given. They consider the relationship between employer and employee to be naturally coercive, at least in part because they assume the right to end employment has very asymmetric benefits since employees have, presumably, much more to lose than employers when the contract ends. BRG assumes that freedom is best served through a democratic workplace with very powerful employees who have few, if any of their rights restricted in the workplace. On the other hand, BHLers believe that it is possible to consent to restrict ones rights within a contractual relationship, they do not tend to accept that the right to exit affords highly asymmetric freedoms&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, and they feel that freedom is maximized by abstaining from limiting private contracts while maximizing the rights to freely enter and exit contracts.&lt;/p&gt;

&lt;p&gt;However, it is macroeconomist Miles Kimball, a recent entrant into the &lt;a href=&#34;http://blog.supplysideliberal.com/&#34;&gt;blogosphere&lt;/a&gt;, whose &lt;a href=&#34;http://blog.supplysideliberal.com/post/26531357710/jobs&#34;&gt;comments&lt;/a&gt; I felt could most directly be applied to education. If I were to summarize his post, it would be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;There are significant pressures against to eliminating freedoms of
your workers that end up making them worse at their jobs or lead to
attracting bad talent.&lt;/li&gt;
&lt;li&gt;Although these pressures exist for &amp;ldquo;The Firm&amp;rdquo;, it is true that
&amp;ldquo;underbosses&amp;rdquo; with significant power can act in ways that maximize
their personal gain instead of what&amp;rsquo;s good for &amp;ldquo;The Firm&amp;rdquo; and the
pressures are less strong against eliminating freedoms for them than
the organization as a whole.&lt;/li&gt;
&lt;li&gt;Nevertheless, they should have the rights to limit/remove freedoms,
and these limitations should be based on whether they are relevant
to achieving the organizations pre-stated mission.&lt;/li&gt;
&lt;li&gt;Ultimately, the right outside force to judge whether this was a
proper imposition to make on employees should be people who have
successfully navigated the same challenges as The Firm but have no
direct interest in The Firms current activities.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of these four points, if they are accepted as true, has some interesting applications to education. My translation for education colleagues would be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Districts and states have little reason to make lives shitty for
teachers. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;But some principals, department heads, and others may have the ability to act in ways that are less than proper. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Actions that restrict teacher rights should be judged on whether they help the school achieve the district or school&amp;rsquo;s pre-stated mission.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Disputes between teachers and their bosses, should not be adjudicated by a typical jury or judge. Instead, the actions of the principal should be judged by other principals who have been successful, preferably with some distance from the actual organization (i.e. not principals who might compete for the offending principals job or may want to hire or be stuck with that teacher based on the proceedings).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think that points 1 and 2 are fairly obvious. Points 3 and 4, however, are far more interesting.&lt;/p&gt;

&lt;p&gt;Kimball is attempting to split the difference a fascinating way. I believe he would accept that employment contracts are, by necessity, &amp;ldquo;ambiguous&amp;rdquo; in the way that BRG defines that term. His argument is, therefore, that the mission and purpose of the organization &lt;strong&gt;should not&lt;/strong&gt;be ambiguous. So long as the organization&amp;rsquo;s mission is clear, an employment contract becomes consent to do whatever has a rational basis for furthering those goals. In this way, there is an ethical standard by which we can judge new situations that could never have been anticipated directly at the contracting stage. For example, it may be perfectly reasonable for a principal to require a teacher to spend lunch in the student cafeteria so long as their is a rational basis for believing this would further the mission of the school.&lt;/p&gt;

&lt;p&gt;In highly unionized workplaces, work rules are so specific that they remove a substantial portion of the ambiguity in contracting. This is generally seen by the left, union members, and other BRG-like thinkers as a huge victory. It implies full consent to the terms of employment and substantial restriction of an employer&amp;rsquo;s ability to abuse their position and abridge the freedoms of their employees in unethical ways. Schools are generally like this. Practically everything is spelled out about a teacher&amp;rsquo;s position, often to the minute. How long they get to eat lunch, how much unstructured time they get during the day, how long they have to spend time working with other teachers, how long they are allowed to be placed in front of kids, how many kids can be placed in front of a teacher at any given time, these conditions and more are detailed in teacher contracts.&lt;/p&gt;

&lt;p&gt;In my experience, when I ask a union supporter why they think unions are good, they almost always point out &amp;ldquo;abuses&amp;rdquo; of employers that occurred often before the union wrestled power from the grips of the few and the privileged back to the laborers. I have to wonder how much of their support comes from a lack of common, clear definition of unethical abridgments of freedom in the workplace. The solution to this ambiguity is requiring that all actions be consented to through negotiation and contracting, which also determines that dispute resolution is a matter of contract law. I have to wonder if both workers and their employers would be better off if there was a universal ethical standard like Kimball proposes. This way consent can be given while allowing more ambiguity in the contract itself. Right now employers fight for this ambiguity depending solely on appeals to trust and cooperation, two things that are rarely earned before working with someone as would be required.&lt;/p&gt;

&lt;p&gt;I can&amp;rsquo;t say that I understand labor dispute resolution well enough to comment on the differences between Kimball&amp;rsquo;s fourth suggestion and current practice. However, it is pretty clear to me that enforcement through contract law is costly and inefficient, regardless whether it is effective in adjudicating disputes in an ethical matter. Labor relations boards, as far as I can tell, seem to be a political tool swaying between dramatically increasing worker power, especially when members are current or former full-time employees and members of a union, and increasing employer power when more corporate representation is assured. If only I believed it were possible to have an apolitical, disinterested board, with sector-specific expertise, determine whether there is a &amp;ldquo;rational basis&amp;rdquo; for employer actions that Kimball envisions.&lt;/p&gt;

&lt;p&gt;I am left with more questions than answers, but, for me, there is a rich appeal to utilizing the mission of an organization to determine whether the actions of both it and its employees are just.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;http://crookedtimber.org/2012/07/01/let-it-bleed-libertarianism-and-the-workplace/&#34;&gt;Here&lt;/a&gt; is the (socialist?) critique of libertarians and right to work that sparked the discussion. Then &lt;a href=&#34;http://marginalrevolution.com/marginalrevolution/2012/07/libertarianism-and-the-workplace.html&#34;&gt;two&lt;/a&gt; economists &lt;a href=&#34;http://marginalrevolution.com/marginalrevolution/2012/07/libertarianism-and-the-workplace-ii.html&#34;&gt;jumped&lt;/a&gt; in. The &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/freedom-and-work/&#34;&gt;response&lt;/a&gt; from Bleeding Heart Libertarians, meanwhile, &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/libertarianism-the-workplace-and-the-reconciling-power-of-the-social-moral-order/&#34;&gt;continues&lt;/a&gt; &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/denmark-vs-france/&#34;&gt;to&lt;/a&gt; &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/why-are-employers-so-mean/&#34;&gt;pour&lt;/a&gt; &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/my-bottom-line-on-worker-freedom/&#34;&gt;in&lt;/a&gt; &lt;a href=&#34;http://bleedingheartlibertarians.com/2012/07/cruelty-and-power/&#34;&gt;rapidly&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;I lean toward the latter, even if I disagree sometimes both with the means and ends of the current reform movement.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;I am skeptical about &lt;a href=&#34;http://conversableeconomist.blogspot.com/2012/05/occupational-licensing-and-low-income.html&#34;&gt;licensing in general&lt;/a&gt;. I&amp;rsquo;ve seen substantially more &lt;a href=&#34;http://www0.gsb.columbia.edu/faculty/jrockoff/certification-final.pdf&#34;&gt;research&lt;/a&gt; to support experience than licensing requirements in education and/or additional education. Various &lt;a href=&#34;http://tntp.org/&#34;&gt;alternative&lt;/a&gt; &lt;a href=&#34;http://www.teachforamerica.org/&#34;&gt;teacher&lt;/a&gt; &lt;a href=&#34;http://www.bostonteacherresidency.org/&#34;&gt;pathways&lt;/a&gt; now exist.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Whereas I find some of the &amp;ldquo;anti-s&amp;rdquo; in the previous sentence worthy of discussion, I find the massive, corporate, right-wing conspiracy stuff to be 98% bullocks.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;BRG= Bertram, Robin, and Gourevitch, authors of the Crooked Timber post. This acronym has been used in contrast with BHL, Bleeding Heart Libertarians, during this debate
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;or perhaps, if they do they feel that this would not be the case in a world that more generally matched BHLs principles
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Here I assume that the proper size unit for &amp;ldquo;The Firm&amp;rdquo; is above the school. I think this generally holds because district boards and state policy makers tend to be more directly accountable to citizens than schools. The relationship here is much more like shareholders and/or customers to business than the citizen to school relationship.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Here, the school-level administration represents the &amp;ldquo;underbosses&amp;rdquo;. Given that much of the debate over teacher evaluation and rules on hiring and firing stem from debates about both principal quality and principal power, I think this is the right assignment.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Algorithms Design and Analysis Part I</title>
      <link>http://www.json.blog/2012/06/algorithms-design-and-analysis-part-i/</link>
      <pubDate>Wed, 13 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/06/algorithms-design-and-analysis-part-i/</guid>
      <description>

&lt;p&gt;Tonight I started Coursera.org&amp;rsquo;s &lt;a href=&#34;http://www.algo-class.org&#34;&gt;Algorithms: Design and Analysis Part I&lt;/a&gt;. This class should pick up right about where I left off my computer science education. I took &lt;a href=&#34;http://www.cs.brown.edu/courses/csci0150.html&#34;&gt;CS15&lt;/a&gt; as a sophomore in college but didn&amp;rsquo;t have the time to take &lt;a href=&#34;http://www.cs.brown.du/courses/csci0160html&#34;&gt;CS16: Introduction to Algorithms and Data Structures&lt;/a&gt;. So, while it&amp;rsquo;s been almost 6 years since I have formally taken a computer science class, it is time to continue my education.&lt;/p&gt;

&lt;p&gt;I plan to write about once a week about my experience. This will serve both as an opportunity to work out ideas spurred by the course as well as a review of the growing area of free, online courses that started way back in 2002 with MIT&amp;rsquo;s &lt;a href=&#34;http://ocw.mit.edu/index.htm&#34;&gt;OpenCourseWare&lt;/a&gt; and continues today with upshots &lt;a href=&#34;http://udacity.com/&#34;&gt;Udacity&lt;/a&gt; and &lt;a href=&#34;http://www.coursera.org&#34;&gt;Coursera&lt;/a&gt;, among &lt;a href=&#34;http://www.codeacademy.com&#34;&gt;other players&lt;/a&gt;. Given the emphasis being placed on the potential for technology as disruptive to classroom teaching over the last 50 years, the topic seems worthy of some experiential learning by a budding young education researcher/wonk.&lt;/p&gt;

&lt;h2 id=&#34;introduction-and-about-the-course&#34;&gt;Introduction and About the Course&lt;/h2&gt;

&lt;p&gt;The Introduction video was a bit scary. Although the content was simple, Professor Tim Roughgarden is a fast talker and he does seem to skip some of the small steps that really trip me up when learning math from lectures. For example, in discussing the first recursive method to $n$-digit multiplication, Professor Roughgarden suddenly throws in a $10^n$ and $10^{n/2}$ term that I just couldn&amp;rsquo;t trace. I kept watching the video waiting for an explanation and pondering it in
my mind when a few minutes later it hit me; the two terms kept the place information lost when a number is split into its constituent digits &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:constituent&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:constituent&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The About this Course video, however, provided some good advice I intend on following; although there will be no code written as a part of this course to be language neutral, I will be attempting to code each of the described algorithms on my own. Professor Roughgarden&amp;rsquo;s assumption is that this is within the skills of students taking this class. Generally, I believe I am capable of achieving this in at least *some* language. Currently, I prefer to use R. This is not because R is best suited to this kind of work. Rather, it is because I am relatively new to R, and I think that learning to program some fundamental computational tasks will be good for learning the ins and outs of the language.&lt;/p&gt;

&lt;p&gt;However, I think I may switch over to using Python later in the course. Why? Because I feel like learning Python and Udacity happens to have a &lt;a href=&#34;http://www.udacity.com/view#Course/cs101/&#34;&gt;course up already to do just that&lt;/a&gt;. My hope is to incorporate free online learning into my routine just like I include reading dead-tree books, Google Reader, and mess around on Twitter. So while I can&amp;rsquo;t swear that I&amp;rsquo;ll actually start moving through these two courses (and two more I&amp;rsquo;m interested in starting June 25), I feel having complimentary, simultaneous course work will push me. Each class should reinforce the other and I should see the most benefit if I keep up with both.&lt;/p&gt;

&lt;p&gt;Finally, this class is a big time commitment. The first week has 3.5 hours of lecture time allotted. A typical Brown class would meet for only about 2.5 hours a week (three 50 minute classes or two 80 minute classes). That means a lot of time, not including homework or spending time actually coding and implementing the introduced algorithms. Although some of this material is &amp;ldquo;optional&amp;rdquo; (about an hour), that&amp;rsquo;s still pretty intimidating for a free, online, spare time class. Make no mistake, if time commitment is any indicator, this will be every bit as challenging (to actually learn) as a real college course that last this many weeks.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:constituent&#34;&gt;The algorithm called to split an $n$ digit number $x$ into two, $n/2$ digit numbers. What was unstated, but of course true, is that this transformation must result in an expression that was equal to $x$. Of course, $x=10^n * a+10^\frac{n}{2} * b$ , because the leading digit of $a$ must be in the $n^{th}$ place and the leading digit of $b$ must be in the $\frac{n}{2}^{th}$ place. Nothing about this is complex to me, but it was not obvious at the speed of conversation. I think working out an actual example of a 4-digit number multiplication, as Professor Roughgarden had with the &amp;ldquo;primitive&amp;rdquo; multiplication algorithm, would have made this far clearer.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:constituent&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fixing Dyer Street</title>
      <link>http://www.json.blog/2012/06/fixing-dyer-street/</link>
      <pubDate>Thu, 07 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/06/fixing-dyer-street/</guid>
      <description>

&lt;p&gt;The removal of I-195 from the Jewelry District is supposed to help spur Providence&amp;rsquo;s second renaissance by providing ample green- and brown-field development sites for a whole host of biomedical companies apparently dying to move to a state and city in fiscal crisis whose current population does not have the required skills to serve as an employment base.&lt;/p&gt;

&lt;p&gt;Seriously, I am quite optimistic about the once-in-a-generation to develop a massive part of what should be an integral part of Providence&amp;rsquo;s downtown core &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:core&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:core&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Buildings will come, even if it is a slow, grueling process. Hopefully jobs will follow. But a key first step the incredibly busy &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:busy&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:busy&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; I-195 commission must take is elevating Dyer Street to a new hub of activity.&lt;/p&gt;

&lt;h2 id=&#34;why-dyer-street&#34;&gt;Why Dyer Street&lt;/h2&gt;

&lt;p&gt;This is a particularly important site for the redevelopment of Providence. One of the highest profile completed development projects in the Jewelry District has been &lt;a href=&#34;http://brown.edu/academics/medical/&#34;&gt;Brown University&amp;rsquo;s Alpert Medical School&lt;/a&gt; at &lt;a href=&#34;http://med.brown.edu/newbuilding/&#34;&gt;Ship Street and Eddy Street&lt;/a&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:dyertoeddy&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:dyertoeddy&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Further down Eddy Street, we find one of the tragic
failures of the Jewelry District, &lt;a href=&#34;http://www.flickr.com/photos/woneffe/4371630714/sizes/l/in/photostream/&#34;&gt;Narragansett Electric Lighting&lt;/a&gt; (Dynamo House), a hulking brick site left open to the elements that was set at one point to become a museum. &lt;a href=&#34;https://maps.google.com/?ll=41.816875,-71.40651&amp;amp;spn=100.50313,204.433594&amp;amp;hnear=385+Westminster+St,+Providence,+Rhode+Island+02903&amp;amp;t=v&amp;amp;z=3&amp;amp;layer=c&amp;amp;panoid=iH5omOmwEwV_aAhyccIScQ&amp;amp;cbll=41.816875,-71.40651&amp;amp;cbp=13,28.53329819751501,,0,-6.703606203030631&#34;&gt;One Davol Square&lt;/a&gt;, a &lt;a href=&#34;http://www.ri-cie.org/start-incubator/start-incubator&#34;&gt;popular site for entrepreneurs&lt;/a&gt; in Providence is found where Eddy meets Point Street.&lt;/p&gt;

&lt;p&gt;Brown University has already purchased 200 Dyer Street, which sits to the north at the &amp;ldquo;start&amp;rdquo; of Dyer Street between Clinton and Dorrance. This site was recently renovated and is now home to &lt;a href=&#34;http://brown.edu/ce/&#34;&gt;Brown University&amp;rsquo;s Continuing Education&lt;/a&gt;, an adult education site primarily mid-career professionals and adults. Already 200 Dyer hosts forums intended for the Providence community and, along with expanding CE into so-called &amp;ldquo;&lt;a href=&#34;http://www.browndailyherald.com/proposed-executive-master-s-program-would-diversify-revenue-streams-1.2718694&#34;&gt;executive master&amp;rsquo;s programs&lt;/a&gt;&amp;rdquo;, this site is likely to be a hub of substantial interaction between Brown and Providence residents.&lt;/p&gt;

&lt;p&gt;It is easy to see that Eddy Street, from Ship Street to Point Street, is already an important hub of job-related activity in the Jewelry District. The very presence of an existing, huge, historic site between
Alpert Medical School and a major center for startups makes it likely that this stretch could see real further development. And with Brown staking claim to the &amp;ldquo;mouth&amp;rdquo; of Dyer Street, the makings of a Brown University &amp;ldquo;West&amp;rdquo; campus &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:campuswest&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:campuswest&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; is coming into view.&lt;/p&gt;

&lt;p&gt;Expanding Riverwalk Park into the space between Dorrance and Ship Street as planned should be the final piece to the Dyer Street puzzle &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:puzzle&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:puzzle&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;It seems that turning Dyer Street into an &amp;ldquo;A&amp;rdquo; street filled with activity should be one of the easiest sells of all in the Jewelry District, given this is one of the few areas where actual purchases have taken place other than the land behind Johnson and Wales.&lt;/p&gt;

&lt;p&gt;Luckily, as far as I can tell, Providence Planning&amp;rsquo;s vision for the repairing of the street grid in this area is right on the mark, because while the land adjacent to Dyer Street from Friendship Street to Ship Street is some of the most &amp;ldquo;shovel-ready&amp;rdquo; land in the Jewelry District, this stretch also represents some of the most obviously damaged by the highway &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:highway&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:highway&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. It is easily fixed. Dyer Street should be two ways all the way and not shift to a one-way at Peck Street. The remnants of an &amp;ldquo;on&amp;rdquo; ramp that serves as the northbound route connecting Ship Street and Peck Street should obviously be eliminated and subsumed in the expanded Riverwalk Park. An additional oddity left from another on ramp between Dorrance and Clifford Street should be removed, allowing the two-way Dyer to have a straighter path. Dyer should potentially be expanded to include bike lanes separated from traffic by trees on the eastern side.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/Chapinero_bike_path.jpg&#34; alt=&#34;Chapinero, Bogotá Bike Path via Wikipedia&#34; title=&#34;Chapinero Bike Path&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Create a street like this. Encourage development on the west led by Brown University connecting Alpert Medical School to Brown Continuing Education. Bring in creative commercial development forming a continuous street wall of jobs from One Davol Square to the new, expanded park. Attach the proposed Greenway through the Jewelry District and the planned pedestrian bridge to Fox Point. Do all of this, and Dyer Street will become one of the most vibrant *places* in Providence.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:core&#34;&gt;I am not being sarcastic here, even if I&amp;rsquo;m generally dismissive and flippant about the wacky ideas that the &amp;ldquo;elite&amp;rdquo; in Providence and the state of Rhode Island have about this space
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:core&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:busy&#34;&gt;Okay, so here I&amp;rsquo;m being sarcastic
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:busy&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:dyertoeddy&#34;&gt;Dyer turns into Eddy past Ship
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:dyertoeddy&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:campuswest&#34;&gt;I don&amp;rsquo;t think they use this term
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:campuswest&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:puzzle&#34;&gt;Although it appears the I-195 commission is getting cold feet on the expanse of this public space
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:puzzle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:highway&#34;&gt;It will likely help to look at &lt;a href=&#34;https://maps.google.com/?ll=41.820716,-71.407483&amp;amp;spn=0.003274,0.006239&amp;amp;hnear=385+Westminster+St,+Providence,+Rhode+Island+02903&amp;amp;t=v&amp;amp;z=18&#34;&gt;this view&lt;/a&gt; while reading this next section
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:highway&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Downtown Improvement District</title>
      <link>http://www.json.blog/2012/04/downtown-improvement-district/</link>
      <pubDate>Wed, 25 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/04/downtown-improvement-district/</guid>
      <description>&lt;p&gt;[&lt;img src=&#34;http://blog.jasonpbecker.com/wp-content/uploads/2012/04/did5.jpeg&#34; alt=&#34;DID taking care of Downcity planters&#34; title=&#34;DID Doing a Great Job&#34; /&gt;][][/caption]&lt;/p&gt;

&lt;p&gt;I am a firm believer that some goods &lt;em&gt;should be&lt;/em&gt; &lt;a href=&#34;http://en.wikipedia.org/wiki/Public_goods&#34;&gt;public&lt;/a&gt;. I do not believe that my tax dollars are about providing *direct* personal benefit. I like redistributed tax policy. But it is hard to be a Rhode Islander, surrounded by government institutions &lt;a href=&#34;http://www.golocalprov.com/news/julia-steiny-woonsockets-nosedive-a-cautionary-tale/&#34;&gt;that&lt;/a&gt; are
&lt;a href=&#34;http://www.golocalprov.com/news/procap/&#34;&gt;failing&lt;/a&gt;, and feel good about the taxes I pay. Corruption and &lt;a href=&#34;http://www2.turnto10.com/news/2012/apr/06/former-uri-president-enters-sport-institute-scanda-ar-991425/&#34;&gt;cronyism&lt;/a&gt; is a daily reality of government business. Some agencies have &lt;a href=&#34;http://www.ripec.org/pdfs/2009-AI-Study.pdf&#34;&gt;tremendous waste and inefficiency&lt;/a&gt;. Worse, many public institutions that are failing their mission and wasting money are actually woefully underfunded. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:underfunded&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:underfunded&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If more government institutions functioned like the Downtown Improvement District, there would be greater trust and support for government services.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some of the best money I spend each year is the approximately \$200-250 that I send to the &lt;a href=&#34;http://downtownprovidence.com/clean-safe/&#34;&gt;Downtown Improvement District&lt;/a&gt; (DID).&lt;/p&gt;

&lt;p&gt;I live within a special assessment district in Providence that levies an additional property tax to pay for the ladies and gentlemen in bright yellow jackets that are a constant presence in my neighborhood. For a small tax each year, my neighborhood gets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;substantial cleanup /sanitation services that removes the mountains of trash that can pile up during a busy night where &lt;a href=&#34;http://jwu.edu/providence/&#34;&gt;college students&lt;/a&gt;, &lt;a href=&#34;http://www.ppacri.org/&#34;&gt;theater&lt;/a&gt; &lt;a href=&#34;http://www.trinityrep.com/&#34;&gt;goers&lt;/a&gt;, &lt;a href=&#34;http://graciesprovidence.com/&#34;&gt;restaurant patrons&lt;/a&gt;, nightclub patrons, &lt;a href=&#34;http://www.hotelprovidence.com/&#34;&gt;tourists&lt;/a&gt;, and &lt;a href=&#34;http://www.waterfire.org/&#34;&gt;Waterfire&lt;/a&gt; visitors all converge on Downcity&lt;/li&gt;
&lt;li&gt;excellent landscaping including planting and pruning trees, maintaining flowerbeds, hanging flower pots on light posts, etc.&lt;/li&gt;
&lt;li&gt;responsive care of public property including removing safety hazards, e.g. removing the cement, waist-high barrier that had fallen on the sidewalk by my building that was a major safety hazard for pedestrians&lt;/li&gt;
&lt;li&gt;easily identifiable public presence in addition to cops that increases the safety and security of busy city street&lt;/li&gt;
&lt;li&gt;much, much more.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It may seem selfish, but honestly, this is the best government service I current receive. It is inexpensive. I am able to see a direct increase in my quality of life in Downcity. It clearly increases and protects my property investment. I get an annual budget that is fairly detailed
mailed annually that explains precisely what my dollars purchased and how they will be used in the coming years.&lt;/p&gt;

&lt;p&gt;When the currently dormant[^0] &lt;a href=&#34;http://providencecoreconnector.com/&#34;&gt;Providence Core Connector&lt;/a&gt; announced they would seek to use a special assessment district to fund operation expenses, &lt;a href=&#34;http://blog.jasonpbecker.com/2011/09/26/downcity-residents-should-support-the-core-connector-and-the-tax-makes-sense/&#34;&gt;I was all for it&lt;/a&gt;. Sure, a
portion of my support came from the simple economics, but I would be lying if I did not admit that the wonderful relationship I have with the Downtown Improvement District was not a part of my consideration. The DID has provided an excellent model to Downcity residents demonstratingthe efficacy of using the greatest (but not sole) beneficiary of place-bound services as a revenue source. Does anyone really believe that Downtown would have doubled its residency from 2000-2010 &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:census&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:census&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; if the DID were not around?&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;[&lt;img src=&#34;http://blog.jasonpbecker.com/wp-content/uploads/2012/04/did5.jpeg&#34; alt=&#34;DID taking care of Downcity planters&#34; title=&#34;DID Doing a Great Job&#34; /&gt;]: &lt;a href=&#34;http://www.providenceri.com/CityNews/newsletter2.php?id=290&#34;&gt;http://www.providenceri.com/CityNews/newsletter2.php?id=290&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:underfunded&#34;&gt;See Pawtucket and Woonsocket on &lt;a href=&#34;http://www.ride.ri.gov/Finance/funding/Uniform%20Chart%20of%20Accounts/2010/STATE/FY10%20Equalized%20Expenditures%20Report%20-%20Sorted.pdf&#34;&gt;this chart&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:underfunded&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:census&#34;&gt;US Census Bureau. Check out this &lt;a href=&#34;http://gis.providenceplanning.org/PVD_2010CensusViewer/&#34;&gt;great resource&lt;/a&gt; that the &lt;a href=&#34;http://www.providenceri.com/planning/&#34;&gt;Providence Planning Department&lt;/a&gt; put up
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:census&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Providence needs a little innovation</title>
      <link>http://www.json.blog/2012/03/providence-needs-a-little-innovation/</link>
      <pubDate>Tue, 20 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/03/providence-needs-a-little-innovation/</guid>
      <description>&lt;p&gt;Have you ever tried to access public information about &lt;a href=&#34;http://www.providenceri.com&#34;&gt;Providence&lt;/a&gt; on the web? Due to the recent, and new, requirement that residents reapply for their homestead tax exemption in Providence, I decided to poke around the Providence webpage to see what kind of &lt;a href=&#34;http://providence.ias-clt.com/parcel.list.php&#34;&gt;public information on property was available online&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I was greeted with an IT nightmare&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The system was clearly a third-party developed or purchased front end designed for searching public records on property and sold to municipalities throughout the country by winning contracts through the RFP process. It&amp;rsquo;s also clearly no longer supported (or supported poorly), outdated, and running on hardware that&amp;rsquo;s probably about as powerful as an 8-year-old desktop computer. The system crawls, when it&amp;rsquo;s not completely still, and provides no means of easy export that I can find.&lt;/p&gt;

&lt;p&gt;This was really disappointing to me. In an earlier post, I began to look at some simple data available on the Providence Journal&amp;rsquo;s webpage about recent sales. I was hoping to use an API (and in the worst case, a massive data dump) to get access to information about the assessed value of recently sold properties and to play around a bit with various heat maps and see what patterns are revealed. Even if there were some way to get access to the data, the application is so poor I definitely do not have the patience required.&lt;/p&gt;

&lt;p&gt;This is a real shame, because one of the great things about data on property is that it is all public information. This means that the data could be shared widely to creative policy wonks, data geeks, and CS nerds looking for a weekend project. There are now two cities&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;in the US that are employing a &lt;a href=&#34;http://www.theatlanticcities.com/technology/2012/03/dawn-muncipal-chief-innovation-officer/1516&#34;&gt;new kind of CIO&lt;/a&gt;&amp;ndash; the Chief &lt;em&gt;Innovation&lt;/em&gt; Officer&amp;ndash; whose role is to connect government resources, be they employees, data, or infrastructure, with folks who can do something new, exciting, and useful for city residents.&lt;/p&gt;

&lt;p&gt;Developers designed one application in San Francisco to reduce the &amp;ldquo;notoriously cumbersome hurdles for starting a new business.&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;Does anyone happen to know of another city, perhaps without the rich, entrepreneurial technology and science economy it lusts for, that is not known as an easy place to set up shop? &lt;a href=&#34;http://www.providenceri.com&#34;&gt;Hint&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When Brown University has an excellent computer science department made up of undergrads who work on side projects like &lt;a href=&#34;http://brown.mochacourses.com&#34;&gt;developing an online course catalog&lt;/a&gt; since the &lt;a href=&#34;http://www.browndailyherald.com/2.12225/ucs-criticizes-new-banner-course-catalog-1.1674252#.T2frNWJSRe8&#34;&gt;Brown purchased system sucks&lt;/a&gt;, it&amp;rsquo;s hard not to conclude that there are latent geek talents just waiting to be tapped. And it&amp;rsquo;s not just Brown that could be engaged. What about &lt;a href=&#34;http://swipely.com/&#34;&gt;Swipely&lt;/a&gt;, the newest tech startup from entrepreneur &lt;a href=&#34;http://angusdavis.com/&#34;&gt;Angus Davis&lt;/a&gt;, a Rhode Island native who is very active in local government and developing Providence. An exciting. fledging technology company with young, smart folks who do application development with huge swaths of data every day is an excellent source of the kind of talent Providence needs. Why hasn&amp;rsquo;t someone approached Swipely about a charity opportunity&amp;ndash; take all non-essential staff off their current projects, give them a break from the deadlines and typical day, and get them working furiously for a week on rolling out something awesome and useful for city residents. It would reinvigorate young coders and greatly benefit the city.&lt;/p&gt;

&lt;p&gt;Heck, something as simple as getting in and teaching government employees how to spin up instances of &lt;a href=&#34;http://aws.amazon.com/&#34;&gt;Amazon EC2&lt;/a&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; so they can migrate off self-managed, slow servers for non-secure information would be huge. Imagine this, plus rolling an API for critical municipal data sets. There would be a rich environment for hobbyists, students, and professionals alike to spark some creative ways to understand and interact with Providence.&lt;/p&gt;

&lt;p&gt;Another example, assuming it&amp;rsquo;s legal, how long do you think it would take a smart developer, dedicated solely to one task, to roll a system that would cross-reference Providence landowners with the DMV registration database so that myself and other residents didn&amp;rsquo;t have to trek out documentation to City Hall to avoid a 50% tax hike?&lt;/p&gt;

&lt;p&gt;Providence is not San Francisco, but there are many talented developers and data scientists who are passionate enough about the city to donate their time and expertise. Honestly, for many of these folks there are real personal benefits, even without appealing to some sentiment of civic duty. So let&amp;rsquo;s open up our data, our infrastructure, and our employees. Let&amp;rsquo;s encourage folks from outside of government to inject excitement and new skills for current government IT employees and analysts.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s time for Providence&amp;rsquo;s own &amp;ldquo;Summer of Code&amp;rdquo;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;To be fair, when I accessed the site today the site was substantially more functional than it was in two previous visits several weeks ago
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;San Francisco, unsurprisingly, and Philadelphia
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;From &lt;a href=&#34;http://www.theatlanticcities.com&#34;&gt;The Atlantic Cities&lt;/a&gt; article linked above.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;&lt;a href=&#34;http://csrc.nist.gov/groups/SMA/fisma/index.html&#34;&gt;FISMA&lt;/a&gt; compliant
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Social Promotion, Tutoring, and Funding</title>
      <link>http://www.json.blog/2012/02/social-promotion-tutoring-and-funding/</link>
      <pubDate>Wed, 15 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/02/social-promotion-tutoring-and-funding/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Social_promotion&#34;&gt;Social promotion&lt;/a&gt;, in education circles, refers to the practice of allowing students to move on to the next grade level or course even though they are unable to demonstrate they have mastered the skills and knowledge they were expected to learn. Ending or reducing social promotion has been a major theme in the standards-based education reform of the last 10-15 years. Ending social promotion feels like a sound, obvious consequence of standards-based education. Each year (or course) comes with set of standards that articulate what students must know and be able to do once complete. Since the standards of the following course will assume proficiency on previous standards, there is a fundamental common sense to prohibiting students to move to the next level before they have conquered all prior levels.&lt;/p&gt;

&lt;p&gt;In reality, this is a gross oversimplification bordering on &lt;em&gt;reductio ad absurdum&lt;/em&gt;. Allow me to throw a few wrinkles into the carrion calls for social promotion&amp;rsquo;s demise. First, some standards do not come packaged with lofty presumptions of prior knowledge or skills. For example, a student could be quite successful in a high school chemistry or physics course without being successful in biology. In fact many students take these three courses in a sequence which explicitly prevents taking advantage of the natural interrelatedness of these sciences&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. For sure there are some skills that serve as critical gateways to future standards and expectations, but a student may fail a course while still having all of the core scaffolding in place for the next course level. Second, it is unclear that repeating a class (or entire grade-level) is an effective mechanism to successfully attain acceptable achievement. What proportion of content that will be repeated has a student already successfully learned without need for reinforcement? Are the strategies and pedagogyÂ employed to teach students new material the same as those used to re-teach material? I&amp;rsquo;m doubtful. Then there are behavioral and social concerns. What are the impacts on a student&amp;rsquo;s self-esteem? What are the impacts, particularly in elementary schools, of mixing students at even greater age ranges? If students mustÂ relearn content, reread the same books, etc, what will happen to their level of engagement with the material? What is the impact of isolating students from their friends in a way that might feel like punishment?&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s research on both sides of this issue&amp;ndash; some that demonstrates that students who are held back &lt;a href=&#34;http://www.mitpressjournals.org/doi/pdf/10.1162/edfp.2007.2.4.319&#34;&gt;do better academically&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and some that show outcomes are no better or worse. The impact on the socio-emotional side also seems mixed, although there is more consensus around negative consequences for being held back. That being said, much of the research I have read on social promotion looks at all students being held back in various contexts and not specifically examining long-term effects within a large-scale implementation that regularizes the process, which perhaps decreases the social stigma of being held back and increases the efficacy of teachers with held back students. I am having a hard time remembering at the moment, but I can&amp;rsquo;t recall a large-scale study that used regression discontinuity (and instrumental variable) like the previously linked Jay P. Greene Florida study that took a robust look at socio-economic outcomes. Less rigorous methodologies may introduce substantial bias to results&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;That being said, I generally think that social promotion is not ideal. In a perfect world, principals and district leaders would have a better sense of how well teachers are able to differentiate instruction in their classrooms more precisely. This way, they could adequately determine when a student is so far behind expectations that it is unreasonable to expect teachers toÂ deliver the necessary instruction in a mixed-ability classroom. When a student is that far behind at the end of the school year, targeted summer intervention would attempt to bring a student to within an acceptable range by the start of the next school year. If this fails, then, and only then, should a student be held back.&lt;/p&gt;

&lt;p&gt;None of this is new. In fact, Wikipedia led me to an &lt;a href=&#34;http://www2.ed.gov/pubs/socialpromotion/intro.html&#34;&gt;article about social promotion&lt;/a&gt; archived on the US Department of Education webpage from May 1999 that is strikingly similar to everything I wrote above&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Emily Richardson has a great post inÂ &lt;a href=&#34;http://www.theatlantic.com/national/archive/2012/02/closing-the-literacy-gap-the-trouble-with-holding-back-students/253065/&#34;&gt;The Atlantic&lt;/a&gt;Â &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;about an intriguing alternative. &lt;a href=&#34;https://webapp4.asu.edu/directory/person/28597&#34;&gt;David Berliner&lt;/a&gt;, an Arizona State University professor of education, says,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Everybody supports the idea that if a student isn&amp;rsquo;t reading well in
third grade that it&amp;rsquo;s a signal that the child needs help. If you hold
them back, you&amp;rsquo;re going to spend roughly another \$10,000 per child
for an extra year of schooling. If you spread out that \$10,000 over
the fourth and fifth grades for extra tutoring, in the long run you&amp;rsquo;re
going to get a better outcome.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There has recently been an increase in &lt;a href=&#34;http://www.economics.harvard.edu/faculty/fryer/files/charter_school_strategies.pdf&#34;&gt;evidence&lt;/a&gt; about the efficacy of intensive, very small group (like two-on-one) tutoring at raising academic achievement based on a &amp;ldquo;successful replication&amp;rdquo; of the &lt;a href=&#34;http://www.matchschool.org/&#34;&gt;MATCH Charter School&amp;rsquo;s&lt;/a&gt; tutoring program in the &lt;a href=&#34;http://www.houstonisd.org/&#34;&gt;Houston Independent School District&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;http://www.houstonisd.org/HISDConnectDS/v/index.jsp?vgnextoid=436bcd7298b69210VgnVCM10000028147fa6RCRD&#34;&gt;Apollo 20&lt;/a&gt; Program&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. Intensive one-on-one support has several key advantages that address some of my &amp;ldquo;wrinkles&amp;rdquo; about social promotion above. The instruction is specifically catered toward the exact standards that a student is weak on (and possibly on standards that are more foundational to future learning). The strategies and techniques used to teach a student may not be the same as whole-classroom, first-time exposure learning. The intervention strategy feels more like a surplus than deficit-driven policy, i.e. students are receiving *more*Â because of their achievement not being &amp;ldquo;punished&amp;rdquo;. And that is not remotely an exhaustive list.&lt;/p&gt;

&lt;p&gt;The problem is that education funding is not structured to spend the way that Berliner is recommending. Revenues are often raised or doled out on a per pupil basis so holding a student for an extra year will virtually automatically result in additional formula-driven dollars&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. There is no way to flag a student as needing the &amp;ldquo;5th-year high school&amp;rdquo; funding now, in the form of two years of intensive, one-on-one tutoring in elementary or middle school. I am not sure I think that&amp;rsquo;s a good thing, because I really do think that Berliner is on to something here. The cost-effectiveness of the total investment in any one student identified as being at risk of falling seriously behind is likely to be far higher providing a huge influx of resources to be used entirely on individualized intervention rather than offering an entire extra year of education overall and repeating a specific grade.&lt;/p&gt;

&lt;p&gt;The logistics of supporting this kind of intervention with anything but local or private revenue is causing my brain to do mental gymnastics, but the complexity might really be worth the benefits.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;An aside for another day.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;ending social promotion based on standardized assessments offers pretty much a textbook example of regression discontinuity studies, which I think is kind of cool
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Although woefully outdated and using a source from 1986, the &lt;a href=&#34;http://en.wikipedia.org/wiki/Grade_retention#Research&#34;&gt;research section of the Wikipedia page on grade retention&lt;/a&gt;Â outlines this in effective and simple language
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;I did find the article after I wrote this post
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;And on her blog, The Educated Reporter
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Of course there is lot of past research to support this idea, but I do think that &lt;a href=&#34;http://www.economics.harvard.edu/faculty/fryer&#34;&gt;Fryer&lt;/a&gt;&amp;rsquo;s Apollo 20 evaluation has reinvigorated discussion around the effectiveness of this type of intervention in the past several months.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;In theory, at least. Of course many states are reducing their state aid formula in funky ways and there are loopholes in most maintenance of effort laws that are now being rigorously used toÂ allow for per pupil decreases in local revenues
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Worth It Five Stories from the Last Week</title>
      <link>http://www.json.blog/2012/02/worth-it-five-stories-from-the-last-week/</link>
      <pubDate>Thu, 09 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/02/worth-it-five-stories-from-the-last-week/</guid>
      <description>

&lt;p&gt;I read literally hundreds of posts from RSS feeds every day. I use &lt;a href=&#34;http://reader.google.com&#34;&gt;Google Reader&lt;/a&gt; as an aggregator, &lt;a href=&#34;http://reederapp.com&#34;&gt;Reeder&lt;/a&gt; to actually read through my feeds, and &lt;a href=&#34;http://www.pinboard.in/u:jasonpbecker&#34;&gt;Pinboard&lt;/a&gt; for social bookmarking and posting&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In order to capture just a small slice of the stories I really enjoyed, I&amp;rsquo;ve decided to start a new feature called &amp;ldquo;Worth It&amp;rdquo;. I hope this will be between 3 and 5 stories each week with some quick commentary that I think are worth anyone&amp;rsquo;s time to read. This week I&amp;rsquo;ve thrown together five stories kind of haphazardly, but in the future I hope these posts will lean toward highlighting longer features or reports as opposed to more blog or typical article-length pieces.&lt;/p&gt;

&lt;p&gt;Feel free to use the comment section to recommend some stories that were &amp;ldquo;Worth It&amp;rdquo; from the last week that I may have missed.&lt;/p&gt;

&lt;h2 id=&#34;from-school-facing-turnaround-a-tale-of-academic-perseverance&#34;&gt;&lt;a href=&#34;http://gothamschools.org/2012/02/07/from-school-facing-turnaround-a-tale-of-academic-perseverance/&#34;&gt;From school facing turnaround, a tale of academic perseverance&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The first &amp;ldquo;Worth It&amp;rdquo; piece is a great &lt;a href=&#34;http://www.gothamschools.org&#34;&gt;Gotham Schools&lt;/a&gt; piece about a student who was nearly lost between the cracks in the New York City school system, doomed to a tough life by coincidence, mishap, and possibly negligence. Unlike most students faced with such abject systematic failure, Moustafa Elhanafi&amp;rsquo;s story has a happy ending. Although he found himself illiterate and with no prospects at 18, he is now set on a course to graduate with his high school diploma ready for college by the time he is 21. Elhanafi was born in the United States but lived in Egypt with his mother from age 2 until age 8. At 8 years old, he moved back to New York City and lived with his father in Queens. When he was 11, the NYC had so totally failed him that they misdiagnosed him with mental retardation. The article hints at several reasons this calamity of errors may have occurred. Elhanafi was an English language learner, which can challenge the typical screening methods that trained social workers, psychologists, etc have at their disposable. He is described as shy and at times, withdrawn. It&amp;rsquo;s quite possible that Elhanafi suffers from one or more learning disabilities and/or other unique psychosocial abnormalities, but it is also abundantly clear that being quarantined in programs designed for students with severe and profound special needs was no help. I strongly recommend you read this story and find out more about just what it takes to educate a student like Elhanafi. Without giving too much away, I&amp;rsquo;ll just say that this is an uplifting story that shows how much compassion and dedication, from teachers, parents, and students can accomplish.&lt;/p&gt;

&lt;h2 id=&#34;why-pay-for-intro-textbooks&#34;&gt;&lt;a href=&#34;http://www.insidehighered.com/news/2012/02/07/rice-university-announces-open-source-textbooks&#34;&gt;Why Pay for Intro Textbooks?&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.rice.edu/&#34;&gt;Rice University&lt;/a&gt; is admirably seeking to tackle the textbook publishing industry the right way&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Producing free, open source, traditional, peer-reviewed textbooks for the post-secondary market is well worth the investment. The internet may have democratized content creation, search may have increased the relevance of the sea of materials, and social media may have helped to curate quality out of the still massive relevant web. None of these are a substitute for true expertise subjected to a robust revision and editorial process on the road to peer expert approval. &lt;a href=&#34;http://en.wikipedia.org&#34;&gt;Wikipedia&lt;/a&gt; is one of the few corners of the web to get quality right, but the mental model users have when in an encyclopedia is perusal; there is no way to clearly stake out a path through Wikipedia to thoroughly learn a set body of knowledge. Textbooks offer an organizational framework that brings clarity, context, and connectivity to the information.&lt;/p&gt;

&lt;h2 id=&#34;charter-advocates-claim-rules-in-works-would-affect-pensions&#34;&gt;&lt;a href=&#34;http://blogs.edweek.org/edweek/campaign-k-12/2012/02/charter_advocates_say_federal.html&#34;&gt;Charter Advocates Claim Rules in Works Would Affect Pensions&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This is a real wonky one. Essentially the IRS is tackling with how we define a government employee. The way some proposed rules are now written, it&amp;rsquo;s possible that charter school teachers will not be considered &amp;ldquo;government employees&amp;rdquo;. As a result, their inclusion in government pension systems can jeopardize several special protections because the systems will no longer be considered public. Virtually all states allow charter school teachers to participate in state plans, and a few, including Rhode Island, require that all charter school teachers take part in the state operated teacher pension system. There are several excellent reasons for this policy, even if it costs charter schools more money than they might like. First, because pension benefits are a major form of compensation for teachers and they accrue with experience, participation in a state pension system serves to immobilize the teacher labor force. In fact, most states centrally operate their pension systems specifically to allow teachers to move across schools and districts without sacrificing their pensions. This is desirable if we want more efficient labor market sorting since optimal sorting requires minimal (and preferably negligent) transaction costs. Charter schools want the option to draw from current public school teachers and their ability to do so is greatly limited if benefits that have accrued over the course of a career are lost or severely diminished due to transitioning into a charter school.&lt;/p&gt;

&lt;p&gt;I am not at all sympathetic to the notion that charter schools are not public schools. Although we might debate the extent to which they are democratic&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, they are clearly public entities. They supply a public good entirely through taxpayer dollars with almost all the financial accountability (and sometimes more) requirements of traditional public schools. All federal public education laws and regulations apply to these schools as do the majority of state law and regulation (in most instances). Charter schools are public schools. But because charter school employees are technically directly accountable to a board that is typically not democratically elected, it is apparently debatable whether or not they are government employees. This seems odd to me. I work for the Rhode Island Department of Education and I am clearly considered a state employee. Yet my employer is a Commissioner of Education who is hired by the Board of Regents. The Board of Regents is an appointed body, not democratically elected. So while they may, in so ways, be directly accountable to elected officials, it&amp;rsquo;s a long way off to find direct democratic accountability for my position. In many ways, charter school employees have far fewer layers between them and the public, yet my government employee status would never come into question.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;I use &lt;a href=&#34;http://www.ifttt.com&#34;&gt;IFTTT&lt;/a&gt; triggers based on Pinboard tags to post to &lt;a href=&#34;http://www.twitter.com/jasonpbecker&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;http://tumblr.jasonpbecker.com&#34;&gt;Tumblr&lt;/a&gt;, &lt;a href=&#34;http://www.facebook.com/jason.p.becker&#34;&gt;Facebook&lt;/a&gt;, etc
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Apple&amp;rsquo;s iBooks Author is awful in comparison. Bringing what are essentially web page authoring tools to the masses and wrapping the materials in a proprietary shell is just awful on so many levels. This strategy just shifts the cost from individual books to the devices the books need to run on. These devices have a lifespan that&amp;rsquo;s shorter than a typical textbook and are very expensive.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;using Sarah Goldrick-Rab&amp;rsquo;s recently offered definition, which described democratic to me as the extent to which stakeholders directly participate in institutional governance and decision-making
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bank of America will leave 111 Westminster</title>
      <link>http://www.json.blog/2012/01/bank-of-america-will-leave-111-westminster/</link>
      <pubDate>Tue, 31 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/bank-of-america-will-leave-111-westminster/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.flickr.com/photos/34463371@N08/&#34;&gt;&lt;img src=&#34;http://www.json.blog/img/111west.jpg&#34; alt=&#34;111 Westminster lit at night by Flickr user kehuston&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I was pretty disappointed, but not surprised, that Bank of America &lt;a href=&#34;http://www.pbn.com/Bank-of-America-to-vacate-Superman-building-,64983&#34;&gt;has chosen to leave&lt;/a&gt; 111 Westminster Street. The building is an iconic anchor to downtown Providence. Unfortunately, this space has not been properly refurbished to more modern standards. The entire building has a single, antiquated utilities system&amp;ndash; heating, cooling, electricity, etc are all setup for a single tenant. Weighing in at 350,000 square feet, there is simply no one in Providence who needs an old, out of date workspace of that size. Renovations, at this point, are likely to be very expensive, although &lt;a href=&#34;http://www.theatlanticcities.com/housing/2012/01/why-most-environmental-building-building-weve-already-built/1016/&#34;&gt;environmental advocates&lt;/a&gt; and &lt;a href=&#34;http://chronicle.com/blogs/buildings/costume-jewelry-factory-will-become-browns-medical-school-saving-35-million/26293&#34;&gt;businessmen&lt;/a&gt; alike should unite around refurbishing over &lt;a href=&#34;http://news.providencejournal.com/breaking-news/2011/11/21/195_redevelopment.JPG&#34;&gt;new construction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Downtown Providence &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:downcity&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:downcity&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; has no shortage of &lt;a href=&#34;http://www.pbn.com/2011-turning-point-for-commercial-real-estate,63603?category_id=79&amp;amp;sub_type=stories,packages&#34;&gt;vacant space&lt;/a&gt;. Saving 111 Westminster is going to take creative thinking and substantial investment. Fortunately, there is no better time since the &lt;a href=&#34;http://www.gcpvd.org/2012/01/20/the-arcade-are-you-kidding-me-with-this/&#34;&gt;Arcade is undergoing some exciting changes&lt;/a&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:arcade&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:arcade&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Because of the near perfect alignment of 111 Westminster and the Arcade,
I&amp;rsquo;d love to see some true, deep collaboration that rethinks the area on
Westminster Street between Exchange and Dorrance Streets &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:dorrance&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:dorrance&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Activating this space at the street level will be a huge boon to the Arcade, 111 Westminster, and the success of Downtown as a whole. I&amp;rsquo;ve got a handful of ideas, but I lack the skills to make beautiful pictures to make my visions tangible.&lt;/p&gt;

&lt;p&gt;So I&amp;rsquo;m going to just have to hope that &lt;a href=&#34;http://www.gcpvd.org&#34;&gt;Greater City: Providence&lt;/a&gt; recognizes this as the opportunity for a new &lt;a href=&#34;http://www.gcpvd.org/category/features/reboot/&#34;&gt;Reboot&lt;/a&gt;, my personal favorite work on the site. In their words,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;REBOOT&lt;/strong&gt; is an occasional series of posts on &lt;em&gt;Greater City:
Providence&lt;/em&gt; where we identify areas of the city that display poor
urbanism and propose ways to improve them. Our interventions may be
simple and quite easily realized, or they may at times be grand and
possibly take years or decades to complete. Either way, we hope they
generate interest and discussion.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Past Reboot&amp;rsquo;s have featured the &lt;a href=&#34;http://www.gcpvd.org/2011/06/08/reboot-providence-train-station/&#34;&gt;Providence Train Station&lt;/a&gt; and &lt;a href=&#34;http://www.gcpvd.org/2010/08/17/reboot-olneyville-square/&#34;&gt;Olneyville Square&lt;/a&gt;, among others. Let&amp;rsquo;s Reboot 111 Westminster and the Arcade, and the whole area east of Dorrance &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:110west&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:110west&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The original title of this post was unintentionally, overly similar to GCPVD&amp;rsquo;s post on this issue. I recognized this oversight independently and have edited the title.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:downcity&#34;&gt;I am desperately trying to drop &amp;ldquo;Downcity&amp;rdquo; in favor of &amp;ldquo;Downtown&amp;rdquo; after training myself to say Downcity.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:downcity&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:arcade&#34;&gt;I know I&amp;rsquo;ve been cranky about the Arcade plans, but I think that&amp;rsquo;s more about my general mood than the merits of the redevelopment. It could work, and if it does, it will be very exciting for Downtown.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:arcade&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:dorrance&#34;&gt;I&amp;rsquo;d love to see traffic closed along both Weybosset and Westminster from Memorial Boulevard down to Dorrance.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:dorrance&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:110west&#34;&gt;And 110 Westminster, while we&amp;rsquo;re at it, the massive condo tower-turned parking lot
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:110west&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Providence Pensions-- Let&#39;s Call a Spade a Spade (or the COLA a Raise)</title>
      <link>http://www.json.blog/2012/01/providence-pensions---lets-call-a-spade-a-spade-or-the-cola-a-raise/</link>
      <pubDate>Wed, 25 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/providence-pensions---lets-call-a-spade-a-spade-or-the-cola-a-raise/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/#!/tednesi&#34;&gt;Ted Nesi&lt;/a&gt; has done a pretty solid job tracing the &lt;a href=&#34;http://www.wpri.com/dpp/news/local_news/providence/prov-pensions-hit-by-comedy-of-errors?2&#34;&gt;history&lt;/a&gt; of some awful decisions made by union-dominated boards that resulted in a significant number of retirees in the early-90s receiving 5% or 6% annually compounded interest on their retirement income. These are often called COLAs, or &lt;a href=&#34;http://en.wikipedia.org/wiki/Cost-of-living_adjustment&#34;&gt;cost-of-living adjustments&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today, I am inspired by Nesi&amp;rsquo;s &lt;a href=&#34;http://blogs.wpri.com/2012/01/25/chart-the-decline-and-fall-of-the-providence-pension-system/&#34;&gt;post&lt;/a&gt; on the rapid decline of the Providence municipal pension fund health that occurred since 6% &amp;ldquo;COLA&amp;rdquo; was introduced in 1989 through today. You see, something has really been bugging me about the conversation on municipal pensions in Rhode Island. A true COLA is key to ensuring that purchasing power is maintained throughout retirement. Essentially, quality of life and ability to buy required goods should be consistent from the day you retire until the day you die. This is a goal that makes a lot of sense. But the cost of goods has not increased 5% or 6% year-over-year ever in the past twenty years &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;So I chose a key moment in the history of Providence municipal pensions&amp;ndash; a 1991 consent decree &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:decree&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:decree&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; that then Mayor Buddy Cianci signed, solidifying and legitimizing the extremely high &amp;ldquo;COLA&amp;rdquo; for workers. I wanted to know, &amp;ldquo;What would a worker retiring in the following year (1992) be making today if they retired with a \$25,000 annual pension and had a 6% &amp;lsquo;COLA&amp;rsquo;, 5% &amp;lsquo;COLA&amp;rsquo;, or a COLA based on the Northeast CPI-U?&amp;rdquo; Not wanting to make a key mistake and &lt;a href=&#34;http://en.wikipedia.org/wiki/Cost-of-living_adjustment#CPI_is_not_a_COLA&#34;&gt;equate a CPI with a COLA&lt;/a&gt;, I increased the CPI-U for each year by 25%, figuring that this is a reasonable approximation of the marginal taxes that would be paid on additional income by these retirees.&lt;/p&gt;

&lt;p&gt;I suspected that 5% and 6% do not really result in a cost-of-living adjustment, but rather a clear wage increase for retired workers. I have no problem maintaining parity or near-parity with retirement level income, but there&amp;rsquo;s absolute no reason someone who retired should receive a wage. My support for a true COLA is so strong that I made the adjustment for taxes on income!&lt;/p&gt;

&lt;p&gt;What were the results?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/inflation1.png&#34; alt=&#34;Amazing what compound interest does&#34; title=&#34;Comparing COLA to CPI-U&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A Providence employee who retired in 1992 with a \$25,000 pension would be receiving \$46,132 in 2011 if their retirement was increased by inflation + the marginal tax rate (assumed here as 25%). But a Providence employee who retired with the same pension in 1992 under the conditions in Providence could expect \$63,174 at 5% or \$75,640 at the 5% and 6% rates, respectively. This is a MASSIVE difference which cannot constitute a &amp;ldquo;COLA&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;So I move that we stop referring to these particular pensions as having a &amp;ldquo;COLA&amp;rdquo;, because what really happened was a fixed raise was created to last for the rest of retirees&amp;rsquo; lives.&lt;/p&gt;

&lt;p&gt;Some additional neat facts:&lt;/p&gt;

&lt;p&gt;Over 20 years, an individual who has a 6% raise per year will have collected \$228,672 more than someone who had a COLA. An individual with a 5% raise per year will have collected \$135,681.10 over the same 20 year period.&lt;/p&gt;

&lt;p&gt;And of course, here&amp;rsquo;s the code I used to produce the graph above in R&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;compound &amp;lt;- function(start,rate,timespan){
  x &amp;lt;- vector(mode = &#39;numeric&#39;, length = timespan)
  for(i in 1:timespan){
    if(i == 1){
      x[i] &amp;lt;- start
    }
    else{
          x[i] &amp;lt;- x[i-1]*(1+rate)
    }
  }
  return(x)
 }
    
inflate &amp;lt;- function(start, inflation){
  x &amp;lt;- vector(mode=&#39;numeric&#39;, length=dim(inflation)[1])
  for(i in 1:dim(inflation)[1]){
    if(i==1){
      x[i] &amp;lt;- start
    }
    else{
      x[i] &amp;lt;- x[i-1]*(1+(1.25*(inflation[i,2]/100)))
    }
  }
  return(x)
}

cpiu &amp;lt;- cbind(seq(from=1992,to=2011), c(0.0, 2.8, 2.4, 2.6, 2.8, 2.4, 1.4, 
                                        2.1, 3.4, 2.8, 2.1, 2.8, 3.5, 3.6,
                                        3.6, 2.6, 4.0, 0.0, 2.0, 3.0))

inflation &amp;lt;- data.frame(cbind(cpiu[,1], inflate(25000, cpiu), 
                              compound(25000, .05, 2), 
                              compound(25000, .06, 20)))

names(inflation) &amp;lt;- c(&#39;year&#39;, &#39;NECPI.U&#39;, &#39;FivePercent&#39;, &#39;SixPercent&#39;)
png(filename=&amp;quot;inflation.png&amp;quot;, height=640, width=800, bg=&amp;quot;white&amp;quot;)
par(mar=c(6, 5, 5, 3))
plot(inflation$NECPI.U, type=&#39;o&#39;, col=rgb(0,0.5,0), ylim=c(20000,80000), 
     axes=FALSE, ann=FALSE, lwd=1.5)
axis(1, at=1:20, lab=inflation$year)
axis(2, las=1, at=seq(from=20000, to=80000, by=10000))
lines(inflation$FivePercent, type=&amp;quot;o&amp;quot;, pch=22, lty=2, col=rgb(0,0,0.5), 
      lwd=1.5)
lines(inflation$SixPercent, type=&amp;quot;o&amp;quot;, pch=23, lty=2, col=&#39;red&#39;, lwd=1.5)
title(main=&amp;quot;COLA or Raise?\n CPI-U v. Pension COLAs in Providence&amp;quot;, col.    main=&amp;quot;black&amp;quot;)
title(xlab=&amp;quot;Year&amp;quot;)
title(ylab=&amp;quot;Annual Pension in Dollars\n&amp;quot;)
legend(1, 80000, c(&#39;CPI-U NE + 25%&#39;, &#39;Five Percent&#39;, &#39;Six Percent&#39;), col=c(   &#39;green&#39;, &#39;blue&#39;, &#39;red&#39;), pch=21:23, lty=1:3)
text(1,25000, 25000, pos=3, col=&#39;black&#39;)
text(20, max(inflation$SixPercent), round(max(inflation$SixPercent), 0), pos=3,     col=&#39;red&#39;)
text(20, max(inflation$FivePercent), round(max(inflation$FivePercent), 0)   ,pos=3, col=rgb(0,0,0.5))
text(20, max(inflation$NECPI.U), round(max(inflation$NECPI.U), 0), pos=3,     col=rgb(0,0.5,0))
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;This post reflects my personal views and opinions. I am a member of Local 2012 of the RIAFT and was a supporter of the statewide pension reform in the Fall of 2011. I am also a resident of Providence.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;http://www.bls.gov/ro1/9140.htm&#34;&gt;Consumer Price Index Northeast from the Bureau of Labor Statistics&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:decree&#34;&gt;See the first link in this post
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:decree&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Sorry this code is not well-commented, but I believe it&amp;rsquo;s fairly straight forward
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Providence Real Estate Sales in R</title>
      <link>http://www.json.blog/2012/01/providence-real-estate-sales-in-r/</link>
      <pubDate>Mon, 23 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/providence-real-estate-sales-in-r/</guid>
      <description>&lt;p&gt;The past few months I&amp;rsquo;ve been learning how to use R. This morning, I decided to try out two first&amp;ndash; importing a table of data that is being read of the web and overlaying location data onto a map.&lt;/p&gt;

&lt;p&gt;With a little bit of Google skills and just enough R know-how I was able to produce this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/homesales.png&#34; alt=&#34;Providence Home Sales 9-12-11 to 12-27-11&#34; title=&#34;Providence Home Sales 9-12-11 to 12-27-11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There were a few things that were kind of tricky for me. First, for sometime I couldn&amp;rsquo;t get latitude and longitude components for the addresses. I figured there was something wrong with the way I was using the *apply class of functions in R. apply() (and the related class of functions lapply, sapply, etc.) are really handy if a bit tricky for beginning R users. This function permits quickly &amp;ldquo;applying&amp;rdquo; a function across multiple elements. Traditionally this is done with a loop, but the apply() functions &amp;ldquo;vectorize&amp;rdquo; this process (R folks always talk about making your code more vectorized which has something to do with the structure of objects in R but is beyond my computer science skills&amp;ndash; essentially, vectorized code runs much faster and more efficiency than loops because of some underlying feature of the language). After playing around with apply, lapply, and sapply, I decided to move back into my &amp;ldquo;old&amp;rdquo; way of thinking and just write a loop:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;latlongroll &amp;lt;- function(address){
  lat &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
  lng &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
  for(i in 1:length(address)){
    latlong &amp;lt;- gGeoCode(address[i])
    lat[i]&amp;lt;-latlong[1]
    lng[i]&amp;lt;-latlong[2]
  }
  return(cbind(lat,lng))
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This still didn&amp;rsquo;t work&amp;ndash; I kept on getting a strange out-of-bounds call. So I decided to go down the rabbit hole of regular expressions and try and see if I could clean up my addresses any further (I couldn&amp;rsquo;t). So, now seemed as good a time as any to figure out how to print to the console while a loop is running to keep track of progress and where exactly my function was stopped. This turned out to be a bit tricky because I didn&amp;rsquo;t know you had to include a tricky line, &lt;code&gt;r flush.console()&lt;/code&gt; in order to get the prints to work. When I figured this out I found out my loop was being caught on my 7th element, a perfectly well formed address. When I ran gGeoCode() on that address only it worked fine. So I thought, maybe Google is bouncing me out because I&amp;rsquo;m hitting it too fast? And bingo, the final (working version):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;latlongroll &amp;lt;- function(address){
 lat &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
 lng &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
 for(i in 1:length(address)){
  print(i)
  flush.console()
  latlong &amp;lt;- gGeoCode(address[i])
  lat[i]&amp;lt;-latlong[1]
  lng[i]&amp;lt;-latlong[2]
  Sys.sleep(0.5)
 }
 return(cbind(lat,lng))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other than that, the whole process was pretty straight forward. I have to thank Tony Breyal for &lt;a href=&#34;http://stackoverflow.com/questions/3257441/geocoding-in-r-with-google-maps&#34;&gt;posting the functions I used&lt;/a&gt; to get latitude and longitude on &lt;a href=&#34;http://stackoverflow.com/&#34;&gt;Stack Overflow&lt;/a&gt;. Also, I found the &lt;a href=&#34;http://cran.r-project.org/web/packages/RgoogleMaps/vignettes/RgoogleMaps-intro.pdf&#34;&gt;RgoogleMaps vignette&lt;/a&gt; to be very helpful, although I wish it had slightly better explained what was going on in qbbox().&lt;/p&gt;

&lt;p&gt;Finally, my full source for the above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Providence Real Estate Transactions over the last 40 days.
# Required Packages
require(&#39;XML&#39;)
require(&#39;RCurl&#39;)
require(&#39;RJSONIO&#39;)
require(&amp;quot;RgoogleMaps&amp;quot;)
# Functions
# Construct URL required to get the Lat and Long from Google Maps
construct.geocode.url &amp;lt;- function(address, return.call = &amp;quot;json&amp;quot;, sensor = &amp;quot;false&amp;quot;   ) {
 root &amp;lt;- &amp;quot;http://maps.google.com/maps/api/geocode/&amp;quot;
 u &amp;lt;- paste(root, return.call, &amp;quot;?address=&amp;quot;, address, &amp;quot;&amp;amp;sensor=&amp;quot;, sensor, sep = &amp;quot;&amp;quot;   )
 return(URLencode(u))
}
# Now that we have the proper Google Maps address, get the resulting latitude     and longitude
gGeoCode &amp;lt;- function(address) {
 u &amp;lt;- construct.geocode.url(address)
 doc &amp;lt;- getURL(u)
 x &amp;lt;- fromJSON(doc,simplify = FALSE)
 lat &amp;lt;- x$results[[1]]$geometry$location$lat
 lng &amp;lt;- x$results[[1]]$geometry$location$lng
 return(c(lat, lng))
}
# Roll through addresses to create lat long
latlongroll &amp;lt;- function(address){
# Initializing the length of a vector dramatically speeds up the code. Far 
# better than reassigning and resizing each time in the loop.
 lat &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
 lng &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = length(address))
 for(i in 1:length(address)){
# I kept the print in because this function takes a long time to run so I 
# like  to watch its progress.
 print(i)
 flush.console()
# To reduce the calls, I chose to store lat and long locally before 
# separating the two whereas initially I hit Google for each separately
 latlong &amp;lt;- gGeoCode(address[i])
 lat[i]&amp;lt;-latlong[1]
 lng[i]&amp;lt;-latlong[2]
# I&#39;ll have to experiment with the sleep time. I&#39;m certain 0.5 seconds is 
# too long (and this is the bulk of the time spent on the whole code).
 Sys.sleep(0.5)
 }
 return(cbind(lat,lng))
}

# Open to the most recent real estate transactions for Providence on the 
# Projo
site &amp;lt;- &#39;http://www.providencejournal.com/homes/real-estate-transactions/assets/pages/real-estate-transactions-providence.htm&#39;
# Read in the table with the header as variable names.
realestate.table&amp;lt;-readHTMLTable(site,header=T,which=1,stringsAsFactors=F)
# Remove the $ sign before the price
realestate.table$Price &amp;lt;- gsub(&amp;quot;([$]{1})([0-9]+)&amp;quot;, &amp;quot;\\2&amp;quot;, 
                               realestate.table$Price)
# Cast price character as numeric
realestate.table$Price&amp;lt;-as.numeric(realestate.table$Price)
# Cast date string as date type (lowercase %y means 2-digit year, 
# uppercase is 4 digit)
realestate.table$Date &amp;lt;- as.Date(realestate.table$Date,format=&#39;%m/%d/%y&#39;)
# Dummy transactions or title changes have a price of $1, removing those 
# from data set
providence &amp;lt;- subset(realestate.table,Price&amp;gt;1)
# Removing properties that do not have an address that start with a street 
# number
providence &amp;lt;- subset(providence, grepl(&amp;quot;^[0-9]+&amp;quot;, providence$Address))
# Add lat and lng coordinates to each address
providence&amp;lt;-cbind(providence, latlongroll(providence[,3]))
# Calculate boundary lat and long for map
bb &amp;lt;- qbbox(providence$lat, providence$lng)
# Gets a map from Google Maps
map &amp;lt;- GetMap.bbox(bb$lonR, bb$latR, zoom=12, maptype=&amp;quot;mobile&amp;quot;)
# plot the points
PlotOnStaticMap(map,lon=providence$lng,lat=providence$lat)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>What if the technology revolution in schools is really about the simple stuff?</title>
      <link>http://www.json.blog/2012/01/what-if-the-technology-revolution-in-schools-is-really-about-the-simple-stuff/</link>
      <pubDate>Wed, 18 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/what-if-the-technology-revolution-in-schools-is-really-about-the-simple-stuff/</guid>
      <description>&lt;p&gt;The complicated school day is essentially designed so the minimum number of staff are away from kids at any given time. Some folks are trying to combat this with &lt;a href=&#34;http://www.allthingsplc.info/pdf/articles/make_time_for_collaboration.pdf&#34;&gt;common planning time&lt;/a&gt; and other &lt;a href=&#34;http://www.erstools.org/Dream/district_es_a.cfm&#34;&gt;scheduling gymnastics&lt;/a&gt;. These attempts are up against a strong opposing priority&amp;ndash; students must be with an adult essentially 100% of the time. Not only that, but the middle of a hectic day focused on teaching is not really conducive to reflection, strategizing, and deep planning on a tight schedule. The temptation to use this precious time to put out the days (or weeks) fires instead of the kind of collaboration and professional learning desired is just too strong.&lt;/p&gt;

&lt;p&gt;And quite honestly, I think teachers should get space in the day to vent, grade papers, setup their classroom, call parents, and do all the normal &amp;ldquo;maintenance&amp;rdquo; required to keep their classes running. The question then remains, how do we create this dynamic space for professional learning, coordination of services, collaboration on lesson delivery, creative thinking about school structures, etc?&lt;/p&gt;

&lt;p&gt;I really think that &lt;a href=&#34;http://www.storiesfromschool.org/travis.html&#34;&gt;Travis&lt;/a&gt; on &lt;a href=&#34;http://www.storiesfromschool.org/&#34;&gt;Stories from School&lt;/a&gt; has it right: &lt;a href=&#34;http://www.storiesfromschool.rg/2012/01/not-meeting.html&#34;&gt;use technology to make it easy for teachers, administrators, and other staff to communicate and coordinate&lt;/a&gt;. There is no shortage of snake oil peddled to &amp;ldquo;solve&amp;rdquo; education in America and one of the most persistent memes is the technological revolution will alter classrooms forever. Technology&amp;rsquo;s real promise is in *schools* and not &lt;em&gt;classrooms&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Sharing assessment results and lesson plans, coordinating with interventions being offered to a student, talking to other teachers who have or have had the same student, and more can all be made much easier with technology. Rather than finding the time to meet face-to-face, faculty members can put energy into building relationships around teaching and learning when they have the time. The opportunity to breakaway the time constraints typically placed on synchronous conversation is huge. The opportunity for rich asynchronous sharing is virtually brand new.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;I do not want this to sound like pushing for &amp;ldquo;social media&amp;rdquo; for teachers, mostly because the technological innovations involved are not a part of the current flavor of Web 2.0 networking. Even the old tools like email, instant messaging, and perhaps the oldest social tool of all, discussion boards, could be extremely helpful for folks.&lt;/p&gt;

&lt;p&gt;None of this is new, most of this has been said, yet it seems like too few schools have found a way to leverage existing and inexpensive technology to implement this kind of communication as an essential part of work culture. That seems like a massive missed opportunity that should not be lost while district officials are distracted by quick-and-dirty 4 week online courses for credit recovery.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;There are several reasons I&amp;rsquo;m not optimistic on technology revolutionizing the classroom. My preferred explanation is that the *technology of learning has not changed, *not to be confused with the technology of human machines, but instead the technology of *the* human machine. Human learning is no different from it was in the past so our technology can only promise new delivery mechanisms for the same thousands of years old approach to teaching and learning. Education can gain efficiencies in delivery that are not to be underestimated. But I think of a revolution as changing a process so dramatically that an observer would barely recognize the process a decade later. The computers and the internet have certainly done that in some fields, but I don&amp;rsquo;t see this happening in schools.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Ok, so I guess teachers could leave a flyer in mailboxes in the past, but come on
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Value-Added on Core Knowledge Blog-- some thoughts on Chetty, Friedman, and Rockoff</title>
      <link>http://www.json.blog/2012/01/value-added-on-core-knowledge-blog---some-thoughts-on-chetty-friedman-and-rockoff/</link>
      <pubDate>Fri, 13 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/value-added-on-core-knowledge-blog---some-thoughts-on-chetty-friedman-and-rockoff/</guid>
      <description>&lt;p&gt;Jessica Lahey wrote an &lt;a href=&#34;http://blog.coreknowledge.org/2012/01/12/what-is-the-value-in-a-high-value-added-teacher/&#34;&gt;interesting post&lt;/a&gt; over on &lt;a href=&#34;http://blog.coreknowledge.org/&#34;&gt;Core Knowledge Blog&lt;/a&gt; that I decided to comment on.&lt;/p&gt;

&lt;p&gt;After I read back my comment, I realized it would be worth copying over here as it&amp;rsquo;s own blog post.&lt;/p&gt;

&lt;p&gt;The most interesting part of the &lt;a href=&#34;http://www.economics.harvard.edu/faculty/chetty&#34;&gt;Chetty&lt;/a&gt;, &lt;a href=&#34;http://www.hks.harvard.edu/fs/jfriedm/&#34;&gt;Friedman&lt;/a&gt;, and &lt;a href=&#34;http://www0.gsb.columbia.edu/faculty/jrockoff/&#34;&gt;Rockoff&lt;/a&gt; &lt;a href=&#34;http://nber.org/papers/w17699&#34;&gt;study&lt;/a&gt; is precisely the most banal- teachers who improve heir students learning as measured by increased achievement on tandardized tests also improve other more distant and relevant factors n children&amp;rsquo;s lives &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This seems obvious to anyone who isn&amp;rsquo;t vehemently anti-testing. For a large group of the anti-testing regime, there is considerable skepticism that the standardized testing intruments being used by states is a valid instrument for the “real” purposes of education. In fact, the line of thinking in this post is a close relative to this critique. Essentially, what is mathematically reliable is not necessarily valid for drawing conclusions.&lt;/p&gt;

&lt;p&gt;CFR in a massive study essentially: 1) Added to a large research base that suggests that teachers can in fact have an impact on standardized test scores; 2) Demonstrate that the impact on standardized test scores are associated with broader, more distant, and, arguably, more important education outcomes; 3) These impacts are persistent throughout the lifetime of students.&lt;/p&gt;

&lt;p&gt;While it does NOT make a great case for teacher dismissal based solely on VAM, like the authors are essentially claiming in popular coverage, it does continue to strengthen the case that standardized tests are relevant, reliable, and meaningful indicators of a successful education system. The impacts on social outcomes (teen pregnancy) and economic outcomes (later earnings) show a broad range of important outcomes we expect from schools are strongly associated with VAMs.&lt;/p&gt;

&lt;p&gt;A good measure does not have to perfectly describe the intracacies of reality, it just has to give a rough, reasonable, and valid facsimile. CFR is just part of a growing tradition that shows there&amp;rsquo;s a good case for VAMs to be a part of that image.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Actually, I think that &lt;a href=&#34;http://schoolfinance101.wordpress.com/2012/01/07/fire-first-ask-questions-later-comments-on-recent-teacher-effectiveness-studies/&#34;&gt;Baker&lt;/a&gt;, &lt;a href=&#34;http://shankerblog.org/?p=4708&#34;&gt;Di Carlo&lt;/a&gt;, nd &lt;a href=&#34;http://shermandorn.com/wordpress/?p=4390&#34;&gt;Dorn&lt;/a&gt; are all probably right that the tests for biasness in VAMs or teachers are the most interesting part, but that&amp;rsquo;s purely from a geeky researcher perspective. I doubt that&amp;rsquo;ll have as much impact as ther portions of the paper, and probably rightfully so
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Associated Press: Israeli schools not looking so good against OECD peers.</title>
      <link>http://www.json.blog/2012/01/associated-press-israeli-schools-not-looking-so-good-against-oecd-peers./</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2012/01/associated-press-israeli-schools-not-looking-so-good-against-oecd-peers./</guid>
      <description>&lt;p&gt;I read an interesting &lt;a href=&#34;http://goo.gl/SSY83&#34;&gt;article&lt;/a&gt; this morning on Israeli
schools. Facing extreme poverty among Arab-Israeli&amp;rsquo;s and the
ultra-orthodox, Israel struggles to maintain three separate school
systems and succeed. It reminded me of some interesting centralized
policy reforms in Israel that have led to great natural experiments. For
example, so-called Maimonides Laws which capped class sizes at 40,
allowed for some really interesting regression-discontinuity studies on
the impact of class size &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:classsize&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:classsize&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;What I found most interesting in this article, however, was the point
made by Jon Medved:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;While agreeing Israeli schools need to raise their standards,
technology entrepreneur Jon Medved doesn&amp;rsquo;t think Israel&amp;rsquo;s test scores
tell the whole story. He says informal education, through the
military, youth movements, and extracurricular activities, builds
skills. He also praised programs for gifted children.&lt;/p&gt;

&lt;p&gt;Consequently, Medved says he isn&amp;rsquo;t worried that Israel&amp;rsquo;s tech-driven
economy will slide because of deficiencies in the school system.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;While I think it&amp;rsquo;s important to sound alarm signals, I haven&amp;rsquo;t heard
from tech companies &amp;hellip; &amp;lsquo;the employees we&amp;rsquo;re getting are not
educated,&amp;lsquo;&amp;rdquo; Medved said.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Two thoughts came to mind. First, we certainly are hearing in the United
States that employers are unhappy with the caliber of candidates for
open positions. It&amp;rsquo;s unclear that in the US this is a result of low
aggregate skills or just tremendous mismatch between the skills that are
attained and the skills that are now valued in the marketplace. Second,
Israel&amp;rsquo;s universal military service provides several additional years of
intense training and skill attainment even before college. Considering
the broad range of roles one can take on in the state military ((For
example, social workers in Israel are basically military trained, given
job experience, and then set out without a need for 5 years of
schooling, although I believe many do receive at least a bachelor&amp;rsquo;s)),
it&amp;rsquo;s hard not to see Israel&amp;rsquo;s mandatory service as a massive vocational
educational program.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t have much analysis here, but mandatory public/civil service is
an interesting concept that this article will keep me thinking about
over the weekend.&lt;/p&gt;

&lt;p&gt;Angrist has also done several other studies in Israel with Lavy, a nice
little summary of which is available here &lt;a href=&#34;http://www.nber.org/reporter/summer03/angrist.html&#34;&gt;http://www.nber.org/reporter/summer03/angrist.html&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:classsize&#34;&gt;&lt;a href=&#34;http://www.economics.harvard.edu/faculty/staiger/files/AngristLavy%2BQJE%2B1999.pdf&#34;&gt;http://www.economics.harvard.edu/faculty/staiger/files/AngristLavy%2BQJE%2B1999.pdf&lt;/a&gt;;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:classsize&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GrapheR is an awesome GUI for R beginners</title>
      <link>http://www.json.blog/2011/12/grapher-is-an-awesome-gui-for-r-beginners/</link>
      <pubDate>Mon, 19 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/12/grapher-is-an-awesome-gui-for-r-beginners/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re just beginning to use R and want a quick and easy way to make some charts/graphs, etc, GrapheR is a great package to quickly produce high quality plots through a self-explanatory GUI. &lt;a href=&#34;http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Herve.pdf&#34;&gt;Here&lt;/a&gt;&amp;rsquo;s an article
in The R Journal today.&lt;/p&gt;

&lt;p&gt;My only complaint is that GrapheR does not appear to have a way to export the code that produced the graph, which would be a very helpful feature for a beginner who wants to learn the guts of producing publication quality charts in R.&lt;/p&gt;

&lt;p&gt;To install and then run&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#39;GrapheR&#39;)
require(GrapheR)
run.GrapheR()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>House wants unemployed to earn GEDs against all economic sense</title>
      <link>http://www.json.blog/2011/12/house-wants-unemployed-to-earn-geds-against-all-economic-sense/</link>
      <pubDate>Wed, 14 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/12/house-wants-unemployed-to-earn-geds-against-all-economic-sense/</guid>
      <description>&lt;p&gt;Here is a doozy.&lt;/p&gt;

&lt;p&gt;What is the purpose of the GED? Is it a &lt;a href=&#34;http://en.wikipedia.org/wiki/Signalling_(economics)&#34;&gt;market signal&lt;/a&gt;, indicating your employability to hiring agents or does it follow a &lt;a href=&#34;http://en.wikipedia.org/wiki/Human_capital&#34;&gt;human capital&lt;/a&gt; path wherein earning a GED is actually a process that increases your skill set?&lt;/p&gt;

&lt;p&gt;The House wants all unemployed workers without a high school diploma &lt;a href=&#34;http://blogs.edweek.org/edweek/campaign-k-12/2011/12/bill_would_require_ged_to_tap.html&#34;&gt;to earn their GED&lt;/a&gt;. Their explanation is assuming that the GED works purely through human capital theory, e.g. currently unemployed workers who don&amp;rsquo;t have a high school diploma will earn more skills that make them increasingly employable by enrolling in and earning their GED. Is there any evidence for this?&lt;/p&gt;

&lt;p&gt;If we look at what the &lt;a href=&#34;http://www.brown.edu/Departments/Education/resources/what_do_we_know.pdf&#34;&gt;research says&lt;/a&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:research&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:research&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, we learn that the GED is only effective at boosting earnings of lower skilled recipients. Tyler suggests that this may indicate that the GED is acting as a labor market signal, demonstrating to future employers that they have the work ethic and extra motivation low-skilled high school drop-outs are believed to lack and are worth hiring. This is supported by the fact that not only are low skilled GED earners the only ones who see greater earnings and employability as a result of earning a GED but also by the fact that there is a lag to the &amp;ldquo;GED impact&amp;rdquo;. One possible mechanism for this lag indicates that the GED is a signal for unobserved characteristics that suggest an employee is ready to learn and earn those additional skills they do not current possess, and therefore, low-skilled workers who earn a GED are more likely to be placed in a position where they will have the opportunity to increase their human capital and future earnings. Since high skilled employees don&amp;rsquo;t seem to gain any benefit from a GED, that suggests that their existing skill set already sufficiently operates as a signal of employability such that there is no need for an additional signal of initial readiness for hiring. Even if this were the case, if the GED was truly ruled by a human capital model, we would expect high skilled GED recipients to have become increasingly skilled through the GED process and, therefore, they should also benefit from increased earnings due to receiving a GED.&lt;/p&gt;

&lt;p&gt;So what does this all mean? Let&amp;rsquo;s remember that the unemployment benefits being offered only exist for 99 weeks now (and would be lowered to 59 weeks in the House proposal). This means that all the unemployed we&amp;rsquo;re worried about were in fact employed within the last year and seven weeks. Do we believe that those who were employed as recently as one year ago, already deep into the economic downturn, who do not have a high school diploma fall into the low or high skilled group? It seems obvious to me that if you were employable 59 weeks ago you would almost certainly be in the upper half (if not higher) of the skills distribution among those who don&amp;rsquo;t have high school diplomas. So the result of the GED policy is likely going to lead to no benefit for these workers and will probably even decrease their earnings since they will have some costs associated with earning the GED&amp;ndash; be it the fee for a course, the fee to take the test, the opportunity costs associated with spending time studying and working toward a credential that has no benefit, etc.&lt;/p&gt;

&lt;p&gt;Worse, by dramatically changing the contexts where folks earn a GED, we&amp;rsquo;re likely to completely change what signal the GED will send future employers. In essence, the impact is unpredictable, but it&amp;rsquo;s hard to see a path whereby earning a GED increases in value as a result of policies that make certain that GEDs will be earned more broadly and under duress and not  by voluntary action.&lt;/p&gt;

&lt;p&gt;So forget about all the other stuff you read on the GED requirement. We don&amp;rsquo;t have to worry about fairness, discrimination, or just plain shitting on people when they&amp;rsquo;re in some of the most dire straits they&amp;rsquo;re likely to see by adding even more to their burdens. The GED-only policy on unemployment benefits is simply unlikely to do anything other than transfer resources from the unemployed to the &lt;a href=&#34;http://www.acenet.edu/AM/Template.cfm?Section=Home&#34;&gt;American Council on Education&lt;/a&gt; and companies that have GED &lt;a href=&#34;http://www.kaptest.com/ged&#34;&gt;prep&lt;/a&gt; &lt;a href=&#34;http://www.randomhouse.com/princetonreview/college/ged/&#34;&gt;classes&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:research&#34;&gt;John Tyler was a former professor of mine. If Brown had a PhD programming in education, I would go back in a heartbeat to work under him.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:research&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diane Ravitch, rebuttals.</title>
      <link>http://www.json.blog/2011/12/diane-ravitch-rebuttals./</link>
      <pubDate>Mon, 12 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/12/diane-ravitch-rebuttals./</guid>
      <description>&lt;p&gt;So I wrote a &lt;a href=&#34;http://blog.jasonpbecker.com/2011/11/28/is-diane-ravitch-a-reliable-historian/&#34;&gt;harsh post&lt;/a&gt; after reading a &lt;a href=&#34;http://www.tnr.com/print/article/politics/magazine/97765/diane-ravitch-education-reform&#34;&gt;harsh article&lt;/a&gt; by Kevin Carey in &lt;a href=&#34;http://www.tnr.com/&#34;&gt;The New Republic&lt;/a&gt; about &lt;a href=&#34;http://www.dianeravitch.com/&#34;&gt;Diane Ravitch&lt;/a&gt;. I still standby what I said. Namely, I&amp;rsquo;m very cautious about trusting Ravitch as a reliable narrator of history because I&amp;rsquo;m:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;unfamiliar with good historiography/methodology so it&amp;rsquo;s hard for me to judge the quality of her work simply from the product itself&lt;/li&gt;
&lt;li&gt;unaware of a rich discourse around education history in NYC and 20th century America in general that wrestles with, or even corroborates, Ravitch&amp;rsquo;s account&lt;/li&gt;
&lt;li&gt;certain that Ravitch&amp;rsquo;s more recent writings often mischaracterizes the power and meaning behind quantitative research and exhibits selection bias to fit a particular narrative&lt;/li&gt;
&lt;li&gt;generally distrustful of public academics, particularly when their writing is mainly outside of their primary discipline.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That being said, there have been several well-written critiques of Carey&amp;rsquo;s piece and I thought it&amp;rsquo;s only fair that I link to them to present a more complete picture of what many folks, some who agree and some who disagree with Ravitch&amp;rsquo;s current ideology, think of Ravitch&amp;rsquo;s work.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.edexcellence.net/about-us/people/michael-j-petrilli.html&#34;&gt;Mike Petrilli&lt;/a&gt; is certainly no fan of Ravitch&amp;rsquo;s rebirth as the anti-choice, anti-accountability voice &lt;em&gt;du jour&lt;/em&gt;. But &lt;a href=&#34;http://www.educationgadfly.net/flypaper/2011/11/what-kevin-carey-didnt-say-about-diane-ravitch-but-should-have/&#34;&gt;his piece&lt;/a&gt; in Flypaper in response to Carey is quite clear: the idea that Ravitch&amp;rsquo;s personal life  had an impact on her criticism of then NYC Schools
Chancellor &lt;a href=&#34;http://en.wikipedia.org/wiki/Joel_Klein&#34;&gt;Joel Klein&lt;/a&gt; is unfair and wrong. As someone who worked directly with Ravitch and who had, independently, overseen the awarding of a grant to &lt;a href=&#34;http://www.huffingtonpost.com/mary-butz&#34;&gt;Mary Butz&lt;/a&gt;&amp;rsquo;s leadership program, Petrilli sees a different line of thought. Ravitch, in his view, simply correctly pointed to the flaw in Klein&amp;rsquo;s &amp;ldquo;clear the field&amp;rdquo; approach that tended to cut down successful or promising programs alongside the dead weight.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.danagoldstein.net/dana_goldstein/about-dana.html&#34;&gt;Dana Goldstein&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;http://www.danagoldstein.net/dana_goldstein/2011/11/thoughts-on-kevin-careys-profile-of-diane-ravitch-history-and-ideology.html&#34;&gt;response&lt;/a&gt; suggests that Kevin Carey ignored the context in which Ravitch wrote. Goldstein suggests that Ravitch had to fight against a sexist academy in a discipline that increasingly had taken on a polemicists tone, as a liberal who did not quite fit the mold
of her times. These factors combined to generate the type of histories and writing that Ravitch would produce and are critical in understanding, without undermining, her work.&lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&#34;http://www.dianasenechal.com/bio.html&#34;&gt;Diane Senechal&lt;/a&gt; writes in The New Republic &lt;a href=&#34;http://www.tnr.com/article/politics/98379/diane-ravitch-school-reform?page=0,0&#34;&gt;today&lt;/a&gt; that Ravitch&amp;rsquo;s history is a far more balanced critique than Carey would have you believe, very well documented, and self-consistent. She does concede that Ravitch writes with a fiery, decidedly non-academic tone that&amp;rsquo;s
intended as a public intellectual. But here, Senechal views this as a strength, &amp;ldquo;arous[ing] general interest in matters that might otherwise seem out of reach or obscure.&amp;rdquo; Ultimately, Senechal&amp;rsquo;s main point is that Ravitch&amp;rsquo;s work is of very high quality and thorough and that her tone should not overshadow the accomplishment of her scholarship.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use Picasa with iOS5</title>
      <link>http://www.json.blog/2011/12/how-to-use-picasa-with-ios5/</link>
      <pubDate>Sun, 11 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/12/how-to-use-picasa-with-ios5/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m mostly writing this post because I had a fairly hard time finding a resolution to a real pesky error. For some reason, my &lt;a href=&#34;http://www.apple.com/iphone/&#34;&gt;iPhone 4S&lt;/a&gt; was recognized by &lt;a href=&#34;http://picasa.google.com/&#34;&gt;Picasa&lt;/a&gt; but always failed to import photos. Whenever I tried, the Picasa was clearly scanning through the files and then presented this error:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An error has occurred while attempting to import. Either the source is
unavailable or the destination is full or read only (1).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The resolution was found on &lt;a href=&#34;http://www.google.com/support/forum/p/Picasa/thread?tid=36adf4b3d0209e55&amp;amp;hl=en&#34;&gt;this page&lt;/a&gt; posted by &lt;a href=&#34;http://www.google.com/support/forum/p/Picasa/user?userid=14055753863597455136&amp;amp;hl=en&#34;&gt;Tradeinstyle&lt;/a&gt;. A slightly more thorough explanation of the solution below.&lt;/p&gt;

&lt;p&gt;If you are seeing this error, what appears to have happened is that several images are &amp;ldquo;corrupted&amp;rdquo; in some way on your iPhone. Unfortunately, this requires opening up &lt;a href=&#34;http://www.apple.com/ilife/iphoto/&#34;&gt;iPhoto&lt;/a&gt;. Once in iPhoto, you should be at the import screen and see all the pictures available on your phone. Several of these pictures will have a thumbnail consisting only of a dotted-line forming a square&amp;ndash; a blank thumbnail. You&amp;rsquo;ll want to import these photos and, after clicking import, be sure to select the option that removes them from your iPhone. Now move these imported photos from your newly create iPhoto library into your normal Pictures folder (or wherever you&amp;rsquo;re watching for pictures in Picasa). They&amp;rsquo;ll load just fine. Exit iPhoto, delete your iPhoto Library (probably located in ~/Pictures/iPhoto\ Library) to avoid duplicates and open Picasa. Because the &amp;ldquo;offending&amp;rdquo; pictures have now been removed, Picasa should be able to easily import your photos.&lt;/p&gt;

&lt;p&gt;This problem does not appear to be specific to the iPhone 4s and is probably applicable to &lt;a href=&#34;http://www.apple.com/ipodtouch/&#34;&gt;all&lt;/a&gt; &lt;a href=&#34;http://www.apple.com/ios/&#34;&gt;iOS5&lt;/a&gt; &lt;a href=&#34;http://www.apple.com/ipad/&#34;&gt;devices&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Social Reading Should Work</title>
      <link>http://www.json.blog/2011/12/how-social-reading-should-work/</link>
      <pubDate>Thu, 08 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/12/how-social-reading-should-work/</guid>
      <description>

&lt;p&gt;I am becoming increasingly frustrated by the failure of all the major players to get social right. I have a very simple dream for how the social web should work and its baffling to me that many obvious use-cases have not been addressed at all by Facebook, Twitter, or Google.&lt;/p&gt;

&lt;p&gt;This is the first of two posts that will describe what I view as a viable framework for a social web experience. The whole goal of social web, in my view, is to read, share, discover, and communicate about found content. This post will focus on finding and reading content. The second will focus on sharing and discussing that content.&lt;/p&gt;

&lt;h2 id=&#34;properly-handle-content-sources&#34;&gt;Properly Handle Content Sources&lt;/h2&gt;

&lt;p&gt;One of the major shortcomings of Facebook, Twitter, and Google+ is source content. The backbone of the ideal social experience is not simply sharing inane details of your personal life. It&amp;rsquo;s making the entire web a community activity. It&amp;rsquo;s about making communication on the internet as rich and natural an experience as possible. The branded pages and official accounts simply do not substitute for an excellent content platform. The origin of this problem is simple&amp;ndash; the modern social network is entirely built upon connecting people, and content generators are just considered a hacked up special class of people. Reading (and generating) content is the ground floor of the social
experience.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s my evidence that this model is insufficient? The popularity of services like Google Reader, Flipboard, Feedly, etc. Need more? The three major social players are all introducing new ways to bring content sources into their services and keep my eyes within their system. Facebook has its Social Reader, which is just creepy to me because I can&amp;rsquo;t share outside of Facebook and I don&amp;rsquo;t want everything my eyes glaze over to be shared instantly with everyone. Twitter has its &amp;ldquo;Discover&amp;rdquo; tab that goes well beyond trending and tries to create a pre-curated reader experience. Google has Currents, a Flipboard clone that&amp;rsquo;s based upon casual magazine style reading, complete with a whole new set of subscriptions, a good mobile experience, and easy sharing into Google Plus.&lt;/p&gt;

&lt;p&gt;But none of these solutions recognize that there are at least three major domains of access content.&lt;/p&gt;

&lt;h2 id=&#34;bookmarking&#34;&gt;Bookmarking&lt;/h2&gt;

&lt;p&gt;The first way people find content is from sources they want to read casually. These are the sites you check when you&amp;rsquo;re bored or when there&amp;rsquo;s a massive breaking story. This is your New York Times or CNN.com pushing out massive amounts of timely information that you just want to dip into time to time. This is one form of content that folks are just starting to get right. Flipboard/Google Currents successfully gives a gorgeous platform for casually reading across many sources. There&amp;rsquo;s no need to keep track of every story or go through content methodically. This reading experience is quick and casual and all about stumbling across something.&lt;/p&gt;

&lt;h2 id=&#34;collecting&#34;&gt;Collecting&lt;/h2&gt;

&lt;p&gt;This is the bread and butter for RSS subscribers and one of the major areas that most social players are ignoring. Collecting means you want to read everything someone or some site writes. You want to make sure to come back and glance over things you don&amp;rsquo;t get a chance to see. Read/unread counts are a critical piece of making sure you read every piece of news that comes through. This is content reading you want to tag, save, easily search through, etc. Another way to think of collecting is the set of information you trust only yourself to sort through a curate. This isn&amp;rsquo;t your list of pretty recipes that come streaming in quickly and are throw-aways. These are your trusted insider industry areas that get at the heart of your job or most important hobbies.&lt;/p&gt;

&lt;h2 id=&#34;streaming&#34;&gt;Streaming&lt;/h2&gt;

&lt;p&gt;This is the traditional &amp;ldquo;news feed&amp;rdquo; of social reading. It&amp;rsquo;s how you see what all your &amp;ldquo;friends&amp;rdquo; are sharing, doing, and saying. This is about finding the trends, the conversations that are blowing up, the short funny statements, etc. You almost definitely don&amp;rsquo;t care if you miss something someone posts here, but you want to be able to see the cream that rises to the top. You want a way that conveniently allows you to enter someone else&amp;rsquo;s workspace and interact with the content they&amp;rsquo;ve shared with you. This is the other area that social has some models that work well and was the basis for all other activities on the social web. It&amp;rsquo;s also one of the social webs major problems. The stream is massive and cannot be absorbed in whole. Most of the content is throwaway. But because almost all other social reading is based on the stream, all of our content, even what we collect and bookmark, becomes throwaway and short-lived, given the same priority as your long lost aunt in an old age home playing Farmville.&lt;/p&gt;

&lt;p&gt;For me, the social web has to start with providing me with a single space that aggregates what I want to read on the web. If content is not easy for me to get access to, I&amp;rsquo;m never going to even worry about being satisfied with my options for sharing. Social fails right now because they haven&amp;rsquo;t gotten the reading paradigm right. It&amp;rsquo;s nowhere close to handling the three major domains&amp;ndash;bookmarking, collecting, and streaming&amp;ndash; effectively under a single attractive interface. I think Google is the closest. If you combine some of the features in Currents, their new attractive mobile reader that works great for bookmarking, with some of the critical features in Google Reader, right now the best collector, then I&amp;rsquo;m very close to an ideal reading experience. My hope is that someone will find a way to combine all three and then provide the robust sharing features I&amp;rsquo;ll write about in my next post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contracting Technology in Education</title>
      <link>http://www.json.blog/2011/11/contracting-technology-in-education/</link>
      <pubDate>Tue, 29 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/11/contracting-technology-in-education/</guid>
      <description>&lt;p&gt;I am glad that &lt;a href=&#34;http://gothamschools.org/author/philissa-cramer/&#34;&gt;Philissa Cramer&lt;/a&gt; is reporting on some of the deeper details of the Special Education Student Information System &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:coverage&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:coverage&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; implementation at the &lt;a href=&#34;http://schools.nyc.gov/default.htm&#34;&gt;New York City Department&lt;/a&gt; of Education &lt;a href=&#34;http://gothamschools.org/2011/11/29/report-links-sesis-struggles-and-does-contractinpractices/&#34;&gt;here&lt;/a&gt;. Many people don&amp;rsquo;t really understand the ins and outs of government contracting. Folks really think NASA designs and builds the lunar module, for example, instead of realizing that they issued an RFP and contracting with &lt;a href=&#34;http://www.northropgrumman.com/&#34;&gt;Northrop Grumman&lt;/a&gt; to do that work for them. Similarly, in education, especially around complex technology projects, most districts and states purchase products or services through a bid product rather than develop solutions in house.&lt;/p&gt;

&lt;p&gt;However, I am a bit disappointed in the angle that &lt;a href=&#34;http://www.citylimits.org&#34;&gt;City Limits&lt;/a&gt;, (Ms. Cramer&amp;rsquo;s &lt;a href=&#34;http://www.theinvestigativefund.org/investigations/politicsandgovernment/1581/beyond_citytime?page=1&#34;&gt;source&lt;/a&gt;) took in their reporting. There are real problems with government contracting, but they really mischaracterize the story around SESIS in an attempt to simplify the issue for casual readers.
City Limits acts as though it is surprising, or even deplorable, that an RFP was awarded to a company with a largely existing product.&lt;/p&gt;

&lt;p&gt;They point to the fact that Maximus, the vendor for the SESIS contract, was modifying an existing product to meet the requirements outlined in NYC DOE&amp;rsquo;s RFP as though this was clearly bad. City Limits uses words like &amp;ldquo;revealed&amp;rdquo; and &amp;ldquo;simply&amp;rdquo; to describe what Maximus was offering. This just ignores the reality of government contracting and shows disregard for risk mitigation. Almost all government agencies handsomely reward companies that can point to successes in developing and implementing solutions that can meet many of the requirements outlined in the issued RFP. The government wants to hire people it believes can do the job and do it well and often one of the best ways to make that determination is to see that someone has done it before. In almost all cases, this means selecting a vendor who has an existing product or process for meeting many of the requirements in the RFP that will be expanded upon or modified. But government purchasers don&amp;rsquo;t just want experienced partners, they also want to leverage efficiencies by not paying for duplication. One of the major reasons for purchasing an existing product or contracting with a vendor is that school systems actually aren&amp;rsquo;t that different from one another and the basic functionality and organizational structures required in an IT solution are shared across schools, districts, and states. Why pay substantially to build the same basic software infrastructure that already exists elsewhere? It&amp;rsquo;s a waste of money most of the time.&lt;/p&gt;

&lt;p&gt;City Limits then goes on to criticize the massive increases that can occur due to change orders. This is a serious problem with government contracting, but they fail to really explore why. A change order occurs when the client wants new or additional functionality that was not included in the initial contract. Their frequency and expense are not an example of why government agencies should not contract with outside vendors, rather, they demonstrate just how poorly bureaucracies are at managing large-scale complex projects. Change orders happen due to several failures, and almost all are the government agencies&amp;rsquo; fault. In no particular order, the government agency:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;failed to do proper discovery before issuing the RFP and, therefore, missed major functional requirements that are not identified until more intensive discovery occurs during development or initial implementation;&lt;/li&gt;
&lt;li&gt;agreed to a contract that was far too specific and did not allow for the reality that requirements do evolve over time (though often not in ways which substantially change the nature or quantity of work);&lt;/li&gt;
&lt;li&gt;agreed to a contract that was far too vague such that the vendor can claim to have delivered a product or service when they did not meet already identified functional requirements for the system;&lt;/li&gt;
&lt;li&gt;did not take into account the preparation and costs required to sustain the product beyond the life of the initial engagement with the vendor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are the main things that lead to change orders. If the government agency is doing a top-notch job, they can all be avoided and the only occasion for a change order should be large external shocks that dramatically alter the functional requirements, intentional decisions to move away from the initial functional requirements that weighed the costs of altering the vendor contract, and a desire to extend and expand a relationship because of the success of the initial implementation resulting in a substantially more advanced or mature product.&lt;/p&gt;

&lt;p&gt;It is really hard to manage vendor contracts right. It requires actually knowing what you want to buy before an RFP is issued (or recognizing what is and is not known and correctly assessing the scope of the impact future decisions on unknowns will have on the work). It requires a really good team of lawyers to fight outside forces that literally make their profits on carefully abdicating as much responsibility as possible at the contracting phase. It requires selecting good partners that are adequately willing and prepared to evolve and work with the agency as their needs and knowledge grow and mature. It requires an honest assessment of future resources that will be available to assure sustainability of large investments. And perhaps most difficult of all, it requires strong project management infrastructure throughout the entire agency to ensure alignment and consistency across multiple products produced internally and by multiple external partners.&lt;/p&gt;

&lt;p&gt;The benefits of outsourcing products can be huge and are worth leveraging. Vendor contracts are difficult to manage, and bureaucracies are not always well-suited to managing these projects, but all government agencies struggle with getting this right.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;One last parting thought&amp;hellip;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In my view, the most challenging aspect of getting vendor contracting right for government agencies is spending ample upfront time, even before issuing an RFP, articulating the functional needs that a solution must meet in detail. I feel that often public sector employees are so
intimidated by the arduous process around issuing and awarding an RFP that they rush to get an RFP out there and worry about the details during contracting and product initiation. Whenever possible, resist this temptation at all costs. Whether custom designed internally or
provided by an external vendor, satisfaction is dependent upon clarity of the desired outcomes. This is particularly true with technology projects. Vendors will always produce &lt;em&gt;something&lt;/em&gt;, but whether the solution is any good is almost entirely up to good requirements gathering.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:coverage&#34;&gt;For pretty good coverage check out all of Gotham School&amp;rsquo;s posts &lt;a href=&#34;http://gothamschools.org/tag/sesis/&#34;&gt;http://gothamschools.org/tag/sesis/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:coverage&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is Diane Ravitch a reliable historian?</title>
      <link>http://www.json.blog/2011/11/is-diane-ravitch-a-reliable-historian/</link>
      <pubDate>Mon, 28 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/11/is-diane-ravitch-a-reliable-historian/</guid>
      <description>&lt;p&gt;&lt;em&gt;UPDATE: You should also read &lt;a href=&#34;http://blog.jasonpbecker.com/2011/12/12/diane-ravitch-rebuttals/&#34;&gt;this page&lt;/a&gt; on jasonpbecker for some very strong and interesting rebuttals to Carey&amp;rsquo;s article that I commented on below.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Do read &lt;a href=&#34;http://www.tnr.com/print/article/politics/magazine/97765/diane-ravitch-education-reform&#34;&gt;this article&lt;/a&gt; on Diane Ravitch. I personally have two major criticisms of Ravitch, both of which Carey exposes eloquently.&lt;/p&gt;

&lt;p&gt;First, I believe that she leverages her respect and expertise as an historian and professor to present herself as an experts in areas of academic research and policy where she has little expertise. This is very common with public intellectuals, and I think it&amp;rsquo;s deceiving and deplorable.&lt;/p&gt;

&lt;p&gt;Second, I am unsure about whether she is a reliable narrator of history because my impression is that she&amp;rsquo;s the &amp;ldquo;best in the game&amp;rdquo; at least in part because so few are playing. I don&amp;rsquo;t personally have the skill to judge her histories and given her blatant academic dishonesty in so many other areas where I have some ability to judge quality, I find it hard to view her as an honest operator.&lt;/p&gt;

&lt;p&gt;What is somewhat new in Carey&amp;rsquo;s take on Ravitch, and what I think most here on Plus will find interesting, are two revelations. First, one I was somewhat acquainted with, it seems possible that some of Ravitch&amp;rsquo;s shift to rhetorical vitriol against someone who seemed a natural ally (Joel Klein) may be partially attributable to a personal dispute involving Ravitch&amp;rsquo;s &amp;ldquo;partner&amp;rdquo; (this and other articles seem to be intentionally ambiguous about the nature of this relationship). Second, and most interesting to me, it appears that Ravitch doesn&amp;rsquo;t have the typical academic acumen of an acclaimed scholar in her field. In fact, it appears that Ravitch has produced almost exclusively popular history throughout her career. This detail in particular plays into some of my very concerns about the reliability of her historical narratives.&lt;/p&gt;

&lt;p&gt;On a side note, I think if I could be one person in education policy today it&amp;rsquo;d be Kevin Carey. He&amp;rsquo;s smart as hell and an excellent writer, even if I disagree with him on higher education issues.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why isn&#39;t the Core Connector using Westminster with full RoW?</title>
      <link>http://www.json.blog/2011/11/why-isnt-the-core-connector-using-westminster-with-full-row/</link>
      <pubDate>Sun, 06 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/11/why-isnt-the-core-connector-using-westminster-with-full-row/</guid>
      <description>&lt;p&gt;I love that Providence is pursuing a streetcar. There are really just
two things I don&amp;rsquo;t understand about the &lt;a href=&#34;http://providencecoreconnector.com/&#34;&gt;Core Connector&lt;/a&gt;&amp;rsquo;s proposal.
I&amp;rsquo;m going to tackle one in this post.&lt;/p&gt;

&lt;p&gt;Why is the entire streetcar route shared with general traffic with no
dedicated right-of-way? Truthfully, this isn&amp;rsquo;t a massive issue except in
the core part of Downcity where there is substantial traffic during rush
hours along the street car route. But this makes the plan even more
perplexing because it&amp;rsquo;s precisely this portion of the route where an
obvious solution for dedicated light rail ROW exists- Westminster
Street.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.json.blog/img/Route-November-3.png&#34; alt=&#34;Quick and dirty Core Connector modification&#34; title=&#34;CoreConnectorRouteModified&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The red route above represents the proposed streetcar line. The green
line represents Westminster Street, a narrow, single lane, one-way
street that cuts through Downcity and in front of &lt;a href=&#34;http://suraprovidence.com/&#34;&gt;restaurants&lt;/a&gt;,
&lt;a href=&#34;http://www.queenofheartsandmodernlove.com/&#34;&gt;boutique&lt;/a&gt; &lt;a href=&#34;http://www.shopwarf.com&#34;&gt;shopping&lt;/a&gt;, &lt;a href=&#34;http://www.uri.edu/prov/&#34;&gt;URI&lt;/a&gt;, etc. It brings the streetcar line
slightly closer to Johnson and Wales and slightly further from the
Dunkin Donuts Arena and Rhode Island Convention Center. While there is
real automobile traffic, this is almost entirely for two reasons. First,
Westminster has substantial on-street metered parking. Second,
Westminster is the East-&amp;gt;West one-way to counter Weybosset&amp;rsquo;s West-East.&lt;/p&gt;

&lt;p&gt;Of course, the two major Downcity planning projects underway are
removing the stress that leads to both of these uses. The Downtown
Circulator project is nearly complete, converting Empire and Weybosset
to two-way streets. Automobile traffic will almost certainly take the
wider and faster Washington and Weybosset Streets, adjacent to
Westminster, unless the goal is to find on the street parking. The
second project is the Core Connector itself, which provides more options
to get into Downcity without a car, hopefully reducing the need for
parking. There are also substantial parking capacity that&amp;rsquo;s
underutilized in the many surface lots and parking garages in the area.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.jasonpbecker.com/wp-content/uploads/2011/11/4256071978_aaf18cb143_z.jpg&#34; alt=&#34;A great shot from Flickr of Westminster at night&#34; title=&#34;Westminster at Night&#34; /&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:sourceone&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:sourceone&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;As far as I can tell, there is really no need for Westminster to have
street traffic. A dedicated ROW will increase the speed and
predictability of the streetcar. Additional pedestrian space along
Westminster could quickly be used by the cafes and restaurants and
street vendors that already are in the area. The only reason I could
come up with for not using Westminster as a dedicated right-of-way for
the streetcar is the need for a turn in or around Kennedy Plaza. There
are so many options for moving between Washington and Westminster that I
just can&amp;rsquo;t buy this as an insurmountable challenge.&lt;/p&gt;

&lt;p&gt;I was unable to make it to the three recent public meetings about the
proposed route. If I were there, this would certainly be my first
question.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:sourceone&#34;&gt;Sourced from &lt;a href=&#34;http://www.flickr.com/photos/ranjith-pix/&#34;&gt;http://www.flickr.com/photos/ranjith-pix/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:sourceone&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GoLocalProv -- Bad at Math</title>
      <link>http://www.json.blog/2011/10/golocalprov----bad-at-math/</link>
      <pubDate>Sun, 16 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/10/golocalprov----bad-at-math/</guid>
      <description>&lt;p&gt;There are lots of things that are misleading about &lt;a href=&#34;http://www.golocalprov.com/news/10820/&#34;&gt;this story&lt;/a&gt; published on GoLocalProv. It is utterly ridiculous to report numbers like how many total tax dollars are being collected by different communities for the sake of comparison. You cannot compare a total number like this which is so dependent upon things like, I don&amp;rsquo;t know, the &lt;strong&gt;dramatic difference in the size of these communities&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;In the 2010 Census, Providence had a population of &lt;strong&gt;178,042&lt;/strong&gt;. New Shoreham had a population of &lt;strong&gt;1,051&lt;/strong&gt;. Is anyone surprised that one of these communities is on the top of the list and the other on the bottom?&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s more than size at play, but the least GLP could have done was to correct for population and present the per capita levy. All it takes is one quick Google search and we can get the 2010 Census numbers which are probably pretty damn close to the current population so we can get a decent, somewhat level playing field to compare cities and towns on. &lt;a href=&#34;http://www.dlt.ri.gov/lmi/census/pop/townpop.htm&#34;&gt;Here&amp;rsquo;s the 2010 Census numbers from the RI Department of Labor and Training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So with 5 minutes in Excel (thanks, GLP, for making your charts images instead of tables), here&amp;rsquo;s a much more interesting list:&lt;/p&gt;

&lt;table width=&#34;250&#34; border=&#34;1&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;
  &lt;colgroup&gt; 
    &lt;col width=&#34;85&#34; align=&#34;left&#34;&gt;&lt;/col&gt;
    &lt;col width=&#34;69&#34; align=&#34;right&#34;&gt;&lt;/col&gt;
    &lt;col width=&#34;50&#34; align=&#34;right&#34;&gt;&lt;/col&gt;
    &lt;col width=&#34;55&#34; align=&#34;right&#34;&gt;&lt;/col&gt;
  &lt;/colgroup&gt;
  &lt;tbody style=&#34;background-color:white&#34;&gt;
  &lt;tr&gt;
    &lt;td width=&#34;75&#34; height=&#34;12&#34; align=&#34;left&#34;&gt;
      **Community**
    &lt;/td&gt;
    &lt;td width=&#34;70&#34;&gt;
      **FY12 Levy**
    &lt;/td&gt;
    &lt;td width=&#34;50&#34;&gt;
      **2010 Pop**
    &lt;/td&gt;
    &lt;td width=&#34;55&#34;&gt;
      **Per Capita**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      New Shoreham
    &lt;/td&gt;
    &lt;td&gt;
      \$8,187,149
    &lt;/td&gt;
    &lt;td&gt;
      1,051
    &lt;/td&gt;
    &lt;td&gt;
      **\$7,790**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Jamestown
    &lt;/td&gt;
    &lt;td&gt;
      \$18,653,102
    &lt;/td&gt;
    &lt;td&gt;
      5,405
    &lt;/td&gt;
    &lt;td&gt;
      **\$3,451**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Barrington

    &lt;/td&gt;
    &lt;td&gt;
      \$55,162,905
    &lt;/td&gt;
    &lt;td&gt;
      16,310
    &lt;/td&gt;
    &lt;td&gt;
      **\$3,382**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      East Greenwich
    &lt;/td&gt;
    &lt;td&gt;
      \$44,015,850
    &lt;/td&gt;
    &lt;td&gt;
     13,146
    &lt;/td&gt;
    &lt;td&gt;
      **\$3,348**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      West Greenwich
    &lt;/td&gt;
    &lt;td&gt;
      \$17,703,664
    &lt;/td&gt;
    &lt;td&gt;
      6,135
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,886**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Little Compton
    &lt;/td&gt;
    &lt;td&gt;
      \$10,004,530
    &lt;/td&gt;
    &lt;td&gt;
      3,492
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,865*&lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Narragansett
    &lt;/td&gt;
    &lt;td&gt;
      \$44,732,180
    &lt;/td&gt;
    &lt;td&gt;
      15,868
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,819**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Westerly
    &lt;/td&gt;
    &lt;td&gt;
      \$63,547,705

    &lt;/td&gt;
    &lt;td&gt;
      22,787
    &lt;/td&gt;
    &lt;td&gt;
    **\$2,789**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Charlestown

    &lt;/td&gt;
    &lt;td&gt;
      \$21,611,447

    &lt;/td&gt;
    &lt;td&gt;
      7,827
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,761**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Portsmouth
    &lt;/td&gt;
    &lt;td&gt;
      \$45,807,376
    &lt;/td&gt;
    &lt;td&gt;
     17,389
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,634**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Warwick
    &lt;/td&gt;
    &lt;td&gt;
      \$216,867,072
    &lt;/td&gt;
    &lt;td&gt;
      82,672
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,623**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Middletown
    &lt;/td&gt;
    &lt;td&gt;
      \$41,588,607
    &lt;/td&gt;
    &lt;td&gt;
      16,150
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,575**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Newport
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      \$63,519,526
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      24,672
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      **\$2,575**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
    North Kingstown
    &lt;/td&gt;
    &lt;td&gt;
    \$67,598,341
    &lt;/td&gt;
    &lt;td&gt;
     26,486
    &lt;/td&gt;
    &lt;td&gt;
    **\$2,552**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Scituate
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      \$25,492,269
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      10,329
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      **\$2,468**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Lincoln
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      \$51,960,896
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      21,105
    &lt;/td&gt;
    &lt;td align=&#34;right&#34;&gt;
      **\$2,462**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Foster
    &lt;/td&gt;
    &lt;td&gt;
      \$11,221,591
    &lt;/td&gt;
    &lt;td&gt;
      4,606
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,436**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Johnston
    &lt;/td&gt;
    &lt;td&gt;
      \$68,570,772
    &lt;/td&gt;
    &lt;td&gt;
     28,769
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,383**
    
    &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      North Smithfield
    &lt;/td&gt;
    &lt;td&gt;
      \$27,592,721
    &lt;/td&gt;
    &lt;td&gt;
     11,967
    &lt;/td&gt;
    &lt;td&gt;
      **\$2,306**
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height=&#34;12&#34;&gt;
      Smithfield
    &lt;td&gt;
      \$49,357,148
    &lt;/td&gt;
&lt;td&gt;
 21,430

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td&gt;
**\$2,303**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Tiverton

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$35,771,014

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 15,780

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,267**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Cranston

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$180,715,853

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 80,387

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,248**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
South Kingstown

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$66,120,832

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 30,639

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,158**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Hopkinton

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$17,630,988

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 8,188

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,153**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Glocester

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$20,971,276

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 9,746

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,152**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
North Providence

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$67,218,014

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 32,078

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,095**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Warren

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$21,971,276

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 10,611

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,071**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Richmond

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$15,705,615

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 7,708

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$2,038**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Exeter

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$12,619,379

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 6,425

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,964**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Providence

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$324,460,407

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 178,042

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,822**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
West Warwick

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$52,337,257

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 29,191

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,793**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Coventry

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
\$61,860,355

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
 35,014

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,767**

&lt;/td&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;/tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;tr&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;td height=&#34;12&#34;&gt;
Cumberland

&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$57,890,766
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
33,506
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,728**
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td height=&#34;12&#34;&gt;
Burrillville
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$26,687,010
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
15,955
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,673**
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td height=&#34;12&#34;&gt;
Bristol
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$35,697,780
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
22,954
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,555**
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td height=&#34;12&#34;&gt;
Pawtucket
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$96,340,757
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
71,148
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,354**
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td height=&#34;12&#34;&gt;
Woonsocket
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$53,984,558
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
41,186
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$1,311**
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td height=&#34;12&#34;&gt;
Central Falls
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
\$13,148,778
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
 19,376
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
**\$679**
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>TEA Party and Occupy Wall Street</title>
      <link>http://www.json.blog/2011/10/tea-party-and-occupy-wall-street/</link>
      <pubDate>Sat, 15 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/10/tea-party-and-occupy-wall-street/</guid>
      <description>&lt;p&gt;I wondered to myself If I could explain these two movements in a few sentences. Is this fair?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Taxed Enough Already (TEA) Party movement is a response to two
large government spending packages, the &amp;ldquo;bailout&amp;rdquo; and the &amp;ldquo;stimulus&amp;rdquo;
package. These people felt that it was inappropriate for the
government spend taxpayer money (and foreign debt) in an attempt to
prevent deeper economic damage from the collapse of the real estate
bubble.&lt;/p&gt;

&lt;p&gt;The Occupy Wall Street movement is a response to the same two large
government spending packages as well as the subsequently ineffectual
American government during the first term of the Obama Presidency.
These people are skeptical that the &amp;ldquo;bailout&amp;rdquo; and &amp;ldquo;stimulus&amp;rdquo; package
addressed the challenges that the vast majority of Americans face
every day in favor of addressing the needs of an elite economic and
political class.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>School facilities-- anachronistic, expensive, isolating, and all around bad public spaces?</title>
      <link>http://www.json.blog/2011/10/school-facilities---anachronistic-expensive-isolating-and-all-around-bad-public-spaces/</link>
      <pubDate>Thu, 06 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/10/school-facilities---anachronistic-expensive-isolating-and-all-around-bad-public-spaces/</guid>
      <description>&lt;p&gt;I wanted to write a lot more about this, but I just don&amp;rsquo;t have the time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://gothamschools.org/2011/10/05/downtown-residents-disappointed-by-school-zones-proposal/&#34;&gt;This story&lt;/a&gt;is about rezoning schools in downtown Manhattan which is struggling to meet the demands of emerging residential neighborhoods. Reading this story (and struggle) just brought up something I&amp;rsquo;ve thought about for some time now.&lt;/p&gt;

&lt;p&gt;The cost of school buildings is ridiculous. Schools are generally built for one purpose. They are generally built to last a very long time. They are generally built to a quality standard that suggests it will perennially be far too expensive to knock down and start over even if renovations are obscenely expensive and inadequate. In most areas (dense urban cities are probably the exception), we build schools on large plots of land with field/park space attached. This land is technically for public use, but in the name of safety for children, land uses are far more restrictive than most public parks.&lt;/p&gt;

&lt;p&gt;It all just seems like an absurd setup that wastes countless public dollars. Why wouldn&amp;rsquo;t we want to have smaller schools in mixed-use spaces that represent far less capital investment and introduce substantial budget flexibility as enrollment patterns change? Why would we want to build separate libraries from existing public resources? Why would we want separate fields rather than bringing students to truly public spaces during the day?&lt;/p&gt;

&lt;p&gt;The school house as a public space that&amp;rsquo;s isolated and locked away from the community that builds it, the school house that&amp;rsquo;s on a 100-year bond designed in such a way that any conversion to other uses is very unlikely&amp;hellip; isn&amp;rsquo;t that school house a bit anachronistic?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downcity Residents Should Support the Core Connector and the Tax Makes Sense</title>
      <link>http://www.json.blog/2011/09/downcity-residents-should-support-the-core-connector-and-the-tax-makes-sense/</link>
      <pubDate>Mon, 26 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/09/downcity-residents-should-support-the-core-connector-and-the-tax-makes-sense/</guid>
      <description>

&lt;p&gt;As a resident of Downcity, I have been closely following the development of &lt;a href=&#34;http://providencecoreconnector.com/wp-content/uploads/2010/09/Final-Alternatives-Fact-Sheet.pdf&#34;&gt;Providence&amp;rsquo;s Core Connector Study&lt;/a&gt;. The official route and payment options have now been proposed, as reported in the &lt;a href=&#34;http://www.projo.com/news/content/STREETCAR_PLAN_09-26-11_UAQI5KV_v15.6c71c.html&#34;&gt;Projo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;route&#34;&gt;Route&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m pleased to see that they prioritized frequent service through the main ridership areas (College Hill to Jewelry District and hospital) were prioritized over service to the train station. Jef Nickerson says it best over at &lt;a href=&#34;http://www.gcpvd.org/2011/09/26/streetcars-the-train-station-is-out-but-thats-ok/&#34;&gt;GCPVD&lt;/a&gt;&amp;ndash; the train station is out-of-the-way and would dramatically increase rider time while having unclear implications for ridership, and the station is already very well served (and could easily be better served) by existing bus routes. The streetcar is really about moving people within Providence and providing a permanency to the connectivity between the current (Brown and hospital) and hopefully future (Jewelry District) economic engines of the city.&lt;/p&gt;

&lt;p&gt;The proposed route uses Washington and Empire Street&amp;ndash; both are wise choices. Washington Street adds the &lt;a href=&#34;http://www.providencebiltmore.com/&#34;&gt;Biltmore&lt;/a&gt;, &lt;a href=&#34;http://www.lupos.com/&#34;&gt;Lupos&lt;/a&gt;, &lt;a href=&#34;http://www.uri.edu/prov/&#34;&gt;URI&lt;/a&gt;, and &lt;a href=&#34;http://www.as220.org/about/the-mercantile-block.html&#34;&gt;AS220&lt;/a&gt; directly to the route while keeping the &lt;a href=&#34;http://www.riconvention.com/&#34;&gt;Convention Center&lt;/a&gt; and the &lt;a href=&#34;http://www.dunkindonutscenter.com/&#34;&gt;Dunk&lt;/a&gt; near by. Washington also has the advantage of a direct route over 95 for a possible South-Westward expansion in the future with limited cost and slow downs due to a lack of turns. Empire Street is also a great choice. The road is only about to undergo construction to be expanded for two-way traffic (one of the last legs of the Downtown Circulator project). The corner of Washington and Empire anchors the streetcar at the &lt;a href=&#34;http://www.provlib.org/&#34;&gt;Providence Central Library&lt;/a&gt;, &lt;a href=&#34;http://www.trinityrep.com/&#34;&gt;Trinity Rep&lt;/a&gt;, &lt;a href=&#34;http://www.as220.org&#34;&gt;AS220&lt;/a&gt;, &lt;a href=&#34;http://38studios.com/&#34;&gt;38 Studios&lt;/a&gt;, and &lt;a href=&#34;http://www.projo.com/economy/HASBRO_LASALLE_SQUARE_07-19-11_EMP8O4D_v19.42183.html&#34;&gt;Hasbro&amp;rsquo;s new Downcity location&lt;/a&gt;. Regency Plaza adds more residential ridership and the massive parking lot across from the Hilton suddenly looks more attractive for infill development. It doesn&amp;rsquo;t hurt that I live at Westminster and Empire and would be excellently served by this location. Overall, I believe this path through Downcity is the easiest to manage while anchoring the streetcar near major hubs of activity. I only wish they could have found a way to bring the streetcar down Westminster and close the street to personal vehicle traffic, but that was never a likely option.&lt;/p&gt;

&lt;p&gt;The Jewelry District part of the route never seemed as controversial to me because there were limited solutions that  were somewhat obvious. The decision to use Chestnut makes sense and solidifies the Westminster-Chestnut Street path as a strong North-South connector through the area. Not going all the way to Prairie Avenue is likely going to anger some Upper South Providence residents, but I&amp;rsquo;m not convinced this is a bad thing. At the &lt;a href=&#34;http://blog.jasonpbecker.com/2011/09/08/community-voice-at-the-knowledge-district-development-framework-meeting/&#34;&gt;Knowledge District Development Framework meeting&lt;/a&gt; it was clear that increasing paucity between the two sides of Prairie Avenue was a major goal of development in the hospital area. Forcing people to walk a bit on Dudley Street may generate the kind of foot traffic needed to make infill in that area with first floor retail a lot more attractive.&lt;/p&gt;

&lt;h2 id=&#34;the-tax&#34;&gt;The Tax&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m in favor of it. This one is a no-brainer in my mind. Property values will certainly increase in the areas being served by the streetcar and therefore current owners stand to have some significant gains in equity if this project moves forward. The attractiveness of living where I am has increased tremendously for Brown faculty and staff, medical students, and some of the entrepreneurs and their employees if they ever materialize in the Jewelry District. That I should have to contribute some of this gained equity back to get the project built makes sense. The question is, will the requested tax be too high to be worth it? So let&amp;rsquo;s do the math. The proposal will hit me with \$0.95 on every \$1,000 of property value. Let&amp;rsquo;s assume that the homestead exemption will not be applied to this tax. Let&amp;rsquo;s also assume that I&amp;rsquo;m looking at a 15 year stay Downcity. This is reasonable because most of the properties around here are condos that are one or two bedroom and are not attractive to
folks with families&amp;ndash; we&amp;rsquo;re filled with young folks and empty-nesters who aren&amp;rsquo;t likely to be looking at this like a 30 year investment. Let&amp;rsquo;s also assume that the economy will continue to stagnate over this period so we only see an inflation rate of, say, 2%. It&amp;rsquo;s likely that this is an overly pessimistic estimate that will increase the cost. What I&amp;rsquo;m interested in is the present discounted value, or the cost to me due to this tax if I were to incur it all up front. The theory goes that money today is worth more than money tomorrow because money depreciates in value due to inflation and because money today can be invested and will grow over time. We calculate PDV much like you would calculate compound interest. The final piece of data to calculate the PDV needed is my home value. Let&amp;rsquo;s assume it hasn&amp;rsquo;t moved at all since I purchased about a year ago, which would peg my condo at \$168,000. Now I want to know if
the PDV of the tax will exceed what I believe is a reasonable estimate for the increase in equity I will realize because of the project.
And the PDV is&amp;hellip;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;\$2,050.74&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think it would be hard to argue that my property value won&amp;rsquo;t increase at least 1.2% because of this project. Adding the line, &amp;ldquo;Steps to the Providence Streetcar that will take you to Brown&amp;rsquo;s Medical School, through the Knowledge District, and to the Hospitals or through Downcity to RISD and Brown,&amp;rdquo; is going to be worth more than that, period. I can&amp;rsquo;t imagine the calculus is dramatically different for other Downcity property owners which means for us, this makes &amp;ldquo;cents&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Justin Baeder asks the Wrong Question on Teacher Evaluation</title>
      <link>http://www.json.blog/2011/09/justin-baeder-asks-the-wrong-question-on-teacher-evaluation/</link>
      <pubDate>Fri, 16 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/09/justin-baeder-asks-the-wrong-question-on-teacher-evaluation/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re interested in education, I highly recommend Justin Baeder&amp;rsquo;s&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &amp;rdquo;&lt;a href=&#34;http://blogs.edweek.org/edweek/on_performance/&#34;&gt;On Performance&lt;/a&gt;&amp;rdquo; blog hosted over on Education Week.&lt;/p&gt;

&lt;p&gt;Today, he ended his &lt;a href=&#34;http://blogs.edweek.org/edweek/on_performance/2011/09/are_we_expecting_too_much_of_teacher_evaluation_systems.html&#34;&gt;post&lt;/a&gt; with a question, &amp;ldquo;I would be very interested to learn of any other sector that has achieved substantial performance gains by reforming its evaluation processes. We&amp;rsquo;re putting a lot of eggs in the &amp;lsquo;improve teacher evaluation to improve student learning&amp;rsquo; basket, but no one even seems to be asking whether this strategy has any merit.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;I think this is the write idea but the wrong question. What we should wonder is whether any other sector has achieved substantial performance gains by reforming its entire process for hiring, retaining, supporting, and terminating its employees when that sector started with an extremely rigid, non-differentiated structure. Teacher evaluation is about providing better professional growth opportunities targeted to an individual&amp;rsquo;s needs. It&amp;rsquo;s about rewarding folks who are doing a stellar job and making sure that you can reward mission-critical people who might otherwise leave for other opportunities. And, much to many union members&amp;rsquo; chagrin, it&amp;rsquo;s also about providing substantial a substantial and trusted evidence base that principals can turn to justify termination decisions.&lt;/p&gt;

&lt;p&gt;Ask your favorite policy professional or administrator why they are pushing for centralized, mandatory, and prescriptive forms of teacher evaluation. I can guarantee they&amp;rsquo;ll include the current lack of serious evaluation in schools. I would bet that most folks also are pushing for these policies as a proactive step to make sure they can win union-based challenges against performance-based terminations and reassignment. Because the teacher unions are so strong and are largely steadfast in their need to treat all teachers equally&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, policymakers feel like they have to wrap evaluations in as much novel social science and standardization as possible so that they have even the tiniest chance in hell of holding up in court. To what extent can the lack of robust evaluation be connected to school leaders&amp;rsquo; lack of self-efficacy for action on this information?&lt;/p&gt;

&lt;p&gt;Teachers fear that a world without these protections would produce unfair evaluations and termination procedures that are subjective. Secretly, I bet that most policymakers would be totally comfortable not pushing hard for value-added models and overly specific observation rubrics. So long as they felt confident they could take action in response to the evaluations, the current evaluation hawks would instead be willing to leave much more to individual professional judgment&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. If the primary relationship in a school building was professional, and not a unionized labor-management split, a lot of the current evaluation policy might not be necessary. In the very least, the policies could be less centralized. But ultimately, professionals are held accountable for their work quality by bosses that employees respect as professionals.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll end with one final thought: I wonder what the teacher evaluation narrative would be like in an alternative history where there was no split between the teacher unions and professional organizations of education administrators and professors.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Important note: while I do work at a state department of education, I am not directly involved in nor am I intimately familiar with our teacher evaluation model or policies. As an employee of the Rhode Island Department of Education, I am also a member of the American Federation of Teachers Local 2012 union. The thoughts I&amp;rsquo;ve expressed in this post are entirely my own and does not represent the AFT or RIDE&amp;rsquo;s position.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Per his EdWeek Bio, &amp;ldquo;Justin Baeder is a public school principal in Seattle and a doctoral student studying principal performance and productivity at the University of Washington. In this blog he aims to examine issues of performance, improvement, and the changing nature of the education profession.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;One day I&amp;rsquo;ll write about the irony of equality of treatment for education professionals. It&amp;rsquo;s strange that our thinking around funding has largely evolved from &amp;ldquo;equality&amp;rdquo; to &amp;ldquo;adequacy&amp;rdquo; but not our treatment of adults
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Related important issue to solve&amp;ndash; low principal quality undermines this possibility. One day I&amp;rsquo;ll write about my belief that the principal role is poorly designed and dooms most people to failure. Rethinking the building principal is a critical structural reform folks will be hearing more and more about
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Libertarians and Charity</title>
      <link>http://www.json.blog/2011/09/libertarians-and-charity/</link>
      <pubDate>Thu, 15 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/09/libertarians-and-charity/</guid>
      <description>&lt;p&gt;As an undergraduate I largely avoided political science because I couldn&amp;rsquo;t imagine getting interested in reading The Republic, &lt;a href=&#34;http://www.coolstuffinc.com/images/Products/mtg%20art/Fourth/Leviathan.jpg&#34;&gt;Leviathan&lt;/a&gt;, or Wealth of Nations. Political philosophy, and philosophy in general, just seemed like a horrible painful exercise, so I avoided it. Of course now that I&amp;rsquo;m involved in &lt;a href=&#34;http://www.youtube.com/watch?v=mEJL2Uuv-oQ&#34;&gt;public policy&lt;/a&gt; and not &lt;a href=&#34;http://io9.com/5840609/the-best-chemistry-geek-anthem-ever&#34;&gt;organic chemistry&lt;/a&gt;, it feels as though I&amp;rsquo;ve done a horrible disservice to myself by not going through and systematically exploring more fundamental questions about the role of the state, ethics and morality, justice, etc.&lt;/p&gt;

&lt;p&gt;Part of my personal re-education in this area has been much easier by having access to a host of well-written blogs that host great conversations about these issues. These sources are smart, generally trustworthy, and are generally collegial. By reading actual academics apply their knowledge to current events, I am able to get access to a much more sophisticated conversation than is available in most popular media.&lt;/p&gt;

&lt;p&gt;One of these sources is &lt;a href=&#34;http://bleedingheartlibertarians.com/&#34;&gt;Bleeding Heart Libertarians&lt;/a&gt;, which seeks to explain how libertarians can have robust participation in social justice. This is a particularly interesting topic since, as I understand it, one of the major critiques of libertarianism is that it does not address social justice in a comprehensive and sufficient way.&lt;/p&gt;

&lt;p&gt;Today, commenting on &lt;a href=&#34;https://twitter.com/#!/ronpaul&#34;&gt;Ron Paul&lt;/a&gt;&amp;rsquo;s response to &lt;a href=&#34;https://twitter.com/#!/wolfblitzercnn&#34;&gt;Wolf Blitzer&lt;/a&gt;&amp;rsquo;s baiting on healthcare&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, BHL contributor &lt;a href=&#34;http://praxeology.net/&#34;&gt;Professor Roderick Long&lt;/a&gt; brought up one of the libertarian arguments that most confounds me&amp;ndash; charity and mutual aid. Long writes that a libertarians second response to an individual&amp;rsquo;s failure to use basic services ((Specifically, Blitzer presents the case of a healthy young man who foregoes health insurance. However, Professor Long&amp;rsquo;s suggested response is sufficiently vague that I believe it is safe to say that he would apply the same three stages to any situation where an individual&amp;rsquo;s circumstances or decisions have jeopardized their access to basic needs. This includes all social safety net programs.)) should be, &amp;ldquo;talk about how charity and mutual aid are more efficient than government welfare, and how we therefore need to shift the venue of assistance from the latter to the former.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;This argument had always felt extremely classist to me. It seems that those who are most vulnerable have never been the folks who have access to mutual aid or charity through local community organizations, family members, friends, and other contacts. General social capital aside, even people who have strong community ties who are most likely to need access to a social safety net live in communities that overwhelming don&amp;rsquo;t have the collective resources to offer sufficient aid to promote the welfare of that community. The whole concept seems steeped in a highly culturally informed sense of reality that imagines a small town church community as opposed to the generationally impoverished&amp;rsquo;s reality.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not that I&amp;rsquo;m not sympathetic to the argument that it is possible that models of mutual aid and charity could ultimately private superior resource allocation, it&amp;rsquo;s just that I don&amp;rsquo;t think that aggregate efficiency is the goal here. I realize that this is a statement of prior moral conviction, but it seems to me that the ostensible purpose of safety nets is to make sure the most coverage against a failure to meet the basic needs of all people. Under this situation, efficiency is desirable but secondary, and I don&amp;rsquo;t see how mutual aid and charity will provide sufficient coverage to meet the needs of the most important beneficiaries of these policies.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;http://bleedingheartlibertarians.com/2011/09/the-libertarian-three-step-program&#34;&gt;http://bleedingheartlibertarians.com/2011/09/the-libertarian-three-step-program&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Apple Migration Assistant</title>
      <link>http://www.json.blog/2011/09/apple-migration-assistant/</link>
      <pubDate>Wed, 14 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/09/apple-migration-assistant/</guid>
      <description>&lt;p&gt;For several years I considered switching to Apple. I&amp;rsquo;ve been a wannabe believer since OSX. I was using &lt;a href=&#34;http://images.memegenerator.net/instances/400x/6300003.jpg&#34;&gt;Linux&lt;/a&gt;as my every day operating system. This was one part about learning, one part frustration with Windows perpetual funky instabilities, and part a growing appreciation for things like the command line interface and free and open source
software. OSX offered many things I liked&amp;ndash; &lt;a href=&#34;http://imgs.xkcd.com/comics/sandwich.png&#34;&gt;a great CLI&lt;/a&gt; I was already getting intimately familiar with, &lt;a href=&#34;http://www.eatliver.com/img/2005/294.jpg&#34;&gt;rock hard stability&lt;/a&gt;, beautiful graphic effects like those I enjoyed with Compiz, etc. More importantly, OSX could do all this while providing me with a decent experience on some of the software I&amp;rsquo;d love to dump but simply could not like Microsoft Office. Additionally, I wouldn&amp;rsquo;t have all kinds of problems on the web surfing pages that were supposedly platform neutral and using browsers that were supposedly cross-platform (Flash on Linux was a joke as recently as two years ago when I abandoned Linux as my every day operating system). Of course, perhaps first and foremost, I would never have to worry about whether an update will cause a conflict because I was forced to use a deprecated driver to get my hardware to work or that some of my hardware would be limited because there wasn&amp;rsquo;t a fully functional driver available. But every time it came to a decision on purchasing a new machine, I could not bring myself to pay the &amp;ldquo;&lt;a href=&#34;http://www.urbandictionary.com/define.php?term=Apple+Tax&#34;&gt;Apple tax&lt;/a&gt;&amp;rdquo;. I was a &lt;a href=&#34;http://hollywoodroaster.com/wp-content/uploads/2008/10/maruchan.jpg&#34;&gt;student&lt;/a&gt;, and while I&amp;rsquo;ve generally felt that the Macbook Pro/Powerbook have had reasonable economics and a physical
design well beyond their competitors, I just couldn&amp;rsquo;t justify the price to power ratio. So I continued to build my own desktops and work on an IBM Thinkpad I would buy after finding a good deal.&lt;/p&gt;

&lt;p&gt;A year ago that changed when I purchased a 13&amp;rdquo; Macbook Pro. My laptop had totally crapped out on me and I needed a replacement fast. Combining the education discount (which I used a recently expired student ID for), a free printer and iPod, and tax-free weekend in Massachusetts meant I could buy a brand new MBP for around \$900. Nothing really could compete with this&amp;ndash; the price was right, the battery life, weight, size, and power were all right and I couldn&amp;rsquo;t even
reach parody with another PC. So I purchased my Macbook Pro.&lt;/p&gt;

&lt;p&gt;I was very happy with that laptop. A great keyboard is a must, and the Macbook Pro was the best I used other than my old Thinkpad T43. In addition to the keyboard, I also got a trackpad that was far superior to any I had used before that actually allowed me to ditch the mouse I normally carried with me everywhere I went. The battery life was a
ridiculous 8hrs&amp;ndash; my previous laptop got 2.5hrs at best and I used to think that was an accomplishment. Power wise I never ran into any hiccups. The stability was solid. OSX was easy to adopt to as a full-time OS and I was on my merry way.&lt;/p&gt;

&lt;p&gt;Except one, tiny, problem.&lt;/p&gt;

&lt;p&gt;I hate using a laptop if I don&amp;rsquo;t have to. Whenever I was home I plugged right into a much bigger screen, an external keyboard, and mouse and sat a desk to use my computer. Call me old-fashioned, but I have never been as comfortable working on a laptop as I am using a separate keyboard, mouse, and monitor. And laptop speakers? Don&amp;rsquo;t even get me started.&lt;/p&gt;

&lt;p&gt;Now none of this would be a problem because I used my laptop on the go. At the time I purchased my machine, I was just coming off of five years of school, during which I constantly worked in libraries, coffee shops, friends houses, etc. I also had worked as a consultant at several places over the past year, so bringing my workstation with me on the go was a
necessity. One month after I got my laptop, I was working a normal desk job but was assigned a desktop from the stone age that was barely functional. I found myself doing a significant amount of my work on my personal laptop that I brought with me from home. But about six months ago, my job purchased me a new desktop that was blazing fast. I work with confidential data virtually all day long, which was a huge hassle when I used an old machine. I often performed various data management activities with no more than one application open on my work computer, prepare the data in non-confidential format, and ship it off to my laptop for more in-depth analysis. The workflow was atrocious. Having a
functional desktop made it pointless to bring my laptop to work&amp;ndash; most of what I do couldn&amp;rsquo;t be done on a personal machine anyway. So while my workflow became much more efficient, my laptop lost utility. More and more I found myself simply leaving my laptop plugged in at my desk at home and operating it like a desktop. Fast forward to today and I fried
my battery, which now holds only 3-4 hours of charge and I haven&amp;rsquo;t used my laptop as a portable computer in months.&lt;/p&gt;

&lt;p&gt;I decided I should sell my laptop and replace it with a Mac Mini, which brings me to the title of this post. Perhaps the most pleasant experience I&amp;rsquo;ve had on any computer since I first used a Gateway 2000 c. 1992 came from using Apple&amp;rsquo;s Migration Assistant. Upon turning on my Mac Mini for the first time, the setup wizard offered  the opportunity to transfer files and settings from another computer. Now this is a feature that browsers and other software have offered for years and the experience has never been all that useful to me. But this time I decided
to try it and I plugged my laptop into my Mac Mini using an ethernet cable. Approximate 2 hours later my Mac Mini restarted and the experience was breathtaking.&lt;/p&gt;

&lt;p&gt;Everything, and I mean everything, transferred over to my new computer. All my applications were installed. All of my settings, including those made by software like Onyx and Geek Tool, transferred over. All my documents were where I left them. The experience was indistinguishable from logging onto my laptop.&lt;/p&gt;

&lt;p&gt;This is an astonishingly great and useful feature. It seems so simple in theory, but execution can easily be botched. Apple hit a home run with Migration Assistant, at least as of the version that comes standard in
Lion.&lt;/p&gt;

&lt;p&gt;Ultimately, my experience with Migration Assistant, along with the great resale value on my Macbook Pro, has pretty much ensured that my next computer will be an Apple.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Community Voice at the Knowledge District Development Framework Meeting</title>
      <link>http://www.json.blog/2011/09/community-voice-at-the-knowledge-district-development-framework-meeting/</link>
      <pubDate>Thu, 08 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/2011/09/community-voice-at-the-knowledge-district-development-framework-meeting/</guid>
      <description>&lt;p&gt;Tonight, I went to a public meeting run by Providence&amp;rsquo;s Department of Planning and Development. A few of upfront observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The folks who work for the Department of Planning and Development (DPD) were professional, kind, and capable, as was the consultants who worked with them. They maintained decorum and a genuine sense of openness even though there was a clear tension in the room between some outspoken (and knowledgeable) community members and activists who had clearly participated in many past meetings.&lt;/li&gt;
&lt;li&gt;The ideas presented for the Knowledge District demonstrated thoughtful, albeit top-down, considerations for the space that showed a remarkable respect for the complexity, size, and importance of the project. I also felt that the DPD and their consultants got all the big ideas right and that they were quite familiar with the community they were re-imagining.&lt;/li&gt;
&lt;li&gt;Despite some great big ideas, there are really important details that are worth memorializing that the DPD is missing. In part, maybe this is because they&amp;rsquo;re simply not at that deep a stage. I&amp;rsquo;m hopeful that the purpose of the public meetings was not just to get
feedback on the big concepts (which seem largely unassailable), but to make sure they get the details right from the people who live and interact with these neighborhoods daily.&lt;/li&gt;
&lt;li&gt;There are some clear areas of consensus among the engaged community, both positive and negative. At times, this consensus suggest that the state, city, and institutions (business, education, health, etc) have very different goals than the community.
∫
Here&amp;rsquo;s my summary of what the community had to say:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Folks want to get rid of surface parking lots and move towards more parking structures. It&amp;rsquo;s very clear that everyone feels these surface lots contribute to the desolate feel of the Jewelry District and hospital area, as well as acting as an extra barrier (beyond the highways) to connecting the Knowledge District to the rest of the city. People want to see mixed-use, with real action on the street level and residences and offices on higher floors. Everyone wants better sight lines to draw people into the Jewelry district and wants to see green space embraced, not solely through a park on the water but intimately placed directly on the streets as trees and other landscaping. People cited Chestnut and Richmond Street  several times as strong streets that acted as the main arteries by which Downcity and the Jewelry District connect. It seems that there is broad agreement that there a reason needs to exist to draw pedestrians into this neighborhood from Downcity, Fox Point, and Upper South Providence if we&amp;rsquo;re going to have a vital, &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt; community. There was also pretty broad acceptance of higher buildings being constructed along I-95 and keeping large footprint buildings out of the Jewelry District.&lt;/p&gt;

&lt;p&gt;Most of the more negative tone of the evening came from two core issues&amp;ndash; the need for more residential development, something that&amp;rsquo;s not seen as high priority or even on the radar of most public officials, and funding. I&amp;rsquo;ll start with funding. There are serious concerns that even if the plans are perfect and great, a total lack of municipal, state, and federal funding for the foreseeable future places major risks on central aspects of the DPD&amp;rsquo;s plans. How can we fund a large, sweeping greenway and inviting, beautiful streets? How can we fund restoring vital roadways absorbed by bad planning in the past? How can we upkeep the parks people crave or build vital family institutions like schools without any public funding? How will we repurpose landmark
buildings like the Dynamo House that has sat vacant even though it&amp;rsquo;s so full of potential? There is a serious sense that all the projects that serve as good models for the Knowledge District required considerable public infrastructure investment that&amp;rsquo;s just not available now. Sadly, to truly &amp;ldquo;fix&amp;rdquo; the Knowledge District will require not just one large project, but several major improvements. I saw little optimism for the proposed Streetcar/light rail system that RIPTA is championing. Some of that was disbelief that it could ever be funded, and some of that was disbelief the streetcar was a solution to a real problem. There&amp;rsquo;s also little optimism that the proposed pedestrian bridge to connect Fox Point to the Jewelry District is going to happen. Everyone begrudged that DPD had little real authority to make anything happen. Zoning is an absolute disgrace in Providence, particularly this area. But even substantially improving the zoning and regulations around development won&amp;rsquo;t actually make sure the projects are mindful of the projects wider goals. More to the point, it&amp;rsquo;s still unclear how Providence can simply use the name &amp;ldquo;Knowledge District&amp;rdquo; to bring in the kind of development needed. There is serious consternation that the entire &amp;ldquo;Knowledge District&amp;rdquo; concept is selling something that doesn&amp;rsquo;t exist and won&amp;rsquo;t have the infrastructure to attract folks. Without the promise of big public infrastructure improvements, developers are going to play the &amp;ldquo;wait and see&amp;rdquo; game.&lt;/p&gt;

&lt;p&gt;Residential development is another important aspect of what the community members craved. It was immediately clear that the desire of folks at this meeting was to have a &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt; neighborhood with mixed-uses, including several calls (from residents, no less) to include low-income and affordable housing. There is strong dislike of the name &amp;ldquo;Knowledge District&amp;rdquo;, especially if it supplants the Jewelry District (which is a subset of the area in question). No one feels that this name captures what they want to use the space for, and no one felt the name had any
real meaning. Folks believed that &amp;ldquo;Knowledge District&amp;rdquo; was as empty marketing that would have no real longterm staying power. But really, this goes beyond a bad name. The members of the community who met with DPD tonight clearly believe that residential development has taken a
major backseat to institutional expansion and large business development. My interpretation was that the community felt that lawmakers were simply hoping that big groups like the hospitals, Brown University, Johnson and Wales, and some yet-to-be-named mid-sized businesses would snap up parcels and build massive buildings that would fill with jobs. First, they believed this vision was largely a fiction (again, see the hesitation folks had that businesses of any remarkable
size would take on the development costs and move to Providence without the public infrastructure investments). Second, these kinds of buildings were not what the community envisioned, particularly for the Jewelry District area that already has much smaller plot sizes and lower building heights. What I heard, rather clearly, was that the sea of parking lots around the hospital and the area with raised highway along I-95 was fair game for the behemoth buildings most lawmakers are picturing for the entire project. But the interior of the Knowledge
District has to be filled in with a place that people want to live, play and work in (in that order).&lt;/p&gt;

&lt;p&gt;I hope to write some more on my thoughts on developing this area in the future. I generally agree with the comments from the community above, but I have a bit more optimism and a bit more faith. Overall, I was really happy with how DPD is framing this project. They are very consciously thinking about distinct portions of the Knowledge District and respecting their differences while simultaneously ensuring cohesion and setting strong, wider goals.&lt;/p&gt;

&lt;p&gt;I just hope we get a damn grocery store and dramatically cut down on surface lots. More on that later.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://www.json.blog/resume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/resume/</guid>
      <description>&lt;html&gt;
  &lt;head&gt;
    &lt;link href = &#34;{{ .Site.BaseURL }}/css/simple-grid.css&#34; rel = &#34;stylesheet&#34; type = &#34;text/css&#34; /&gt;
    &lt;link href = &#34;//cdn.jsdelivr.net/font-hack/2.020/css/hack.min.css&#34; rel = &#34;stylesheet&#34; /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class = &#34;container&#34;&gt;
      &lt;div class = &#34;row&#34;&gt;
        &lt;div class = &#34;col-12&#34;&gt;
          &lt;h1 style = &#34;font-size: 2.5rem;&#34;&gt; Jason Becker &lt;/h1&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&#34;row&#34;&gt;
        &lt;div class=&#34;col-3&#34;&gt;
          &lt;h3 class=&#34;font-light&#34; style=&#34;-webkit-margin-before: 0em&#34;&gt;Skills&lt;/h3&gt;
        &lt;/div&gt;
        &lt;div class=&#34;col-6&#34;&gt;
          &lt;h2 style=&#34;-webkit-margin-before: 0em&#34;&gt;Experience &lt;/h2&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class = &#34;row&#34; style = &#34;display: flex;&#34;&gt;
        &lt;div class=&#34;col-3&#34;&gt;
        &lt;/div&gt;
        &lt;div class = &#34;col-6 font-light&#34; style = &#34;padding-left: 0.5em;&#34;&gt;
          &lt;h3 style = &#34;-webkit-margin-after:0em; -webkit-margin-before:0em&#34;&gt;Allovue&lt;/h3&gt;&lt;br /&gt;
          Chief Product Officer &lt;br /&gt;
          Director of Data &lt;br /&gt;
        &lt;/div&gt;
        &lt;div class = &#34;col-3 font-light&#34;&gt;
          &lt;h3 style = &#34;-webkit-margin-after:0em; -webkit-margin-before:0em;&#34;&gt;Baltimore, MD&lt;/h3&gt;&lt;br /&gt;
          2015&amp;ndash;Present &lt;br /&gt;
          2014&amp;ndash;2015
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class = &#34;row&#34;&gt;
        &lt;div class = &#34;col-3&#34;&gt;
        &lt;/div&gt;
        &lt;div class = &#34;col-8&#34; style = &#34;padding-left:0.25em&#34;&gt;
          &lt;h3 class=&#34;font-light&#34;&gt; Product Management &lt;/h3&gt;
          &lt;ul&gt;
            &lt;li&gt; Define and maintain product roadmap.&lt;/li&gt;
            &lt;li&gt; Gather business requirements&lt;/li&gt;
            &lt;li&gt; Produce information architecture diagrams and data flow diagrams &lt;/li&gt;
            &lt;li&gt; Work collaboratively with designers to generate wireframes &lt;/li&gt;
            &lt;li&gt; Gather feedback from customers and internal stakeholders on new features and requirements &lt;/li&gt;
            &lt;li&gt; Test new features to ensure they meet acceptance criteria and do not result in regressions &lt;/li&gt;
            &lt;li&gt; Onboard, manage, and support product managers &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h3 class = &#34;font-light&#34;&gt; Data Integration &lt;/h3&gt;
          &lt;ul&gt;
            &lt;li&gt; Develop R packages to easily connect to hetergenous remote data sources &lt;/li&gt;
            &lt;li&gt; Produce ETL process to map any ERP system to Allovue&#39;s data schema within 15 days. &lt;/li&gt;
            &lt;li&gt; Develop ETL connectors in SQL and R&lt;/li&gt;
            &lt;li&gt; Provide data analaysis and quality assurance support &lt;/li&gt;
            &lt;li&gt; Onboard, train, manage, and support data associates &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h3 class = &#34;font-light&#34;&gt; Customer Relationships&lt;/h3&gt;
          &lt;ul&gt;
            &lt;li&gt; Own technical customer relationship from pre-sales through integration. &lt;/li&gt;
            &lt;li&gt; Present product demos to qualified leads&lt;/li&gt;
            &lt;li&gt; Solve sales engineering problems pre-sale through sale to help close new relationships &lt;/li&gt;
            &lt;li&gt; Co-author proposals&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/div&gt;
      &lt;/div&gt;
  &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title>Archive</title>
      <link>http://www.json.blog/archive/archive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://www.json.blog/archive/archive/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
